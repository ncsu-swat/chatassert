[INFO] Scanning for projects...
[INFO] 
[INFO] -------------------< ncsusoftware:canonicalization >--------------------
[INFO] Building DemoMavenProject 1.0-SNAPSHOT
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ canonicalization ---
[INFO] Deleting /mnt/beegfs/ihayet/ogpt/oragen/astvisitors/target
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ canonicalization ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] skip non existing resourceDirectory /mnt/beegfs/ihayet/ogpt/oragen/astvisitors/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ canonicalization ---
[INFO] Changes detected - recompiling the module!
[WARNING] File encoding has not been set, using platform encoding UTF-8, i.e. build is platform dependent!
[INFO] Compiling 16 source files to /mnt/beegfs/ihayet/ogpt/oragen/astvisitors/target/classes
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  2.335 s
[INFO] Finished at: 2023-07-01T13:45:17-04:00
[INFO] ------------------------------------------------------------------------
[INFO] Scanning for projects...
[INFO] 
[INFO] -------------------< ncsusoftware:canonicalization >--------------------
[INFO] Building DemoMavenProject 1.0-SNAPSHOT
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-dependency-plugin:2.8:build-classpath (default-cli) @ canonicalization ---
[WARNING] The artifact xml-apis:xml-apis:jar:2.0.2 has been relocated to xml-apis:xml-apis:jar:1.0.b2
[INFO] Wrote classpath file '/mnt/beegfs/ihayet/ogpt/oragen/astvisitors/cp.txt'.
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  0.767 s
[INFO] Finished at: 2023-07-01T13:45:19-04:00
[INFO] ------------------------------------------------------------------------
java processing server waiting for calls...
Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.urlfrontier;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.persistence.AbstractQueryingSpout;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import crawlercommons.urlfrontier.URLFrontierGrpc;
import crawlercommons.urlfrontier.URLFrontierGrpc.URLFrontierStub;
import crawlercommons.urlfrontier.Urlfrontier.GetParams;
import crawlercommons.urlfrontier.Urlfrontier.URLInfo;
import io.grpc.ManagedChannel;
import io.grpc.ManagedChannelBuilder;
import io.grpc.stub.StreamObserver;
import java.util.Map;
import java.util.concurrent.atomic.AtomicInteger;
import org.apache.storm.Config;
import org.apache.storm.spout.SpoutOutputCollector;
import org.apache.storm.task.TopologyContext;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

@SuppressWarnings("serial")
public class Spout extends AbstractQueryingSpout {

    public static final Logger LOG = LoggerFactory.getLogger(Spout.class);

    private ManagedChannel channel;

    private URLFrontierStub frontier;

    private int maxURLsPerBucket;

    private int maxBucketNum;

    private int delayRequestable;

    @SuppressWarnings({"rawtypes", "unchecked"})
    @Override
    public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) {

        super.open(conf, context, collector);
        // host and port of URL Frontier

        String host = ConfUtils.getString(conf, "urlfrontier.host", "localhost");
        int port = ConfUtils.getInt(conf, "urlfrontier.port", 7071);

        maxURLsPerBucket = ConfUtils.getInt(conf, "urlfrontier.max.urls.per.bucket", 10);

        maxBucketNum = ConfUtils.getInt(conf, "urlfrontier.max.buckets", 10);

        // initialise the delay requestable with the timeout for messages
        delayRequestable =
                ConfUtils.getInt(conf, Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS, delayRequestable);

        // then override with any user specified config
        delayRequestable =
                ConfUtils.getInt(conf, "urlfrontier.delay.requestable", delayRequestable);

        LOG.info("Initialisation of connection to URLFrontier service on {}:{}", host, port);

        channel = ManagedChannelBuilder.forAddress(host, port).usePlaintext().build();
        frontier = URLFrontierGrpc.newStub(channel);
    }

    @Override
    protected void populateBuffer() {

        LOG.debug(
                "Populating buffer - max queues {} - max URLs per queues {}",
                maxBucketNum,
                maxURLsPerBucket);

        GetParams request =
                GetParams.newBuilder()
                        .setMaxUrlsPerQueue(maxURLsPerBucket)
                        .setMaxQueues(maxBucketNum)
                        .setDelayRequestable(delayRequestable)
                        .build();

        AtomicInteger atomicint = new AtomicInteger();

        StreamObserver<URLInfo> responseObserver =
                new StreamObserver<URLInfo>() {

                    @Override
                    public void onNext(URLInfo item) {
                        Metadata m = new Metadata();
                        item.getMetadataMap()
                                .forEach(
                                        (k, v) -> {
                                            for (int index = 0;
                                                    index < v.getValuesCount();
                                                    index++) {
                                                m.addValue(k, v.getValues(index));
                                            }
                                        });
                        buffer.add(item.getUrl(), m, item.getKey());
                        atomicint.addAndGet(1);
                    }

                    @Override
                    public void onError(Throwable t) {
                        LOG.error("Exception caught", t);
                    }

                    @Override
                    public void onCompleted() {
                        markQueryReceivedNow();
                    }
                };

        isInQuery.set(true);

        frontier.getURLs(request, responseObserver);

        LOG.debug("Got {} URLs from the frontier", atomicint.get());
    }

    @Override
    public void ack(Object msgId) {
        LOG.debug("Ack for {}", msgId);
        super.ack(msgId);
    }

    @Override
    public void fail(Object msgId) {
        LOG.info("Fail for {}", msgId);
        super.fail(msgId);
    }

    @Override
    public void close() {
        super.close();
        LOG.info("Shutting down connection to URLFrontier service");
        channel.shutdown();
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.urlfrontier;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.persistence.AbstractStatusUpdaterBolt;
import com.digitalpebble.stormcrawler.persistence.Status;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import com.digitalpebble.stormcrawler.util.URLPartitioner;
import com.github.benmanes.caffeine.cache.Cache;
import com.github.benmanes.caffeine.cache.Caffeine;
import com.github.benmanes.caffeine.cache.RemovalCause;
import com.github.benmanes.caffeine.cache.RemovalListener;
import crawlercommons.urlfrontier.URLFrontierGrpc;
import crawlercommons.urlfrontier.URLFrontierGrpc.URLFrontierStub;
import crawlercommons.urlfrontier.Urlfrontier.DiscoveredURLItem;
import crawlercommons.urlfrontier.Urlfrontier.KnownURLItem;
import crawlercommons.urlfrontier.Urlfrontier.StringList;
import crawlercommons.urlfrontier.Urlfrontier.StringList.Builder;
import crawlercommons.urlfrontier.Urlfrontier.URLInfo;
import crawlercommons.urlfrontier.Urlfrontier.URLItem;
import io.grpc.ManagedChannel;
import io.grpc.ManagedChannelBuilder;
import io.grpc.stub.StreamObserver;
import java.util.Date;
import java.util.HashMap;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.Optional;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicInteger;
import org.apache.storm.metric.api.MultiCountMetric;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.utils.Utils;
import org.checkerframework.checker.nullness.qual.Nullable;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

@SuppressWarnings("serial")
public class StatusUpdaterBolt extends AbstractStatusUpdaterBolt
        implements RemovalListener<String, List<Tuple>>,
                StreamObserver<crawlercommons.urlfrontier.Urlfrontier.String> {

    public static final Logger LOG = LoggerFactory.getLogger(StatusUpdaterBolt.class);
    private URLFrontierStub frontier;
    private ManagedChannel channel;
    private URLPartitioner partitioner;
    private StreamObserver<URLItem> requestObserver;
    private Cache<String, List<Tuple>> waitAck;

    private int maxMessagesinFlight = 100000;
    private int throttleTime = 10;

    private AtomicInteger messagesinFlight = new AtomicInteger();

    private MultiCountMetric eventCounter;

    public StatusUpdaterBolt() {
        waitAck =
                Caffeine.newBuilder()
                        .expireAfterWrite(60, TimeUnit.SECONDS)
                        .removalListener(this)
                        .build();
    }

    @SuppressWarnings({"rawtypes", "unchecked"})
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
        super.prepare(stormConf, context, collector);
        String host = ConfUtils.getString(stormConf, "urlfrontier.host", "localhost");
        int port = ConfUtils.getInt(stormConf, "urlfrontier.port", 7071);

        maxMessagesinFlight =
                ConfUtils.getInt(
                        stormConf, "urlfrontier.max.messages.in.flight", maxMessagesinFlight);
        throttleTime =
                ConfUtils.getInt(stormConf, "urlfrontier.throttling.time.msec", throttleTime);

        this.eventCounter =
                context.registerMetric(this.getClass().getSimpleName(), new MultiCountMetric(), 30);

        maxMessagesinFlight =
                ConfUtils.getInt(
                        stormConf, "urlfrontier.updater.max.messages", maxMessagesinFlight);

        LOG.info("Initialisation of connection to URLFrontier service on {}:{}", host, port);
        LOG.info("Allowing up to {} message in flight", maxMessagesinFlight);

        channel = ManagedChannelBuilder.forAddress(host, port).usePlaintext().build();
        frontier = URLFrontierGrpc.newStub(channel);

        partitioner = new URLPartitioner();
        partitioner.configure(stormConf);

        requestObserver = frontier.putURLs(this);
    }

    @Override
    public void onNext(final crawlercommons.urlfrontier.Urlfrontier.String value) {
        final String url = value.getValue();
        synchronized (waitAck) {
            List<Tuple> xx = waitAck.getIfPresent(url);
            if (xx != null) {
                waitAck.invalidate(url);
            } else {
                LOG.warn("Could not find unacked tuple for {}", url);
            }
        }
    }

    @Override
    public void onError(Throwable t) {
        LOG.error("Error received", t);
    }

    @Override
    public void onCompleted() {
        // end of stream - nothing special to do?
    }

    @Override
    public void store(
            String url, Status status, Metadata metadata, Optional<Date> nextFetch, Tuple t)
            throws Exception {

        while (messagesinFlight.get() >= this.maxMessagesinFlight) {
            LOG.debug("{} messages in flight - waiting a bit...", messagesinFlight.get());
            eventCounter.scope("timeSpentThrottling").incrBy(throttleTime);
            Utils.sleep(throttleTime);
        }

        // only 1 thread at a time will access the store method
        // but onNext() might try to access waitAck at the same time
        synchronized (waitAck) {

            // tuples received for the same URL
            // could be the same URL discovered from different pages
            // at the same time
            // or a page fetched linking to itself
            List<Tuple> tt = waitAck.get(url, k -> new LinkedList<Tuple>());

            // check that the same URL is not being sent to the frontier
            if (status.equals(Status.DISCOVERED) && !tt.isEmpty()) {
                // if this object is discovered - adding another version of it
                // won't make any difference
                LOG.debug("Already being sent to urlfrontier {} with status {}", url, status);
                // ack straight away!
                eventCounter.scope("acked").incrBy(1);
                super.ack(t, url);
                return;
            }

            String partitionKey = partitioner.getPartition(url, metadata);
            if (partitionKey == null) {
                partitionKey = "_DEFAULT_";
            }

            final Map<String, StringList> mdCopy = new HashMap<>(metadata.size());
            for (String k : metadata.keySet()) {
                String[] vals = metadata.getValues(k);
                Builder builder = StringList.newBuilder();
                for (String v : vals) builder.addValues(v);
                mdCopy.put(k, builder.build());
            }

            URLInfo info =
                    URLInfo.newBuilder()
                            .setKey(partitionKey)
                            .setUrl(url)
                            .putAllMetadata(mdCopy)
                            .build();

            crawlercommons.urlfrontier.Urlfrontier.URLItem.Builder itemBuilder =
                    URLItem.newBuilder();
            if (status.equals(Status.DISCOVERED)) {
                itemBuilder.setDiscovered(DiscoveredURLItem.newBuilder().setInfo(info).build());
            } else {
                // next fetch date
                long date = 0;
                if (nextFetch.isPresent()) {
                    date = nextFetch.get().toInstant().getEpochSecond();
                }
                itemBuilder.setKnown(
                        KnownURLItem.newBuilder()
                                .setInfo(info)
                                .setRefetchableFromDate(date)
                                .build());
            }

            messagesinFlight.incrementAndGet();
            requestObserver.onNext(itemBuilder.build());

            tt.add(t);
            LOG.trace("Added to waitAck {} with ID {} total {}", url, url, tt.size());
        }
    }

    @Override
    public void onRemoval(@Nullable String key, @Nullable List<Tuple> values, RemovalCause cause) {
        final String url = key;

        // explicit removal
        if (!cause.wasEvicted()) {
            LOG.debug("Acked {} tuple(s) for ID {}", values.size(), url);
            for (Tuple x : values) {
                messagesinFlight.decrementAndGet();
                eventCounter.scope("acked").incrBy(1);
                super.ack(x, url);
            }
            return;
        }

        LOG.error("Evicted {} from waitAck with {} values", url, values.size());

        for (Tuple t : values) {
            messagesinFlight.decrementAndGet();
            eventCounter.scope("failed").incrBy(1);
            _collector.fail(t);
        }
    }

    @Override
    public void cleanup() {
        requestObserver.onCompleted();
        channel.shutdownNow();
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.aws.s3;

import com.amazonaws.services.s3.model.AmazonS3Exception;
import com.amazonaws.services.s3.model.S3Object;
import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import java.io.UnsupportedEncodingException;
import java.net.URLEncoder;
import java.util.Map;
import org.apache.commons.io.IOUtils;
import org.apache.storm.metric.api.MultiCountMetric;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.tuple.Fields;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.tuple.Values;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Checks whether a URL can be found in the cache, if not delegate to the following bolt e.g.
 * Fetcher, which gets bypassed otherwise. Does not enforce any politeness. The credentials must be
 * stored in ~/.aws/credentials
 */
@SuppressWarnings("serial")
public class S3CacheChecker extends AbstractS3CacheBolt {

    public static final Logger LOG = LoggerFactory.getLogger(S3CacheChecker.class);

    public static final String CACHE_STREAM = "cached";

    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
        super.prepare(conf, context, collector);
        bucketName = ConfUtils.getString(conf, BUCKET);

        boolean bucketExists = client.doesBucketExist(bucketName);
        if (!bucketExists) {
            String message = "Bucket " + bucketName + " does not exist";
            throw new RuntimeException(message);
        }
        this.eventCounter = context.registerMetric("s3cache_counter", new MultiCountMetric(), 10);
    }

    @Override
    public void execute(Tuple tuple) {
        String url = tuple.getStringByField("url");
        Metadata metadata = (Metadata) tuple.getValueByField("metadata");

        // normalises URL
        String key = "";
        try {
            key = URLEncoder.encode(url, "UTF-8");
        } catch (UnsupportedEncodingException e) {
            // ignore it - we know UTF-8 is valid
        }
        // check size of the key
        if (key.length() >= 1024) {
            LOG.info("Key too large : {}", key);
            eventCounter.scope("result_keytoobig").incrBy(1);
            _collector.emit(tuple, new Values(url, metadata));
            // ack it no matter what
            _collector.ack(tuple);
            return;
        }

        long preCacheQueryTime = System.currentTimeMillis();
        S3Object obj = null;
        try {
            obj = client.getObject(bucketName, key);
        } catch (AmazonS3Exception e) {
            eventCounter.scope("result_misses").incrBy(1);
            // key does not exist?
            // no need for logging
        }
        long postCacheQueryTime = System.currentTimeMillis();
        LOG.debug("Queried S3 cache in {} msec", (postCacheQueryTime - preCacheQueryTime));

        if (obj != null) {
            try {
                byte[] content = IOUtils.toByteArray(obj.getObjectContent());
                eventCounter.scope("result_hits").incrBy(1);
                eventCounter.scope("bytes_fetched").incrBy(content.length);

                metadata.setValue(INCACHE, "true");

                _collector.emit(CACHE_STREAM, tuple, new Values(url, content, metadata));
                _collector.ack(tuple);
                return;
            } catch (Exception e) {
                eventCounter.scope("result.exception").incrBy(1);
                LOG.error("IOException when extracting byte array", e);
            }
        }

        _collector.emit(tuple, new Values(url, metadata));
        _collector.ack(tuple);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("url", "metadata"));
        declarer.declareStream(CACHE_STREAM, new Fields("url", "content", "metadata"));
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.aws.s3;

import com.digitalpebble.stormcrawler.Metadata;

/** Caches byte[] content into S3 */
@SuppressWarnings("serial")
public class S3ContentCacher extends S3Cacher {

    @Override
    protected byte[] getContentToCache(Metadata metadata, byte[] content, String url) {
        if (!"true".equalsIgnoreCase(metadata.getFirstValue("http.trimmed"))) {
            return content;
        }

        LOG.info("Content was trimmed, so will not return to be cached");
        return null;
    }

    @Override
    protected String getKeyPrefix() {
        return "";
    }

    @Override
    protected String getMetricPrefix() {
        return "counters_" + getClass().getSimpleName();
    }

    @Override
    protected boolean shouldOverwrite(Metadata metadata) {
        return (!"true".equalsIgnoreCase(metadata.getFirstValue(INCACHE)));
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.aws.s3;

import com.amazonaws.ClientConfiguration;
import com.amazonaws.auth.AWSCredentials;
import com.amazonaws.auth.AWSCredentialsProvider;
import com.amazonaws.auth.DefaultAWSCredentialsProviderChain;
import com.amazonaws.regions.RegionUtils;
import com.amazonaws.services.s3.AmazonS3Client;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import java.util.Map;
import org.apache.commons.lang.StringUtils;
import org.apache.storm.metric.api.MultiCountMetric;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.topology.base.BaseRichBolt;
import org.apache.storm.tuple.Fields;

@SuppressWarnings("serial")
public abstract class AbstractS3CacheBolt extends BaseRichBolt {

    public static final String S3_PREFIX = "s3.";
    public static final String ENDPOINT = S3_PREFIX + "endpoint";
    public static final String BUCKET = S3_PREFIX + "bucket";

    // is the region needed?
    public static final String REGION = S3_PREFIX + "region";

    public static final String INCACHE = S3_PREFIX + "inCache";

    protected OutputCollector _collector;
    protected MultiCountMetric eventCounter;

    protected AmazonS3Client client;

    protected String bucketName;

    /** Returns an S3 client given the configuration * */
    public static AmazonS3Client getS3Client(Map conf) {
        AWSCredentialsProvider provider = new DefaultAWSCredentialsProviderChain();
        AWSCredentials credentials = provider.getCredentials();
        ClientConfiguration config = new ClientConfiguration();

        AmazonS3Client client = new AmazonS3Client(credentials, config);

        String regionName = ConfUtils.getString(conf, REGION);
        if (StringUtils.isNotBlank(regionName)) {
            client.setRegion(RegionUtils.getRegion(regionName));
        }

        String endpoint = ConfUtils.getString(conf, ENDPOINT);
        if (StringUtils.isNotBlank(endpoint)) {
            client.setEndpoint(endpoint);
        }
        return client;
    }

    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
        _collector = collector;
        client = getS3Client(conf);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("url", "content", "metadata"));
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.aws.s3;

import com.amazonaws.services.s3.model.AmazonS3Exception;
import com.amazonaws.services.s3.model.ObjectMetadata;
import com.amazonaws.services.s3.model.PutObjectResult;
import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import java.io.ByteArrayInputStream;
import java.io.IOException;
import java.io.UnsupportedEncodingException;
import java.net.URLEncoder;
import java.util.Map;
import org.apache.storm.metric.api.MultiCountMetric;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.tuple.Values;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/** Stores binary content into Amazon S3. The credentials must be stored in ~/.aws/credentials */
@SuppressWarnings("serial")
public abstract class S3Cacher extends AbstractS3CacheBolt {

    public static final Logger LOG = LoggerFactory.getLogger(S3Cacher.class);

    protected abstract byte[] getContentToCache(Metadata metadata, byte[] content, String url);

    protected abstract String getKeyPrefix();

    protected abstract String getMetricPrefix();

    protected abstract boolean shouldOverwrite(Metadata metadata);

    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollector collector) {

        super.prepare(conf, context, collector);

        bucketName = ConfUtils.getString(conf, BUCKET);

        boolean bucketExists = client.doesBucketExist(bucketName);
        if (!bucketExists) {
            String message = "Bucket " + bucketName + " does not exist";
            throw new RuntimeException(message);
        }
        this.eventCounter =
                context.registerMetric(
                        getMetricPrefix() + "s3cache_counter", new MultiCountMetric(), 10);
    }

    @Override
    public void execute(Tuple tuple) {
        // stores the binary content on S3

        byte[] content = tuple.getBinaryByField("content");
        String url = tuple.getStringByField("url");
        final Metadata metadata = (Metadata) tuple.getValueByField("metadata");

        // If there is no content
        byte[] contentToCache = getContentToCache(metadata, content, url);
        if (contentToCache == null) {
            LOG.info("{} had no data to cache", url);
            _collector.emit(tuple, new Values(url, content, metadata));
            // ack it no matter what
            _collector.ack(tuple);
            return;
        }

        // already in the cache
        // don't need to recache it
        if (!shouldOverwrite(metadata)) {
            eventCounter.scope("already_in_cache").incr();
            _collector.emit(tuple, new Values(url, content, metadata));
            // ack it no matter what
            _collector.ack(tuple);
            return;
        }

        // normalises URL
        String key = "";
        try {
            key = URLEncoder.encode(url, "UTF-8");
        } catch (UnsupportedEncodingException e) {
            // ignore it - we know UTF-8 is valid
        }
        // check size of the key
        if (key.length() >= 1024) {
            LOG.info("Key too large : {}", key);
            eventCounter.scope("key_too_large").incr();
            _collector.emit(tuple, new Values(url, content, metadata));
            // ack it no matter what
            _collector.ack(tuple);
            return;
        }

        ByteArrayInputStream input = new ByteArrayInputStream(contentToCache);

        ObjectMetadata md = new ObjectMetadata();
        md.setContentLength(contentToCache.length);
        md.setHeader("x-amz-storage-class", "STANDARD_IA");

        try {
            PutObjectResult result = client.putObject(bucketName, getKeyPrefix() + key, input, md);
            eventCounter.scope("cached").incr();
            // TODO check something with the result?
        } catch (AmazonS3Exception exception) {
            LOG.error("AmazonS3Exception while storing {}", url, exception);
            eventCounter.scope("s3_exception").incr();
        } finally {
            try {
                input.close();
            } catch (IOException e) {
                LOG.error("Error while closing ByteArrayInputStream", e);
            }
        }

        _collector.emit(tuple, new Values(url, content, metadata));
        // ack it no matter what
        _collector.ack(tuple);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.aws.bolt;

public interface CloudSearchConstants {
    public static final String CLOUDSEARCH_PREFIX = "cloudsearch.";
    public static final String ENDPOINT = CLOUDSEARCH_PREFIX + "endpoint";
    public static final String REGION = CLOUDSEARCH_PREFIX + "region";
    public static final String BATCH_DUMP = CLOUDSEARCH_PREFIX + "batch.dump";
    public static final String MAX_DOCS_BATCH = CLOUDSEARCH_PREFIX + "batch.maxSize";
    public static final String MAX_TIME_BUFFERED = CLOUDSEARCH_PREFIX + "batch.max.time.buffered";
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.aws.bolt;

import static com.digitalpebble.stormcrawler.Constants.StatusStreamName;

import com.amazonaws.regions.RegionUtils;
import com.amazonaws.services.cloudsearchdomain.AmazonCloudSearchDomainClient;
import com.amazonaws.services.cloudsearchdomain.model.ContentType;
import com.amazonaws.services.cloudsearchdomain.model.DocumentServiceWarning;
import com.amazonaws.services.cloudsearchdomain.model.UploadDocumentsRequest;
import com.amazonaws.services.cloudsearchdomain.model.UploadDocumentsResult;
import com.amazonaws.services.cloudsearchv2.AmazonCloudSearchClient;
import com.amazonaws.services.cloudsearchv2.model.DescribeDomainsRequest;
import com.amazonaws.services.cloudsearchv2.model.DescribeDomainsResult;
import com.amazonaws.services.cloudsearchv2.model.DescribeIndexFieldsRequest;
import com.amazonaws.services.cloudsearchv2.model.DescribeIndexFieldsResult;
import com.amazonaws.services.cloudsearchv2.model.DomainStatus;
import com.amazonaws.services.cloudsearchv2.model.IndexFieldStatus;
import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.indexing.AbstractIndexerBolt;
import com.digitalpebble.stormcrawler.persistence.Status;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.node.ArrayNode;
import com.fasterxml.jackson.databind.node.ObjectNode;
import java.io.ByteArrayInputStream;
import java.io.File;
import java.io.IOException;
import java.io.InputStream;
import java.nio.charset.StandardCharsets;
import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import org.apache.commons.io.FileUtils;
import org.apache.commons.lang.StringUtils;
import org.apache.storm.Config;
import org.apache.storm.metric.api.MultiCountMetric;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.tuple.Values;
import org.apache.storm.utils.TupleUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/** Writes documents to CloudSearch. */
@SuppressWarnings("serial")
public class CloudSearchIndexerBolt extends AbstractIndexerBolt {

    public static final Logger LOG = LoggerFactory.getLogger(CloudSearchIndexerBolt.class);

    private static final int MAX_SIZE_BATCH_BYTES = 5242880;
    private static final int MAX_SIZE_DOC_BYTES = 1048576;

    private static final SimpleDateFormat DATE_FORMAT =
            new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss.SSS'Z'");

    private AmazonCloudSearchDomainClient client;

    private int maxDocsInBatch = -1;

    private StringBuffer buffer;

    private int numDocsInBatch = 0;

    /** Max amount of time wait before indexing * */
    private int maxTimeBuffered = 10;

    private boolean dumpBatchFilesToTemp = false;

    private OutputCollector _collector;

    private MultiCountMetric eventCounter;

    private Map<String, String> csfields = new HashMap<>();

    private long timeLastBatchSent = System.currentTimeMillis();

    private List<Tuple> unacked = new ArrayList<>();

    @SuppressWarnings({"rawtypes", "unchecked"})
    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
        super.prepare(conf, context, collector);
        _collector = collector;

        this.eventCounter =
                context.registerMetric("CloudSearchIndexer", new MultiCountMetric(), 10);

        maxTimeBuffered = ConfUtils.getInt(conf, CloudSearchConstants.MAX_TIME_BUFFERED, 10);

        maxDocsInBatch = ConfUtils.getInt(conf, CloudSearchConstants.MAX_DOCS_BATCH, -1);

        buffer = new StringBuffer(MAX_SIZE_BATCH_BYTES).append('[');

        dumpBatchFilesToTemp = ConfUtils.getBoolean(conf, "cloudsearch.batch.dump", false);

        if (dumpBatchFilesToTemp) {
            // only dumping to local file
            // no more config required
            return;
        }

        String endpoint = ConfUtils.getString(conf, "cloudsearch.endpoint");

        if (StringUtils.isBlank(endpoint)) {
            String message = "Missing CloudSearch endpoint";
            LOG.error(message);
            throw new RuntimeException(message);
        }

        String regionName = ConfUtils.getString(conf, CloudSearchConstants.REGION);

        AmazonCloudSearchClient cl = new AmazonCloudSearchClient();
        if (StringUtils.isNotBlank(regionName)) {
            cl.setRegion(RegionUtils.getRegion(regionName));
        }

        String domainName = null;

        // retrieve the domain name
        DescribeDomainsResult domains = cl.describeDomains(new DescribeDomainsRequest());

        Iterator<DomainStatus> dsiter = domains.getDomainStatusList().iterator();
        while (dsiter.hasNext()) {
            DomainStatus ds = dsiter.next();
            if (ds.getDocService().getEndpoint().equals(endpoint)) {
                domainName = ds.getDomainName();
                break;
            }
        }
        // check domain name
        if (StringUtils.isBlank(domainName)) {
            throw new RuntimeException("No domain name found for CloudSearch endpoint");
        }

        DescribeIndexFieldsResult indexDescription =
                cl.describeIndexFields(new DescribeIndexFieldsRequest().withDomainName(domainName));
        for (IndexFieldStatus ifs : indexDescription.getIndexFields()) {
            String indexname = ifs.getOptions().getIndexFieldName();
            String indextype = ifs.getOptions().getIndexFieldType();
            LOG.info("CloudSearch index name {} of type {}", indexname, indextype);
            csfields.put(indexname, indextype);
        }

        client = new AmazonCloudSearchDomainClient();
        client.setEndpoint(endpoint);
    }

    @Override
    public void execute(Tuple tuple) {

        if (TupleUtils.isTick(tuple)) {
            // check when we last sent a batch
            long now = System.currentTimeMillis();
            long gap = now - timeLastBatchSent;
            if (gap >= maxTimeBuffered * 1000) {
                sendBatch();
            }
            _collector.ack(tuple);
            return;
        }

        String url = tuple.getStringByField("url");
        // Distinguish the value used for indexing
        // from the one used for the status
        String normalisedurl = valueForURL(tuple);

        Metadata metadata = (Metadata) tuple.getValueByField("metadata");
        String text = tuple.getStringByField("text");

        boolean keep = filterDocument(metadata);
        if (!keep) {
            eventCounter.scope("Filtered").incrBy(1);
            // treat it as successfully processed even if
            // we do not index it
            _collector.emit(StatusStreamName, tuple, new Values(url, metadata, Status.FETCHED));
            _collector.ack(tuple);
            return;
        }

        try {
            ObjectMapper objectMapper = new ObjectMapper();
            ObjectNode doc_builder = objectMapper.createObjectNode();

            doc_builder.put("type", "add");

            // generate the id from the normalised url
            String ID = CloudSearchUtils.getID(normalisedurl);
            doc_builder.put("id", ID);

            ObjectNode fields = objectMapper.createObjectNode();

            // which metadata to include as fields
            Map<String, String[]> keyVals = filterMetadata(metadata);

            for (final Entry<String, String[]> e : keyVals.entrySet()) {
                String fieldname = CloudSearchUtils.cleanFieldName(e.getKey());
                String type = csfields.get(fieldname);

                // undefined in index
                if (type == null && !this.dumpBatchFilesToTemp) {
                    LOG.info(
                            "Field {} not defined in CloudSearch domain for {} - skipping.",
                            fieldname,
                            url);
                    continue;
                }

                String[] values = e.getValue();

                // check that there aren't multiple values if not defined so in
                // the index
                if (values.length > 1 && !StringUtils.containsIgnoreCase(type, "-array")) {
                    LOG.info(
                            "{} values found for field {} of type {} - keeping only the first one. {}",
                            values.length,
                            fieldname,
                            type,
                            url);
                    values = new String[] {values[0]};
                }

                ArrayNode arrayNode = doc_builder.arrayNode();

                // write the values
                for (String value : values) {
                    // Check that the date format is correct
                    if (StringUtils.containsIgnoreCase(type, "date")) {
                        try {
                            DATE_FORMAT.parse(value);
                        } catch (ParseException pe) {
                            LOG.info("Unparsable date {}", value);
                            continue;
                        }
                    }
                    // normalise strings
                    else {
                        value = CloudSearchUtils.stripNonCharCodepoints(value);
                    }

                    arrayNode.add(value);
                }

                if (arrayNode.size() > 0) {
                    doc_builder.set(fieldname, arrayNode);
                }
            }

            // include the url ?
            String fieldNameForURL = fieldNameForURL();
            if (StringUtils.isNotBlank(fieldNameForURL)) {
                fieldNameForURL = CloudSearchUtils.cleanFieldName(fieldNameForURL);
                if (this.dumpBatchFilesToTemp || csfields.get(fieldNameForURL) != null) {
                    String _url = CloudSearchUtils.stripNonCharCodepoints(normalisedurl);
                    fields.put(fieldNameForURL, _url);
                }
            }

            // include the text ?
            String fieldNameForText = fieldNameForText();
            if (StringUtils.isNotBlank(fieldNameForText)) {
                fieldNameForText = CloudSearchUtils.cleanFieldName(fieldNameForText);
                if (this.dumpBatchFilesToTemp || csfields.get(fieldNameForText) != null) {
                    text = CloudSearchUtils.stripNonCharCodepoints(trimText(text));
                    fields.put(fieldNameForText, text);
                }
            }

            doc_builder.set("fields", fields);

            addToBatch(objectMapper.writeValueAsString(doc_builder), url, tuple);

        } catch (Exception e) {
            LOG.error("Exception caught while building JSON object", e);
            // resending would produce the same results no point in retrying
            _collector.emit(StatusStreamName, tuple, new Values(url, metadata, Status.ERROR));
            _collector.ack(tuple);
        }
    }

    private void addToBatch(String currentDoc, String url, Tuple tuple) {
        int currentDocLength = currentDoc.getBytes(StandardCharsets.UTF_8).length;

        // check that the doc is not too large -> skip it if it does
        if (currentDocLength > MAX_SIZE_DOC_BYTES) {
            LOG.error("Doc too large. currentDoc.length {} : {}", currentDocLength, url);
            return;
        }

        int currentBufferLength = buffer.toString().getBytes(StandardCharsets.UTF_8).length;

        LOG.debug("currentDoc.length {}, buffer length {}", currentDocLength, currentBufferLength);

        // can add it to the buffer without overflowing?
        if (currentDocLength + 2 + currentBufferLength < MAX_SIZE_BATCH_BYTES) {
            if (numDocsInBatch != 0) buffer.append(',');
            buffer.append(currentDoc);
            this.unacked.add(tuple);
            numDocsInBatch++;
        }
        // flush the previous batch and create a new one with this doc
        else {
            sendBatch();
            buffer.append(currentDoc);
            this.unacked.add(tuple);
            numDocsInBatch++;
        }

        // have we reached the max number of docs in a batch after adding
        // this doc?
        if (maxDocsInBatch > 0 && numDocsInBatch == maxDocsInBatch) {
            sendBatch();
        }
    }

    public void sendBatch() {

        timeLastBatchSent = System.currentTimeMillis();

        // nothing to do
        if (numDocsInBatch == 0) {
            return;
        }

        // close the array
        buffer.append(']');

        LOG.info("Sending {} docs to CloudSearch", numDocsInBatch);

        byte[] bb = buffer.toString().getBytes(StandardCharsets.UTF_8);

        if (dumpBatchFilesToTemp) {
            try {
                File temp = File.createTempFile("CloudSearch_", ".json");
                FileUtils.writeByteArrayToFile(temp, bb);
                LOG.info("Wrote batch file {}", temp.getName());
                // ack the tuples
                for (Tuple t : unacked) {
                    String url = t.getStringByField("url");
                    Metadata metadata = (Metadata) t.getValueByField("metadata");
                    _collector.emit(StatusStreamName, t, new Values(url, metadata, Status.FETCHED));
                    _collector.ack(t);
                }
                unacked.clear();
            } catch (IOException e1) {
                LOG.error("Exception while generating batch file", e1);
                // fail the tuples
                for (Tuple t : unacked) {
                    _collector.fail(t);
                }
                unacked.clear();
            } finally {
                // reset buffer and doc counter
                buffer = new StringBuffer(MAX_SIZE_BATCH_BYTES).append('[');
                numDocsInBatch = 0;
            }
            return;
        }
        // not in debug mode
        try (InputStream inputStream = new ByteArrayInputStream(bb)) {
            UploadDocumentsRequest batch = new UploadDocumentsRequest();
            batch.setContentLength((long) bb.length);
            batch.setContentType(ContentType.Applicationjson);
            batch.setDocuments(inputStream);
            UploadDocumentsResult result = client.uploadDocuments(batch);
            LOG.info(result.getStatus());
            for (DocumentServiceWarning warning : result.getWarnings()) {
                LOG.info(warning.getMessage());
            }
            if (!result.getWarnings().isEmpty()) {
                eventCounter.scope("Warnings").incrBy(result.getWarnings().size());
            }
            eventCounter.scope("Added").incrBy(result.getAdds());
            // ack the tuples
            for (Tuple t : unacked) {
                String url = t.getStringByField("url");
                Metadata metadata = (Metadata) t.getValueByField("metadata");
                _collector.emit(StatusStreamName, t, new Values(url, metadata, Status.FETCHED));
                _collector.ack(t);
            }
            unacked.clear();
        } catch (Exception e) {
            LOG.error("Exception while sending batch", e);
            LOG.error(buffer.toString());
            // fail the tuples
            for (Tuple t : unacked) {
                _collector.fail(t);
            }
            unacked.clear();
        } finally {
            // reset buffer and doc counter
            buffer = new StringBuffer(MAX_SIZE_BATCH_BYTES).append('[');
            numDocsInBatch = 0;
        }
    }

    @Override
    public void cleanup() {
        // This will flush any unsent documents.
        sendBatch();
        client.shutdown();
    }

    @Override
    public Map<String, Object> getComponentConfiguration() {
        Config conf = new Config();
        conf.put(Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS, 1);
        return conf;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.aws.bolt;

import java.nio.charset.StandardCharsets;
import java.security.MessageDigest;
import java.security.NoSuchAlgorithmException;
import java.util.regex.Pattern;
import org.apache.commons.codec.binary.Hex;

public class CloudSearchUtils {

    private static MessageDigest digester;

    private static final Pattern INVALID_XML_CHARS =
            Pattern.compile("[^\\u0009\\u000A\\u000D\\u0020-\\uD7FF\\uE000-\\uFFFD]");

    static {
        try {
            digester = MessageDigest.getInstance("SHA-512");
        } catch (NoSuchAlgorithmException e) {
            throw new RuntimeException(e);
        }
    }

    private CloudSearchUtils() {}

    /** Returns a normalised doc ID based on the URL of a document * */
    public static String getID(String url) {

        // the document needs an ID
        // see
        // http://docs.aws.amazon.com/cloudsearch/latest/developerguide/preparing-data.html#creating-document-batches
        // A unique ID for the document. A document ID can contain any
        // letter or number and the following characters: _ - = # ; : / ? @
        // &. Document IDs must be at least 1 and no more than 128
        // characters long.
        byte[] dig = digester.digest(url.getBytes(StandardCharsets.UTF_8));
        String ID = Hex.encodeHexString(dig);
        // is that even possible?
        if (ID.length() > 128) {
            throw new RuntimeException("ID larger than max 128 chars");
        }
        return ID;
    }

    public static String stripNonCharCodepoints(String input) {
        return INVALID_XML_CHARS.matcher(input).replaceAll("");
    }

    /**
     * Remove the non-cloudSearch-legal characters. Note that this might convert two fields to the
     * same name.
     *
     * @see <a
     *     href="http://docs.aws.amazon.com/cloudsearch/latest/developerguide/configuring-index-fields.html">
     *     configuring-index-fields.html</a>
     * @param name
     * @return
     */
    public static String cleanFieldName(String name) {
        String lowercase = name.toLowerCase();
        lowercase = lowercase.replaceAll("[^a-z_0-9]", "_");
        if (lowercase.length() < 3 || lowercase.length() > 64)
            throw new RuntimeException("Field name must be between 3 and 64 chars : " + lowercase);
        if (lowercase.equals("score")) throw new RuntimeException("Field name must be score");
        return lowercase;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.solr.metrics;

import com.digitalpebble.stormcrawler.solr.SolrConnection;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import java.text.DateFormat;
import java.text.SimpleDateFormat;
import java.util.Collection;
import java.util.Date;
import java.util.Iterator;
import java.util.Map;
import java.util.Map.Entry;
import org.apache.solr.common.SolrInputDocument;
import org.apache.storm.metric.api.IMetricsConsumer;
import org.apache.storm.task.IErrorReporter;
import org.apache.storm.task.TopologyContext;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class MetricsConsumer implements IMetricsConsumer {

    private final Logger LOG = LoggerFactory.getLogger(MetricsConsumer.class);

    private final DateFormat df = new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss'Z'");

    private static final String BOLT_TYPE = "metrics";

    private static final String SolrTTLParamName = "solr.metrics.ttl";
    private static final String SolrTTLFieldParamName = "solr.metrics.ttl.field";

    private String ttlField;
    private String ttl;

    private SolrConnection connection;

    @Override
    public void prepare(
            Map stormConf,
            Object registrationArgument,
            TopologyContext topologyContext,
            IErrorReporter errorReporter) {

        ttlField = ConfUtils.getString(stormConf, SolrTTLFieldParamName, "__ttl__");
        ttl = ConfUtils.getString(stormConf, SolrTTLParamName, null);

        try {
            connection = SolrConnection.getConnection(stormConf, BOLT_TYPE);
        } catch (Exception e) {
            LOG.error("Can't connect to Solr: {}", e);
            throw new RuntimeException(e);
        }
    }

    @Override
    public void handleDataPoints(TaskInfo taskInfo, Collection<DataPoint> dataPoints) {
        final Date now = new Date();
        for (DataPoint dataPoint : dataPoints) {
            handleDataPoints(taskInfo, dataPoint.name, dataPoint.value, now);
        }
    }

    private void handleDataPoints(
            final TaskInfo taskInfo, final String nameprefix, final Object value, final Date now) {
        if (value instanceof Number) {
            indexDataPoint(taskInfo, now, nameprefix, ((Number) value).doubleValue());
        } else if (value instanceof Map) {
            Iterator<Entry> keyValiter = ((Map) value).entrySet().iterator();
            while (keyValiter.hasNext()) {
                Entry entry = keyValiter.next();
                String newnameprefix = nameprefix + "." + entry.getKey();
                handleDataPoints(taskInfo, newnameprefix, entry.getValue(), now);
            }
        } else if (value instanceof Collection) {
            for (Object collectionObj : (Collection) value) {
                handleDataPoints(taskInfo, nameprefix, collectionObj, now);
            }
        } else {
            LOG.warn("Found data point value {} of {}", nameprefix, value.getClass().toString());
        }
    }

    private void indexDataPoint(TaskInfo taskInfo, Date timestamp, String name, double value) {
        try {
            SolrInputDocument doc = new SolrInputDocument();

            doc.addField("srcComponentId", taskInfo.srcComponentId);
            doc.addField("srcTaskId", taskInfo.srcTaskId);
            doc.addField("srcWorkerHost", taskInfo.srcWorkerHost);
            doc.addField("srcWorkerPort", taskInfo.srcWorkerPort);
            doc.addField("name", name);
            doc.addField("value", value);

            String ftmp = df.format(timestamp);
            doc.addField("timestamp", ftmp);

            if (this.ttl != null) {
                doc.addField(ttlField, ttl);
            }

            connection.getClient().add(doc);
        } catch (Exception e) {
            LOG.error("Problem building a document to Solr", e);
        }
    }

    @Override
    public void cleanup() {
        if (connection != null) {
            try {
                connection.close();
            } catch (Exception e) {
                LOG.error("Can't close connection to Solr", e);
            }
        }
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.solr.persistence;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.persistence.AbstractStatusUpdaterBolt;
import com.digitalpebble.stormcrawler.persistence.Status;
import com.digitalpebble.stormcrawler.solr.SolrConnection;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import com.digitalpebble.stormcrawler.util.URLUtil;
import java.util.Date;
import java.util.Iterator;
import java.util.Map;
import java.util.Optional;
import org.apache.solr.common.SolrInputDocument;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.tuple.Tuple;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class StatusUpdaterBolt extends AbstractStatusUpdaterBolt {

    private static final Logger LOG = LoggerFactory.getLogger(StatusUpdaterBolt.class);

    private static final String BOLT_TYPE = "status";

    private static final String SolrMetadataPrefix = "solr.status.metadata.prefix";

    private String mdPrefix;

    private SolrConnection connection;

    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {

        super.prepare(stormConf, context, collector);

        mdPrefix = ConfUtils.getString(stormConf, SolrMetadataPrefix, "metadata");

        try {
            connection = SolrConnection.getConnection(stormConf, BOLT_TYPE);
        } catch (Exception e) {
            LOG.error("Can't connect to Solr: {}", e);
            throw new RuntimeException(e);
        }
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {}

    @Override
    public void store(
            String url, Status status, Metadata metadata, Optional<Date> nextFetch, Tuple t)
            throws Exception {

        SolrInputDocument doc = new SolrInputDocument();

        doc.setField("url", url);

        doc.setField("host", URLUtil.getHost(url));

        doc.setField("status", status.name());

        Iterator<String> keyIterator = metadata.keySet().iterator();
        while (keyIterator.hasNext()) {
            String key = keyIterator.next();
            String[] values = metadata.getValues(key);
            doc.setField(String.format("%s.%s", mdPrefix, key), values);
        }

        if (nextFetch.isPresent()) {
            doc.setField("nextFetchDate", nextFetch.get());
        }

        connection.getClient().add(doc);

        super.ack(t, url);
    }

    @Override
    public void cleanup() {
        if (connection != null) {
            try {
                connection.close();
            } catch (Exception e) {
                LOG.error("Can't close connection to Solr: {}", e);
            }
        }
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.solr.persistence;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.persistence.AbstractQueryingSpout;
import com.digitalpebble.stormcrawler.solr.SolrConnection;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import java.time.Instant;
import java.util.Collection;
import java.util.Iterator;
import java.util.Map;
import org.apache.commons.lang.StringUtils;
import org.apache.solr.client.solrj.SolrQuery;
import org.apache.solr.client.solrj.response.QueryResponse;
import org.apache.solr.common.SolrDocument;
import org.apache.solr.common.SolrDocumentList;
import org.apache.storm.spout.SpoutOutputCollector;
import org.apache.storm.task.TopologyContext;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class SolrSpout extends AbstractQueryingSpout {

    private static final Logger LOG = LoggerFactory.getLogger(SolrSpout.class);

    private static final String BOLT_TYPE = "status";

    private static final String SolrDiversityFieldParam = "solr.status.bucket.field";
    private static final String SolrDiversityBucketParam = "solr.status.bucket.maxsize";
    private static final String SolrMetadataPrefix = "solr.status.metadata.prefix";
    private static final String SolrMaxResultsParam = "solr.status.max.results";

    private SolrConnection connection;

    private int maxNumResults = 10;

    private int lastStartOffset = 0;

    private Instant lastNextFetchDate = null;

    private String diversityField = null;

    private int diversityBucketSize = 0;

    private String mdPrefix;

    @Override
    public void open(Map stormConf, TopologyContext context, SpoutOutputCollector collector) {

        super.open(stormConf, context, collector);

        // This implementation works only where there is a single instance
        // of the spout. Having more than one instance means that they would run
        // the same queries and send the same tuples down the topology.

        int totalTasks = context.getComponentTasks(context.getThisComponentId()).size();
        if (totalTasks > 1) {
            throw new RuntimeException("Can't have more than one instance of SOLRSpout");
        }

        diversityField = ConfUtils.getString(stormConf, SolrDiversityFieldParam);
        diversityBucketSize = ConfUtils.getInt(stormConf, SolrDiversityBucketParam, 5);
        // the results have the first hit separate from the expansions
        diversityBucketSize--;

        mdPrefix = ConfUtils.getString(stormConf, SolrMetadataPrefix, "metadata");

        maxNumResults = ConfUtils.getInt(stormConf, SolrMaxResultsParam, 10);

        try {
            connection = SolrConnection.getConnection(stormConf, BOLT_TYPE);
        } catch (Exception e) {
            LOG.error("Can't connect to Solr: {}", e);
            throw new RuntimeException(e);
        }
    }

    @Override
    public void close() {
        if (connection != null) {
            try {
                connection.close();
            } catch (Exception e) {
                LOG.error("Can't close connection to Solr: {}", e);
            }
        }
    }

    protected void populateBuffer() {

        SolrQuery query = new SolrQuery();

        if (lastNextFetchDate == null) {
            lastNextFetchDate = Instant.now();
            lastStartOffset = 0;
            lastTimeResetToNOW = Instant.now();
        }
        // reset the value for next fetch date if the previous one is too
        // old
        else if (resetFetchDateAfterNSecs != -1) {
            Instant changeNeededOn =
                    Instant.ofEpochMilli(
                            lastTimeResetToNOW.toEpochMilli() + (resetFetchDateAfterNSecs * 1000));
            if (Instant.now().isAfter(changeNeededOn)) {
                LOG.info(
                        "lastDate reset based on resetFetchDateAfterNSecs {}",
                        resetFetchDateAfterNSecs);
                lastNextFetchDate = Instant.now();
                lastStartOffset = 0;
            }
        }

        query.setQuery("*:*")
                .addFilterQuery("nextFetchDate:[* TO " + lastNextFetchDate + "]")
                .setStart(lastStartOffset)
                .setRows(this.maxNumResults);

        if (StringUtils.isNotBlank(diversityField) && diversityBucketSize > 0) {
            query.addFilterQuery(
                    String.format("{!collapse field=%s sort='nextFetchDate asc'}", diversityField));
            query.set("expand", "true").set("expand.rows", diversityBucketSize);
            query.set("expand.sort", "nextFetchDate asc");
        }

        LOG.debug("QUERY => {}", query.toString());

        try {
            long startQuery = System.currentTimeMillis();
            QueryResponse response = connection.getClient().query(query);
            long endQuery = System.currentTimeMillis();

            queryTimes.addMeasurement(endQuery - startQuery);

            SolrDocumentList docs = new SolrDocumentList();

            LOG.debug("Response : {}", response.toString());

            // add the main results
            docs.addAll(response.getResults());

            // Add the documents collapsed by the CollapsingQParser
            Map<String, SolrDocumentList> expandedResults = response.getExpandedResults();
            if (StringUtils.isNotBlank(diversityField) && expandedResults != null) {
                for (String key : expandedResults.keySet()) {
                    docs.addAll(expandedResults.get(key));
                }
            }

            int numhits = response.getResults().size();

            // no more results?
            if (numhits == 0) {
                lastStartOffset = 0;
                lastNextFetchDate = null;
            } else {
                lastStartOffset += numhits;
            }

            String prefix = mdPrefix.concat(".");

            int alreadyProcessed = 0;
            int docReturned = 0;

            for (SolrDocument doc : docs) {
                String url = (String) doc.get("url");

                docReturned++;

                // is already being processed - skip it!
                if (beingProcessed.containsKey(url)) {
                    alreadyProcessed++;
                    continue;
                }

                Metadata metadata = new Metadata();

                Iterator<String> keyIterators = doc.getFieldNames().iterator();
                while (keyIterators.hasNext()) {
                    String key = keyIterators.next();

                    if (key.startsWith(prefix)) {
                        Collection<Object> values = doc.getFieldValues(key);

                        key = key.substring(prefix.length());
                        Iterator<Object> valueIterator = values.iterator();
                        while (valueIterator.hasNext()) {
                            String value = (String) valueIterator.next();
                            metadata.addValue(key, value);
                        }
                    }
                }

                buffer.add(url, metadata);
            }

            LOG.info(
                    "SOLR returned {} results from {} buckets in {} msec including {} already being processed",
                    docReturned,
                    numhits,
                    (endQuery - startQuery),
                    alreadyProcessed);

        } catch (Exception e) {
            LOG.error("Exception while querying Solr", e);
        }
    }

    @Override
    public void ack(Object msgId) {
        LOG.debug("Ack for {}", msgId);
        super.ack(msgId);
    }

    @Override
    public void fail(Object msgId) {
        LOG.info("Fail for {}", msgId);
        super.fail(msgId);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.solr;

import com.digitalpebble.stormcrawler.util.ConfUtils;
import java.io.IOException;
import java.util.Map;
import org.apache.solr.client.solrj.SolrClient;
import org.apache.solr.client.solrj.SolrServerException;
import org.apache.solr.client.solrj.impl.CloudSolrClient;
import org.apache.solr.client.solrj.impl.ConcurrentUpdateSolrClient;
import org.apache.solr.client.solrj.impl.HttpSolrClient;
import org.apache.solr.client.solrj.request.UpdateRequest;
import org.apache.storm.shade.org.apache.commons.lang.StringUtils;

@SuppressWarnings("serial")
public class SolrConnection {

    private SolrClient client;
    private UpdateRequest request;

    private SolrConnection(SolrClient sc, UpdateRequest r) {
        client = sc;
        request = r;
    }

    public SolrClient getClient() {
        return client;
    }

    public UpdateRequest getRequest() {
        return request;
    }

    public static SolrClient getClient(Map stormConf, String boltType) {
        String zkHost = ConfUtils.getString(stormConf, "solr." + boltType + ".zkhost", null);
        String solrUrl = ConfUtils.getString(stormConf, "solr." + boltType + ".url", null);
        String collection =
                ConfUtils.getString(stormConf, "solr." + boltType + ".collection", null);
        int queueSize = ConfUtils.getInt(stormConf, "solr." + boltType + ".queueSize", -1);

        SolrClient client;

        if (StringUtils.isNotBlank(zkHost)) {
            client = new CloudSolrClient.Builder().withZkHost(zkHost).build();
            if (StringUtils.isNotBlank(collection)) {
                ((CloudSolrClient) client).setDefaultCollection(collection);
            }
        } else if (StringUtils.isNotBlank(solrUrl)) {
            if (queueSize == -1) {
                client = new HttpSolrClient.Builder(solrUrl).build();
            } else {
                client =
                        new ConcurrentUpdateSolrClient.Builder(solrUrl)
                                .withQueueSize(queueSize)
                                .build();
            }
        } else {
            throw new RuntimeException("SolrClient should have zk or solr URL set up");
        }

        return client;
    }

    public static UpdateRequest getRequest(Map stormConf, String boltType) {
        int commitWithin = ConfUtils.getInt(stormConf, "solr." + boltType + ".commit.within", -1);

        UpdateRequest request = new UpdateRequest();

        if (commitWithin != -1) {
            request.setCommitWithin(commitWithin);
        }

        return request;
    }

    public static SolrConnection getConnection(Map stormConf, String boltType) {
        SolrClient client = getClient(stormConf, boltType);
        UpdateRequest request = getRequest(stormConf, boltType);

        return new SolrConnection(client, request);
    }

    public void close() throws IOException, SolrServerException {
        if (client != null) {
            client.commit();
            client.close();
        }
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.solr.bolt;

import static com.digitalpebble.stormcrawler.Constants.StatusStreamName;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.indexing.AbstractIndexerBolt;
import com.digitalpebble.stormcrawler.persistence.Status;
import com.digitalpebble.stormcrawler.solr.SolrConnection;
import java.util.Iterator;
import java.util.Map;
import org.apache.commons.lang.StringUtils;
import org.apache.solr.common.SolrInputDocument;
import org.apache.storm.metric.api.MultiCountMetric;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.tuple.Values;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class IndexerBolt extends AbstractIndexerBolt {

    private static final Logger LOG = LoggerFactory.getLogger(IndexerBolt.class);

    private static final String BOLT_TYPE = "indexer";

    private OutputCollector _collector;

    private MultiCountMetric eventCounter;

    private SolrConnection connection;

    @SuppressWarnings({"unchecked", "rawtypes"})
    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
        super.prepare(conf, context, collector);

        _collector = collector;

        try {
            connection = SolrConnection.getConnection(conf, BOLT_TYPE);
        } catch (Exception e) {
            LOG.error("Can't connect to Solr: {}", e);
            throw new RuntimeException(e);
        }

        this.eventCounter = context.registerMetric("SolrIndexerBolt", new MultiCountMetric(), 10);
    }

    @Override
    public void cleanup() {
        if (connection != null)
            try {
                connection.close();
            } catch (Exception e) {
                LOG.error("Can't close connection to Solr: {}", e);
            }
    }

    @Override
    public void execute(Tuple tuple) {

        String url = tuple.getStringByField("url");

        Metadata metadata = (Metadata) tuple.getValueByField("metadata");

        boolean keep = filterDocument(metadata);
        if (!keep) {
            eventCounter.scope("Filtered").incrBy(1);
            // treat it as successfully processed even if
            // we do not index it
            _collector.emit(StatusStreamName, tuple, new Values(url, metadata, Status.FETCHED));
            _collector.ack(tuple);
            return;
        }

        try {
            SolrInputDocument doc = new SolrInputDocument();

            // index text content
            if (StringUtils.isNotBlank(fieldNameForText())) {
                String text = tuple.getStringByField("text");
                doc.addField(fieldNameForText(), trimText(text));
            }

            // url
            if (StringUtils.isNotBlank(fieldNameForURL())) {
                // Distinguish the value used for indexing
                // from the one used for the status
                String normalisedurl = valueForURL(tuple);
                doc.addField(fieldNameForURL(), normalisedurl);
            }

            // select which metadata to index
            Map<String, String[]> keyVals = filterMetadata(metadata);

            Iterator<String> iterator = keyVals.keySet().iterator();
            while (iterator.hasNext()) {
                String fieldName = iterator.next();
                String[] values = keyVals.get(fieldName);
                for (String value : values) {
                    doc.addField(fieldName, value);
                }
            }

            connection.getClient().add(doc);

            eventCounter.scope("Indexed").incrBy(1);

            _collector.emit(StatusStreamName, tuple, new Values(url, metadata, Status.FETCHED));
            _collector.ack(tuple);

        } catch (Exception e) {
            LOG.error("Send update request to SOLR failed due to {}", e);
            _collector.fail(tuple);
        }
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.solr;

import com.digitalpebble.stormcrawler.ConfigurableTopology;
import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.bolt.FetcherBolt;
import com.digitalpebble.stormcrawler.bolt.JSoupParserBolt;
import com.digitalpebble.stormcrawler.bolt.SiteMapParserBolt;
import com.digitalpebble.stormcrawler.bolt.URLPartitionerBolt;
import com.digitalpebble.stormcrawler.solr.bolt.IndexerBolt;
import com.digitalpebble.stormcrawler.solr.metrics.MetricsConsumer;
import com.digitalpebble.stormcrawler.solr.persistence.SolrSpout;
import com.digitalpebble.stormcrawler.solr.persistence.StatusUpdaterBolt;
import org.apache.storm.topology.TopologyBuilder;
import org.apache.storm.tuple.Fields;

/** Dummy topology to play with the spouts and bolts on Solr */
public class SolrCrawlTopology extends ConfigurableTopology {

    public static void main(String[] args) throws Exception {
        ConfigurableTopology.start(new SolrCrawlTopology(), args);
    }

    @Override
    protected int run(String[] args) {
        TopologyBuilder builder = new TopologyBuilder();

        builder.setSpout("spout", new SolrSpout());

        builder.setBolt("partitioner", new URLPartitionerBolt()).shuffleGrouping("spout");

        builder.setBolt("fetch", new FetcherBolt())
                .fieldsGrouping("partitioner", new Fields("key"));

        builder.setBolt("sitemap", new SiteMapParserBolt()).localOrShuffleGrouping("fetch");

        builder.setBolt("parse", new JSoupParserBolt()).localOrShuffleGrouping("sitemap");

        builder.setBolt("indexer", new IndexerBolt()).localOrShuffleGrouping("parse");

        builder.setBolt("status", new StatusUpdaterBolt())
                .localOrShuffleGrouping("fetch", Constants.StatusStreamName)
                .localOrShuffleGrouping("sitemap", Constants.StatusStreamName)
                .localOrShuffleGrouping("parse", Constants.StatusStreamName)
                .localOrShuffleGrouping("indexer", Constants.StatusStreamName);

        conf.registerMetricsConsumer(MetricsConsumer.class);

        return submit("crawl", conf, builder);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.solr;

import com.digitalpebble.stormcrawler.ConfigurableTopology;
import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.solr.persistence.StatusUpdaterBolt;
import com.digitalpebble.stormcrawler.spout.FileSpout;
import org.apache.storm.topology.TopologyBuilder;
import org.apache.storm.tuple.Fields;

/**
 * Topology which reads from a file containing seeds and distributes to SQS queues based on the IP /
 * hostname / domain of the URLs. Used in local mode to bootstrap a crawl.
 */
public class SeedInjector extends ConfigurableTopology {

    public static void main(String[] args) throws Exception {
        ConfigurableTopology.start(new SeedInjector(), args);
    }

    @Override
    public int run(String[] args) {

        if (args.length == 0) {
            System.err.println("SeedInjector seed_dir file_filter");
            return -1;
        }

        conf.setDebug(false);

        TopologyBuilder builder = new TopologyBuilder();

        builder.setSpout("spout", new FileSpout(args[0], args[1], true));

        Fields key = new Fields("url");

        builder.setBolt("enqueue", new StatusUpdaterBolt())
                .fieldsGrouping("spout", Constants.StatusStreamName, key);

        return submit("SeedInjector", conf, builder);
    }
}

Parse Compilation Unit:
package com.digitalpebble.stormcrawler.warc;

import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertFalse;
import static org.junit.Assert.assertTrue;
import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.protocol.ProtocolResponse;
import java.nio.charset.StandardCharsets;
import org.apache.storm.tuple.Tuple;
import org.junit.Test;

public class WARCRecordFormatTest {

    private String protocolMDprefix = "http.";

    @Test
    public void testGetDigestSha1() {
        byte[] content = {'a', 'b', 'c', 'd', 'e', 'f'};
        String sha1str = "sha1:D6FMCDZDYW23YELHXWUEXAZ6LQCXU56S";
        assertEquals("Wrong sha1 digest", sha1str, WARCRecordFormat.getDigestSha1(content));
    }

    @Test
    public void testGetDigestSha1Empty() {
        byte[] content = {};
        String sha1str = "sha1:3I42H3S6NNFQ2MSVX7XZKYAYSCX5QBYJ";
        assertEquals("Wrong sha1 digest", sha1str, WARCRecordFormat.getDigestSha1(content));
    }

    @Test
    public void testGetDigestSha1TwoByteArrays() {
        byte[] content1 = {'a', 'b', 'c'};
        byte[] content2 = {'d', 'e', 'f'};
        String sha1str = "sha1:D6FMCDZDYW23YELHXWUEXAZ6LQCXU56S";
        assertEquals(
                "Wrong sha1 digest", sha1str, WARCRecordFormat.getDigestSha1(content1, content2));
    }

    @Test
    public void testGetDigestSha1RobotsTxt() {
        // trivial robots.txt file, sha1 from WARC file written by Nutch
        String robotsTxt = "User-agent: *\r\nDisallow:";
        byte[] content = robotsTxt.getBytes(StandardCharsets.UTF_8);
        String sha1str = "sha1:DHBVNHAJABWFHIYUHNCKYYIB3OBPFX3Y";
        assertEquals("Wrong sha1 digest", sha1str, WARCRecordFormat.getDigestSha1(content));
    }

    @Test
    public void testWarcRecord() {
        // test validity of WARC record
        String txt = "abcdef";
        byte[] content = txt.getBytes(StandardCharsets.UTF_8);
        String sha1str = "sha1:D6FMCDZDYW23YELHXWUEXAZ6LQCXU56S";
        Metadata metadata = new Metadata();
        metadata.addValue(
                protocolMDprefix + ProtocolResponse.RESPONSE_HEADERS_KEY,
                "HTTP/1.1 200 OK\r\nContent-Type: text/html\r\n");
        Tuple tuple = mock(Tuple.class);
        when(tuple.getBinaryByField("content")).thenReturn(content);
        when(tuple.getStringByField("url")).thenReturn("https://www.example.org/");
        when(tuple.getValueByField("metadata")).thenReturn(metadata);
        WARCRecordFormat format = new WARCRecordFormat(protocolMDprefix);
        byte[] warcBytes = format.format(tuple);
        String warcString = new String(warcBytes, StandardCharsets.UTF_8);
        // the WARC record has the form:
        // WARC header
        // \r\n\r\n
        // HTTP header
        // \r\n\r\n
        // payload
        // \r\n\r\n
        assertTrue(
                "WARC record: incorrect format of WARC header", warcString.startsWith("WARC/1.0"));
        assertTrue(
                "WARC record: incorrect format of HTTP header",
                warcString.contains("\r\n\r\nHTTP/1.1 200 OK\r\n"));
        assertTrue(
                "WARC record: single empty line between HTTP header and payload",
                warcString.contains("Content-Type: text/html\r\n\r\nabcdef"));
        assertTrue(
                "WARC record: record is required to end with \\r\\n\\r\\n",
                warcString.endsWith("\r\n\r\n"));
        assertTrue("WARC record: payload mangled", warcString.endsWith("\r\n\r\nabcdef\r\n\r\n"));
        assertTrue(
                "WARC record: no or incorrect payload digest",
                warcString.contains("\r\nWARC-Payload-Digest: " + sha1str + "\r\n"));
    }

    @Test
    public void testReplaceHeaders() {
        // test whether wrong/misleading HTTP headers are replaced
        // because payload is not saved in original Content-Encoding and
        // Transfer-Encoding
        String txt = "abcdef";
        byte[] content = txt.getBytes(StandardCharsets.UTF_8);
        String sha1str = "sha1:D6FMCDZDYW23YELHXWUEXAZ6LQCXU56S";
        Metadata metadata = new Metadata();
        metadata.addValue(
                protocolMDprefix + ProtocolResponse.RESPONSE_HEADERS_KEY, //
                "HTTP/1.1 200 OK\r\n" //
                        + "Content-Type: text/html\r\n" //
                        + "Content-Encoding: gzip\r\n" //
                        + "Content-Length: 26\r\n" //
                        + "Connection: close");
        Tuple tuple = mock(Tuple.class);
        when(tuple.getBinaryByField("content")).thenReturn(content);
        when(tuple.getStringByField("url")).thenReturn("https://www.example.org/");
        when(tuple.getValueByField("metadata")).thenReturn(metadata);
        WARCRecordFormat format = new WARCRecordFormat(protocolMDprefix);
        byte[] warcBytes = format.format(tuple);
        String warcString = new String(warcBytes, StandardCharsets.UTF_8);
        assertFalse(
                "WARC record: HTTP header Content-Encoding not replaced",
                warcString.contains("\r\nContent-Encoding: gzip\r\n"));
        assertFalse(
                "WARC record: HTTP header Content-Length not replaced",
                warcString.contains("\r\nContent-Length: 26\r\n"));
        // the correct Content-Length is 6 (txt = "abcdef")
        assertTrue(
                "WARC record: HTTP header Content-Length does not match payload length",
                warcString.contains("\r\nContent-Length: 6\r\n"));
        assertTrue(
                "WARC record: HTTP header does not end with \\r\\n\\r\\n",
                warcString.contains("\r\nConnection: close\r\n\r\nabcdef"));
    }

    @Test
    public void testWarcDateFormat() {
        Metadata metadata = new Metadata();
        /*
         * To meet the WARC 1.0 standard the ISO date format with seconds
         * precision (not milliseconds) is expected. We pass epoch millisecond 1
         * to the formatter to ensure that the precision isn't increased on
         * demand (as by the ISO_INSTANT formatter):
         */
        metadata.addValue(protocolMDprefix + ProtocolResponse.REQUEST_TIME_KEY, "1");
        WARCRecordFormat format = new WARCRecordFormat(protocolMDprefix);
        assertEquals("1970-01-01T00:00:00Z", format.getCaptureTime(metadata));
    }
}

Parse Compilation Unit:
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license
 * agreements. See the NOTICE file distributed with this work for additional information regarding
 * copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance with the License. You may obtain a
 * copy of the License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.warc;

import java.io.IOException;
import java.net.URI;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.storm.hdfs.bolt.AbstractHdfsBolt;
import org.apache.storm.hdfs.bolt.format.FileNameFormat;
import org.apache.storm.hdfs.bolt.format.RecordFormat;
import org.apache.storm.hdfs.bolt.rotation.FileRotationPolicy;
import org.apache.storm.hdfs.bolt.sync.SyncPolicy;
import org.apache.storm.hdfs.common.AbstractHDFSWriter;
import org.apache.storm.hdfs.common.HDFSWriter;
import org.apache.storm.hdfs.common.rotation.RotationAction;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.utils.Utils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/** Unlike the standard HdfsBolt this one writes to a gzipped stream with per-record compression. */
@SuppressWarnings("serial")
public class GzipHdfsBolt extends AbstractHdfsBolt {

    private static final Logger LOG = LoggerFactory.getLogger(GzipHdfsBolt.class);

    protected transient FSDataOutputStream out = null;

    protected RecordFormat format;

    public static class GzippedRecordFormat implements RecordFormat {

        private RecordFormat baseFormat;

        /**
         * whether to skip empty records (byte[] of length 0), i.e. do not create an gzip container
         * containing nothing
         */
        protected boolean compressEmpty = false;

        public GzippedRecordFormat(RecordFormat format) {
            baseFormat = format;
        }

        @Override
        public byte[] format(Tuple tuple) {
            byte[] bytes = baseFormat.format(tuple);
            if (bytes.length == 0 && !compressEmpty) {
                return new byte[] {};
            }
            return Utils.gzip(bytes);
        }
    }

    public static class MultipleRecordFormat implements RecordFormat {

        private List<RecordFormat> formats = new ArrayList<>();

        public MultipleRecordFormat(RecordFormat format) {
            addFormat(format, 0);
        }

        public void addFormat(RecordFormat format, int position) {
            if (position < 0 || position > formats.size()) {
                formats.add(format);
            } else {
                formats.add(position, format);
            }
        }

        @Override
        public byte[] format(Tuple tuple) {
            byte[][] tmp = new byte[formats.size()][];
            int i = -1;
            int size = 0;
            for (RecordFormat format : formats) {
                tmp[++i] = format.format(tuple);
                size += tmp[i].length;
            }
            byte[] res = new byte[size];
            int pos = 0;
            for (i = 0; i < tmp.length; i++) {
                System.arraycopy(tmp[i], 0, res, pos, tmp[i].length);
                pos += tmp[i].length;
            }
            return res;
        }
    }

    public GzipHdfsBolt withFsUrl(String fsUrl) {
        this.fsUrl = fsUrl;
        return this;
    }

    public GzipHdfsBolt withConfigKey(String configKey) {
        this.configKey = configKey;
        return this;
    }

    public GzipHdfsBolt withFileNameFormat(FileNameFormat fileNameFormat) {
        this.fileNameFormat = fileNameFormat;
        return this;
    }

    /** Sets the record format, overwriting the existing one(s) */
    public GzipHdfsBolt withRecordFormat(RecordFormat format) {
        this.format = new GzippedRecordFormat(format);
        return this;
    }

    /** Add an additional record format at end of existing ones */
    public GzipHdfsBolt addRecordFormat(RecordFormat format) {
        return addRecordFormat(format, -1);
    }

    /** Add an additional record format at given position */
    public GzipHdfsBolt addRecordFormat(RecordFormat format, int position) {
        MultipleRecordFormat formats;
        if (this.format == null) {
            formats = new MultipleRecordFormat(format);
            this.format = formats;
        } else {
            if (this.format instanceof MultipleRecordFormat) {
                formats = (MultipleRecordFormat) this.format;
            } else {
                formats = new MultipleRecordFormat(this.format);
                this.format = formats;
            }
            formats.addFormat(new GzippedRecordFormat(format), position);
        }
        return this;
    }

    public GzipHdfsBolt withSyncPolicy(SyncPolicy syncPolicy) {
        this.syncPolicy = syncPolicy;
        return this;
    }

    public GzipHdfsBolt withRotationPolicy(FileRotationPolicy rotationPolicy) {
        this.rotationPolicy = rotationPolicy;
        return this;
    }

    public GzipHdfsBolt addRotationAction(RotationAction action) {
        this.rotationActions.add(action);
        return this;
    }

    @Override
    public void doPrepare(Map conf, TopologyContext topologyContext, OutputCollector collector)
            throws IOException {
        LOG.info("Preparing HDFS Bolt...");
        this.fs = FileSystem.get(URI.create(this.fsUrl), hdfsConfig);
    }

    @Override
    public void cleanup() {
        LOG.info("Cleanup called on bolt");
        if (this.out == null) {
            LOG.warn("Nothing to cleanup: output stream not initialized");
            return;
        }
        try {
            this.out.flush();
            this.out.close();
        } catch (IOException e) {
            LOG.error("Exception while calling cleanup");
        }
    }

    @Override
    protected String getWriterKey(Tuple tuple) {
        return "CONSTANT";
    }

    @Override
    protected AbstractHDFSWriter makeNewWriter(Path path, Tuple tuple) throws IOException {
        out = this.fs.create(path);
        return new HDFSWriter(rotationPolicy, path, out, format);
    }
}

Parse Compilation Unit:
package com.digitalpebble.stormcrawler.warc;

import static com.digitalpebble.stormcrawler.protocol.ProtocolResponse.REQUEST_TIME_KEY;
import static com.digitalpebble.stormcrawler.protocol.ProtocolResponse.RESPONSE_HEADERS_KEY;
import static com.digitalpebble.stormcrawler.protocol.ProtocolResponse.RESPONSE_IP_KEY;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.protocol.HttpHeaders;
import com.digitalpebble.stormcrawler.protocol.ProtocolResponse;
import java.net.URI;
import java.nio.ByteBuffer;
import java.nio.charset.StandardCharsets;
import java.security.MessageDigest;
import java.time.DateTimeException;
import java.time.Instant;
import java.time.format.DateTimeFormatter;
import java.time.format.DateTimeFormatterBuilder;
import java.util.Iterator;
import java.util.Locale;
import java.util.Map;
import java.util.Map.Entry;
import java.util.UUID;
import java.util.regex.Pattern;
import org.apache.commons.codec.binary.Base32;
import org.apache.commons.codec.digest.DigestUtils;
import org.apache.commons.lang.StringUtils;
import org.apache.storm.hdfs.bolt.format.RecordFormat;
import org.apache.storm.tuple.Tuple;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/** Generate a byte representation of a WARC entry from a tuple * */
@SuppressWarnings("serial")
public class WARCRecordFormat implements RecordFormat {

    // WARC record types, cf.
    // http://iipc.github.io/warc-specifications/specifications/warc-format/warc-1.1/#warc-record-types
    /** WARC record type to hold a HTTP request */
    protected static final String WARC_TYPE_REQUEST = "request";
    /** WARC record type to hold a HTTP response */
    protected static final String WARC_TYPE_RESPONSE = "response";
    /**
     * WARC record type to hold any other resource, including a HTTP response with no HTTP headers
     * available
     */
    protected static final String WARC_TYPE_RESOURCE = "resource";

    protected static final String WARC_TYPE_WARCINFO = "warcinfo";

    protected static final String WARC_VERSION = "WARC/1.0";
    protected static final String CRLF = "\r\n";
    protected static final byte[] CRLF_BYTES = {13, 10};

    private static final Logger LOG = LoggerFactory.getLogger(WARCRecordFormat.class);

    /**
     * Date formatter format the WARC-Date.
     *
     * <p>Note: to meet the WARC 1.0 standard the precision is in seconds.
     */
    public static final DateTimeFormatter WARC_DF =
            new DateTimeFormatterBuilder().appendInstant(0).toFormatter(Locale.ROOT);

    protected static final Pattern PROBLEMATIC_HEADERS =
            Pattern.compile("(?i)(?:Content-(?:Encoding|Length)|Transfer-Encoding)");
    protected static final String X_HIDE_HEADER = "X-Crawler-";

    private static final Base32 base32 = new Base32();
    private static final String digestNoContent = getDigestSha1(new byte[0]);

    protected final String protocolMDprefix;

    public WARCRecordFormat(String protocolMDprefix) {
        this.protocolMDprefix = protocolMDprefix;
    }

    public static String getDigestSha1(byte[] bytes) {
        return "sha1:" + base32.encodeAsString(DigestUtils.sha1(bytes));
    }

    public static String getDigestSha1(byte[] bytes1, byte[] bytes2) {
        MessageDigest sha1 = DigestUtils.getSha1Digest();
        sha1.update(bytes1);
        return "sha1:" + base32.encodeAsString(sha1.digest(bytes2));
    }

    /** Generates a WARC info entry which can be stored at the beginning of each WARC file. */
    public static byte[] generateWARCInfo(Map<String, String> fields) {
        StringBuffer buffer = new StringBuffer();
        buffer.append(WARC_VERSION);
        buffer.append(CRLF);

        buffer.append("WARC-Type: ").append(WARC_TYPE_WARCINFO).append(CRLF);

        String mainID = UUID.randomUUID().toString();

        // retrieve the date and filename from the map
        String date = fields.get("WARC-Date");
        buffer.append("WARC-Date: ").append(date).append(CRLF);

        String filename = fields.get("WARC-Filename");
        buffer.append("WARC-Filename: ").append(filename).append(CRLF);

        buffer.append("WARC-Record-ID")
                .append(": ")
                .append("<urn:uuid:")
                .append(mainID)
                .append(">")
                .append(CRLF);

        buffer.append("Content-Type").append(": ").append("application/warc-fields").append(CRLF);

        StringBuilder fieldsBuffer = new StringBuilder();

        // add WARC fields
        // http://bibnum.bnf.fr/warc/WARC_ISO_28500_version1_latestdraft.pdf
        Iterator<Entry<String, String>> iter = fields.entrySet().iterator();
        while (iter.hasNext()) {
            Entry<String, String> entry = iter.next();
            String key = entry.getKey();
            if (key.startsWith("WARC-")) continue;
            fieldsBuffer.append(key).append(": ").append(entry.getValue()).append(CRLF);
        }

        buffer.append("Content-Length")
                .append(": ")
                .append(fieldsBuffer.toString().getBytes(StandardCharsets.UTF_8).length)
                .append(CRLF);

        buffer.append(CRLF);

        buffer.append(fieldsBuffer.toString());

        buffer.append(CRLF);
        buffer.append(CRLF);

        return buffer.toString().getBytes(StandardCharsets.UTF_8);
    }

    /**
     * Modify verbatim HTTP response headers: remove or replace headers <code>Content-Length</code>,
     * <code>Content-Encoding</code> and <code>Transfer-Encoding</code> which may confuse WARC
     * readers. Ensure that the header end with a single empty line (<code>\r\n\r\n</code>).
     *
     * @param headers HTTP 1.1 or 1.0 response header string, CR-LF-separated lines, first line is
     *     status line
     * @return safe HTTP response header
     */
    public static String fixHttpHeaders(String headers, int contentLength) {
        int start = 0, lineEnd = 0, last = 0, trailingCrLf = 0;
        StringBuilder replace = new StringBuilder();
        while (start < headers.length()) {
            lineEnd = headers.indexOf(CRLF, start);
            trailingCrLf = 1;
            if (lineEnd == -1) {
                lineEnd = headers.length();
                trailingCrLf = 0;
            }
            int colonPos = -1;
            for (int i = start; i < lineEnd; i++) {
                if (headers.charAt(i) == ':') {
                    colonPos = i;
                    break;
                }
            }
            if (colonPos == -1) {
                boolean valid = true;
                if (start == 0) {
                    // status line (without colon)
                } else if ((lineEnd + 4) == headers.length() && headers.endsWith(CRLF + CRLF)) {
                    // ok, trailing empty line
                    trailingCrLf = 2;
                } else if (start == lineEnd) {
                    // skip/remove empty line
                    valid = false;
                } else {
                    LOG.warn("Invalid header line: {}", headers.substring(start, lineEnd));
                    valid = false;
                }
                if (!valid) {
                    if (last < start) {
                        replace.append(headers.substring(last, start));
                    }
                    last = lineEnd + 2 * trailingCrLf;
                }
                start = lineEnd + 2 * trailingCrLf;
                /*
                 * skip over invalid header line, no further check for
                 * problematic headers required
                 */
                continue;
            }
            String name = headers.substring(start, colonPos);
            if (PROBLEMATIC_HEADERS.matcher(name).matches()) {
                boolean needsFix = true;
                if (name.equalsIgnoreCase("content-length")) {
                    String value = headers.substring(colonPos + 1, lineEnd).trim();
                    try {
                        int l = Integer.parseInt(value);
                        if (l == contentLength) {
                            needsFix = false;
                        }
                    } catch (NumberFormatException e) {
                        // needs to be fixed
                    }
                }
                if (needsFix) {
                    if (last < start) {
                        replace.append(headers.substring(last, start));
                    }
                    last = lineEnd + 2 * trailingCrLf;
                    replace.append(X_HIDE_HEADER)
                            .append(headers.substring(start, lineEnd + 2 * trailingCrLf));
                    if (trailingCrLf == 0) {
                        replace.append(CRLF);
                        trailingCrLf = 1;
                    }
                    if (name.equalsIgnoreCase("content-length")) {
                        // add effective uncompressed and unchunked length of
                        // content
                        replace.append("Content-Length")
                                .append(": ")
                                .append(contentLength)
                                .append(CRLF);
                    }
                }
            }
            start = lineEnd + 2 * trailingCrLf;
        }
        if (last > 0 || trailingCrLf != 2) {
            if (last < headers.length()) {
                // append trailing headers
                replace.append(headers.substring(last));
            }
            while (trailingCrLf < 2) {
                replace.append(CRLF);
                trailingCrLf++;
            }
            return replace.toString();
        }
        return headers;
    }

    /**
     * Get the actual fetch time from metadata and format it as required by the WARC-Date field. If
     * no fetch time is found in metadata (key {@link REQUEST_TIME_KEY}), the current time is taken.
     */
    protected String getCaptureTime(Metadata metadata) {
        String captureTimeMillis = metadata.getFirstValue(REQUEST_TIME_KEY, this.protocolMDprefix);
        Instant capturedAt = Instant.now();
        if (captureTimeMillis != null) {
            try {
                long millis = Long.parseLong(captureTimeMillis);
                capturedAt = Instant.ofEpochMilli(millis);
            } catch (NumberFormatException | DateTimeException e) {
                LOG.warn("Failed to parse capture time:", e);
            }
        }
        return WARC_DF.format(capturedAt);
    }

    @Override
    public byte[] format(Tuple tuple) {

        byte[] content = tuple.getBinaryByField("content");
        String url = tuple.getStringByField("url");
        Metadata metadata = (Metadata) tuple.getValueByField("metadata");

        // were the headers stored as is? Can write a response element then
        String headersVerbatim =
                metadata.getFirstValue(RESPONSE_HEADERS_KEY, this.protocolMDprefix);
        byte[] httpheaders = new byte[0];
        if (StringUtils.isNotBlank(headersVerbatim)) {
            headersVerbatim = fixHttpHeaders(headersVerbatim, content.length);
            httpheaders = headersVerbatim.getBytes();
        }

        StringBuilder buffer = new StringBuilder();
        buffer.append(WARC_VERSION);
        buffer.append(CRLF);

        String mainID = UUID.randomUUID().toString();

        buffer.append("WARC-Record-ID")
                .append(": ")
                .append("<urn:uuid:")
                .append(mainID)
                .append(">")
                .append(CRLF);

        String warcRequestId = metadata.getFirstValue("_request.warc_record_id_");
        if (warcRequestId != null) {
            buffer.append("WARC-Concurrent-To")
                    .append(": ")
                    .append("<urn:uuid:")
                    .append(warcRequestId)
                    .append(">")
                    .append(CRLF);
        }

        int contentLength = 0;
        String payloadDigest = digestNoContent;
        String blockDigest;
        if (content != null) {
            contentLength = content.length;
            payloadDigest = getDigestSha1(content);
            blockDigest = getDigestSha1(httpheaders, content);
        } else {
            blockDigest = getDigestSha1(httpheaders);
        }

        // add the length of the http header
        contentLength += httpheaders.length;

        buffer.append("Content-Length")
                .append(": ")
                .append(Integer.toString(contentLength))
                .append(CRLF);

        String captureTime = getCaptureTime(metadata);
        buffer.append("WARC-Date").append(": ").append(captureTime).append(CRLF);

        // if HTTP headers have been stored verbatim,
        // generate a "response" record, otherwise a "resource" record
        String WARCTypeValue = WARC_TYPE_RESOURCE;

        if (StringUtils.isNotBlank(headersVerbatim)) {
            WARCTypeValue = WARC_TYPE_RESPONSE;
        }

        buffer.append("WARC-Type").append(": ").append(WARCTypeValue).append(CRLF);

        // "WARC-IP-Address" if present
        String IP = metadata.getFirstValue(RESPONSE_IP_KEY);
        if (StringUtils.isNotBlank(IP)) {
            buffer.append("WARC-IP-Address").append(": ").append(IP).append(CRLF);
        }

        // must be a valid URI
        try {
            String normalised = url.replaceAll(" ", "%20");
            String targetURI = URI.create(normalised).toASCIIString();
            buffer.append("WARC-Target-URI").append(": ").append(targetURI).append(CRLF);
        } catch (Exception e) {
            LOG.warn("Incorrect URI: {}", url);
            return new byte[] {};
        }

        // provide a ContentType if type response
        if (WARCTypeValue.equals("response")) {
            buffer.append("Content-Type: application/http; msgtype=response").append(CRLF);
        }
        // for resources just use the content type provided by the server if any
        else {
            String ct = metadata.getFirstValue(HttpHeaders.CONTENT_TYPE);
            if (StringUtils.isBlank(ct)) {
                ct = "application/octet-stream";
            }
            buffer.append("Content-Type: ").append(ct).append(CRLF);
        }

        String truncated = metadata.getFirstValue("http.trimmed");
        if (truncated != null) {
            // content is truncated
            truncated = metadata.getFirstValue("http.trimmed.reason");
            if (truncated == null) {
                truncated =
                        ProtocolResponse.TrimmedContentReason.UNSPECIFIED
                                .toString()
                                .toLowerCase(Locale.ROOT);
            }
            buffer.append("WARC-Truncated").append(": ").append(truncated).append(CRLF);
        }

        buffer.append("WARC-Payload-Digest").append(": ").append(payloadDigest).append(CRLF);
        buffer.append("WARC-Block-Digest").append(": ").append(blockDigest).append(CRLF);

        byte[] buffasbytes = buffer.toString().getBytes(StandardCharsets.UTF_8);

        // work out the *exact* length of the bytebuffer - do not add any extra
        // bytes which are appended as trailing zero bytes causing invalid WARC
        // files
        int capacity = 6 + buffasbytes.length + httpheaders.length;
        if (content != null) {
            capacity += content.length;
        }

        ByteBuffer bytebuffer = ByteBuffer.allocate(capacity);
        bytebuffer.put(buffasbytes);
        bytebuffer.put(CRLF_BYTES);
        bytebuffer.put(httpheaders);
        // the binary content itself
        if (content != null) {
            bytebuffer.put(content);
        }
        bytebuffer.put(CRLF_BYTES);
        bytebuffer.put(CRLF_BYTES);

        return bytebuffer.array();
    }
}

Parse Compilation Unit:
package com.digitalpebble.stormcrawler.warc;

import com.digitalpebble.stormcrawler.protocol.ProtocolResponse;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import java.io.IOException;
import java.time.Instant;
import java.util.HashMap;
import java.util.Map;
import org.apache.hadoop.fs.Path;
import org.apache.storm.hdfs.bolt.rotation.FileSizeRotationPolicy;
import org.apache.storm.hdfs.bolt.rotation.FileSizeRotationPolicy.Units;
import org.apache.storm.hdfs.bolt.sync.CountSyncPolicy;
import org.apache.storm.hdfs.common.AbstractHDFSWriter;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.utils.Utils;

@SuppressWarnings("serial")
public class WARCHdfsBolt extends GzipHdfsBolt {

    private Map<String, String> header_fields = new HashMap<>();

    // by default remains as is-pre 1.17
    private String protocolMDprefix = "";

    private boolean withRequestRecords = false;

    public WARCHdfsBolt() {
        super();
        FileSizeRotationPolicy rotpol = new FileSizeRotationPolicy(1.0f, Units.GB);
        withRotationPolicy(rotpol);
        // dummy sync policy
        withSyncPolicy(new CountSyncPolicy(10));
        // default local filesystem
        withFsUrl("file:///");
    }

    public WARCHdfsBolt withHeader(Map<String, String> header_fields) {
        this.header_fields = header_fields;
        return this;
    }

    public WARCHdfsBolt withRequestRecords() {
        withRequestRecords = true;
        return this;
    }

    @Override
    public void doPrepare(Map conf, TopologyContext topologyContext, OutputCollector collector)
            throws IOException {
        super.doPrepare(conf, topologyContext, collector);
        protocolMDprefix = ConfUtils.getString(conf, ProtocolResponse.PROTOCOL_MD_PREFIX_PARAM, "");
        withRecordFormat(new WARCRecordFormat(protocolMDprefix));
        if (withRequestRecords) {
            addRecordFormat(new WARCRequestRecordFormat(protocolMDprefix), 0);
        }
    }

    @Override
    protected AbstractHDFSWriter makeNewWriter(Path path, Tuple tuple) throws IOException {
        AbstractHDFSWriter writer = super.makeNewWriter(path, tuple);

        Instant now = Instant.now();

        // overrides the filename and creation date in the headers
        header_fields.put("WARC-Date", WARCRecordFormat.WARC_DF.format(now));
        header_fields.put("WARC-Filename", path.getName());

        byte[] header = WARCRecordFormat.generateWARCInfo(header_fields);

        // write the header at the beginning of the file
        if (header != null && header.length > 0) {
            super.out.write(Utils.gzip(header));
        }

        return writer;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.warc;

import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.persistence.Status;
import com.digitalpebble.stormcrawler.protocol.ProtocolResponse;
import com.digitalpebble.stormcrawler.spout.FileSpout;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import java.io.IOException;
import java.net.URL;
import java.nio.ByteBuffer;
import java.nio.channels.Channels;
import java.nio.channels.FileChannel;
import java.nio.channels.ReadableByteChannel;
import java.nio.charset.StandardCharsets;
import java.nio.file.Paths;
import java.util.ArrayList;
import java.util.List;
import java.util.Locale;
import java.util.Map;
import java.util.Optional;
import org.apache.storm.metric.api.MultiCountMetric;
import org.apache.storm.spout.SpoutOutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.tuple.Fields;
import org.apache.storm.tuple.Values;
import org.netpreserve.jwarc.HttpMessage;
import org.netpreserve.jwarc.HttpRequest;
import org.netpreserve.jwarc.HttpResponse;
import org.netpreserve.jwarc.IOUtils;
import org.netpreserve.jwarc.MediaType;
import org.netpreserve.jwarc.ParsingException;
import org.netpreserve.jwarc.WarcPayload;
import org.netpreserve.jwarc.WarcReader;
import org.netpreserve.jwarc.WarcRecord;
import org.netpreserve.jwarc.WarcRequest;
import org.netpreserve.jwarc.WarcResponse;
import org.netpreserve.jwarc.WarcTruncationReason;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Read WARC files from the local files system and emit the WARC captures as tuples into the
 * topology same way as done by {@link com.digitalpebble.stormcrawler.bolt.FetcherBolt}.
 */
@SuppressWarnings("serial")
public class WARCSpout extends FileSpout {

    private static final Logger LOG = LoggerFactory.getLogger(WARCSpout.class);

    private int maxContentSize = -1;
    private int contentBufferSize = 8192;

    private boolean storeHTTPHeaders = false;
    private String protocolMDprefix = "";

    private WarcReader warcReader;
    private String warcFileInProgress;
    private WarcRequest precedingWarcRequest;
    private Optional<WarcRecord> record;

    private MultiCountMetric eventCounter;

    public WARCSpout(String... files) {
        super(false, files);
    }

    public WARCSpout(String dir, String filter) {
        super(dir, filter, false);
    }

    /**
     * Holder of truncation status when WARC payload exceeding the content length limit
     * (http.content.limit) is truncated.
     */
    public static class TruncationStatus {
        boolean isTruncated = false;
        long originalSize = -1;

        public void set(boolean isTruncated) {
            this.isTruncated = isTruncated;
        }

        public boolean get() {
            return isTruncated;
        }

        public void setOriginalSize(long size) {
            originalSize = size;
        }

        public long getOriginalSize() {
            return originalSize;
        }
    }

    private void openWARC() {
        if (warcReader != null) {
            try {
                warcReader.close();
            } catch (IOException e) {
                LOG.warn("Failed to close open WARC file", e);
            }
            warcReader = null;
        }

        byte[] head = buffer.removeFirst();
        List<Object> fields = _scheme.deserialize(ByteBuffer.wrap(head));
        warcFileInProgress = (String) fields.get(0);
        if (warcFileInProgress == null) return;

        LOG.info("Reading WARC file {}", warcFileInProgress);
        ReadableByteChannel warcChannel = null;
        try {
            warcChannel = openChannel(warcFileInProgress);
            warcReader = new WarcReader(warcChannel);
        } catch (IOException e) {
            LOG.error("Failed to open WARC file " + warcFileInProgress, e);
            warcFileInProgress = null;
            if (warcChannel != null) {
                try {
                    warcChannel.close();
                } catch (IOException ex) {
                }
            }
        }
    }

    private static ReadableByteChannel openChannel(String path) throws IOException {
        if (path.matches("^https?://.*")) {
            URL warcUrl = new URL(path);
            return Channels.newChannel(warcUrl.openStream());
        } else {
            return FileChannel.open(Paths.get(path));
        }
    }

    private void closeWARC() {
        LOG.info("Finished reading WARC file {}", warcFileInProgress);
        try {
            warcReader.close();
        } catch (IOException e) {
            LOG.warn("Failed to close WARC reader", e);
        }
        warcReader = null;
    }

    /**
     * Proceed to next WARC record, calculate record length of current record and add the length to
     * metadata
     */
    private void nextRecord(long offset, Metadata metadata) {
        long nextOffset = nextRecord();
        if (nextOffset > offset) {
            metadata.addValue("warc.record.length", Long.toString(nextOffset - offset));
        } else {
            LOG.error(
                    "Implausible offset of next WARC record: {} - current offset: {}",
                    nextOffset,
                    offset);
        }
    }

    /**
     * Proceed to next WARC record.
     *
     * @return offset of next record in WARC file
     */
    private long nextRecord() {
        long nextOffset;
        while (warcReader == null && !buffer.isEmpty()) {
            openWARC();
        }
        if (warcReader == null) {
            // failed to open any new WARC file
            record = Optional.empty();
            return -1;
        }
        try {
            record = warcReader.next();
            nextOffset = warcReader.position();
            if (!record.isPresent()) {
                closeWARC();
            }
        } catch (IOException e) {
            LOG.error(
                    "Failed to read WARC {} at position {}:",
                    warcFileInProgress,
                    warcReader.position(),
                    e);
            nextOffset = warcReader.position();
            record = Optional.empty();
            closeWARC();
        }
        return nextOffset;
    }

    private boolean isHttpResponse(Optional<WarcRecord> record) {
        if (!record.isPresent()) return false;
        if (!(record.get() instanceof WarcResponse)) return false;
        if (record.get().contentType().equals(MediaType.HTTP_RESPONSE)) return true;
        return false;
    }

    private byte[] getContent(WarcResponse record, TruncationStatus isTruncated)
            throws IOException {
        Optional<WarcPayload> payload = record.payload();
        if (!payload.isPresent()) {
            return new byte[0];
        }
        long size = payload.get().body().size();
        ReadableByteChannel body = payload.get().body();

        // Check HTTP Content-Encoding header whether payload needs decoding
        List<String> contentEncodings = record.http().headers().all("Content-Encoding");
        try {
            if (contentEncodings.size() > 1) {
                LOG.error("Multiple Content-Encodings not supported: {}", contentEncodings);
                LOG.warn("Trying to read payload of {} without Content-Encoding", record.target());
            } else if (contentEncodings.isEmpty()
                    || contentEncodings.get(0).equalsIgnoreCase("identity")
                    || contentEncodings.get(0).equalsIgnoreCase("none")) {
                // no need for decoding
            } else if (contentEncodings.get(0).equalsIgnoreCase("gzip")
                    || contentEncodings.get(0).equalsIgnoreCase("x-gzip")) {
                LOG.debug(
                        "Decoding payload of {} from Content-Encoding {}",
                        record.target(),
                        contentEncodings.get(0));
                body = IOUtils.gunzipChannel(body);
                body.read(ByteBuffer.allocate(0));
                size = -1;
            } else if (contentEncodings.get(0).equalsIgnoreCase("deflate")) {
                LOG.debug(
                        "Decoding payload of {} from Content-Encoding {}",
                        record.target(),
                        contentEncodings.get(0));
                body = IOUtils.inflateChannel(body);
                body.read(ByteBuffer.allocate(0));
                size = -1;
            } else {
                LOG.error("Content-Encoding not supported: {}", contentEncodings.get(0));
                LOG.warn("Trying to read payload of {} without Content-Encoding", record.target());
            }
        } catch (IOException e) {
            LOG.error(
                    "Failed to read payload with Content-Encoding {}: {}",
                    contentEncodings.get(0),
                    e.getMessage());
            LOG.warn("Trying to read payload of {} without Content-Encoding", record.target());
            body = payload.get().body();
        }

        isTruncated.set(false);
        if (size > maxContentSize) {
            LOG.info(
                    "WARC payload of size {} to be truncated to {} bytes for {}",
                    size,
                    maxContentSize,
                    record.target());
            size = maxContentSize;
        }
        ByteBuffer buf;
        if (size >= 0) {
            buf = ByteBuffer.allocate((int) size);
        } else {
            buf = ByteBuffer.allocate(contentBufferSize);
        }
        // dynamically growing list of buffers for large content of unknown size
        ArrayList<ByteBuffer> bufs = new ArrayList<>();
        int r, read = 0;
        while (read < maxContentSize) {
            try {
                if ((r = body.read(buf)) < 0) break; // eof
            } catch (ParsingException e) {
                LOG.error("Failed to read chunked content of {}: {}", record.target(), e);
                /*
                 * caused by an invalid Transfer-Encoding or a HTTP header
                 * `Transfer-Encoding: chunked` removed although the
                 * Transfer-Encoding was removed in the WARC file
                 */
                // TODO: should retry without chunked Transfer-Encoding
                break;
            } catch (IOException e) {
                LOG.error("Failed to read content of {}: {}", record.target(), e);
                break;
            }
            if (r == 0 && !buf.hasRemaining()) {
                buf.flip();
                bufs.add(buf);
                buf = ByteBuffer.allocate(Math.min(contentBufferSize, (maxContentSize - read)));
            }
            read += r;
        }
        buf.flip();
        if (read == maxContentSize) {
            // to mark truncation: check whether there is more content
            r = body.read(ByteBuffer.allocate(1));
            if (r > -1) {
                isTruncated.set(true);
                // read remaining body (also to figure out original length)
                long truncatedLength = r;
                ByteBuffer buffer = ByteBuffer.allocate(8192);
                try {
                    while ((r = body.read(buffer)) >= 0) {
                        buffer.clear();
                        truncatedLength += r;
                    }
                } catch (IOException e) {
                    // log and ignore, it's about unused content
                    LOG.info("Exception while determining length of truncation:", e);
                }
                isTruncated.setOriginalSize(read + truncatedLength);
                LOG.info(
                        "WARC payload of size {} is truncated to {} bytes for {}",
                        isTruncated.getOriginalSize(),
                        maxContentSize,
                        record.target());
            }
        }
        if (read == size) {
            // short-cut: return buffer-internal array
            return buf.array();
        }
        // copy buffers into result byte[]
        byte[] arr = new byte[read];
        int pos = 0;
        for (ByteBuffer b : bufs) {
            r = b.remaining();
            b.get(arr, pos, r);
            pos += r;
        }
        buf.get(arr, pos, buf.remaining());
        return arr;
    }

    private static String httpHeadersVerbatim(HttpMessage http) {
        return new String(http.serializeHeader(), StandardCharsets.UTF_8);
    }

    private void addVerbatimHttpHeaders(
            Metadata metadata, WarcResponse response, HttpResponse http, HttpRequest request) {
        metadata.addValue(
                protocolMDprefix + ProtocolResponse.REQUEST_TIME_KEY,
                Long.toString(response.date().toEpochMilli()));
        if (response.ipAddress().isPresent()) {
            metadata.addValue(
                    protocolMDprefix + ProtocolResponse.RESPONSE_IP_KEY,
                    response.ipAddress().get().getHostAddress());
        }
        if (request != null) {
            metadata.addValue(
                    protocolMDprefix + ProtocolResponse.REQUEST_HEADERS_KEY,
                    httpHeadersVerbatim(request));
        }
        metadata.addValue(
                protocolMDprefix + ProtocolResponse.RESPONSE_HEADERS_KEY,
                httpHeadersVerbatim(http));
    }

    @SuppressWarnings({"rawtypes", "unchecked"})
    @Override
    public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) {
        _collector = collector;
        record = Optional.empty();

        maxContentSize = ConfUtils.getInt(conf, "http.content.limit", -1);
        if (maxContentSize == -1 || maxContentSize > Constants.MAX_ARRAY_SIZE) {
            // maximum possible payload length, must fit into an array
            maxContentSize = Constants.MAX_ARRAY_SIZE;
        }
        if (contentBufferSize > maxContentSize) {
            // no need to buffer more content than max. used
            contentBufferSize = maxContentSize;
        }
        storeHTTPHeaders = ConfUtils.getBoolean(conf, "http.store.headers", false);
        protocolMDprefix =
                ConfUtils.getString(
                        conf, ProtocolResponse.PROTOCOL_MD_PREFIX_PARAM, protocolMDprefix);

        int metricsTimeBucketSecs = ConfUtils.getInt(conf, "fetcher.metrics.time.bucket.secs", 10);
        eventCounter =
                context.registerMetric(
                        "warc_spout_counter", new MultiCountMetric(), metricsTimeBucketSecs);
    }

    @Override
    public void nextTuple() {
        if (!active) return;

        if (buffer.isEmpty()) {
            try {
                populateBuffer();
            } catch (IOException e) {
                throw new RuntimeException(e);
            }
        }

        if (warcReader == null && buffer.isEmpty()) {
            // input exhausted
            return;
        }

        if (!record.isPresent()) nextRecord();

        while (record.isPresent() && !isHttpResponse(record)) {
            String warcType = record.get().type();
            if (warcType == null) {
                LOG.warn("No type for {}", record.get().getClass());
            } else {
                eventCounter.scope("warc_skipped_record_of_type_" + warcType).incr();
                LOG.debug("Skipped WARC record of type {}", warcType);
            }
            if (storeHTTPHeaders && record.get() instanceof WarcRequest) {
                // store request records to be able to add HTTP request
                // header to metadata
                precedingWarcRequest = (WarcRequest) record.get();
                try {
                    // need to read and parse HTTP header right now
                    // (otherwise it's skipped)
                    precedingWarcRequest.http();
                } catch (IOException e) {
                    LOG.error(
                            "Failed to read HTTP request for {} in {}: {}",
                            precedingWarcRequest.target(),
                            warcFileInProgress,
                            e);
                    precedingWarcRequest = null;
                }
            }
            nextRecord();
        }

        if (!record.isPresent()) return;

        eventCounter.scope("warc_http_response_record").incr();
        WarcResponse w = (WarcResponse) record.get();

        String url = w.target();
        HttpResponse http;
        try {
            http = w.http();
        } catch (IOException e) {
            LOG.error("Failed to read HTTP response for {} in {}: {}", url, warcFileInProgress, e);
            nextRecord();
            return;
        }
        LOG.info("Fetched {} with status {}", url, http.status());
        eventCounter.scope("fetched").incrBy(1);

        final Status status = Status.fromHTTPCode(http.status());
        eventCounter.scope("status_" + http.status()).incrBy(1);

        Metadata metadata = new Metadata();

        // add HTTP status code expected by schedulers
        metadata.addValue("fetch.statusCode", Integer.toString(http.status()));

        // add time when page was fetched (capture time)
        metadata.addValue(
                protocolMDprefix + ProtocolResponse.REQUEST_TIME_KEY,
                Long.toString(w.date().toEpochMilli()));

        // Add HTTP response headers to metadata
        for (Map.Entry<String, List<String>> e : http.headers().map().entrySet()) {
            metadata.addValues(protocolMDprefix + e.getKey(), e.getValue());
        }

        if (storeHTTPHeaders) {
            // if recording HTTP headers: add IP address, fetch date time,
            // literal request and response headers
            HttpRequest req = null;
            if (precedingWarcRequest != null
                    && (w.concurrentTo().contains(precedingWarcRequest.id())
                            || w.target().equals(precedingWarcRequest.target()))) {
                try {
                    req = precedingWarcRequest.http();
                } catch (IOException e) {
                    // ignore, no HTTP request headers are no issue
                }
            }
            addVerbatimHttpHeaders(metadata, w, http, req);
        }

        // add WARC record information
        metadata.addValue("warc.file.name", warcFileInProgress);
        long offset = warcReader.position();
        metadata.addValue("warc.record.offset", Long.toString(offset));
        /*
         * note: warc.record.length must be calculated after WARC record has
         * been entirely processed
         */

        if (status == Status.FETCHED && http.status() != 304) {
            byte[] content;
            TruncationStatus isTruncated = new TruncationStatus();
            try {
                content = getContent(w, isTruncated);
            } catch (IOException e) {
                LOG.error("Failed to read payload for {} in {}: {}", url, warcFileInProgress, e);
                content = new byte[0];
            }
            eventCounter.scope("bytes_fetched").incrBy(content.length);

            if (isTruncated.get() || w.truncated() != WarcTruncationReason.NOT_TRUNCATED) {
                WarcTruncationReason reason = WarcTruncationReason.LENGTH;
                if (w.truncated() != WarcTruncationReason.NOT_TRUNCATED) {
                    reason = w.truncated();
                }
                metadata.setValue(protocolMDprefix + ProtocolResponse.TRIMMED_RESPONSE_KEY, "true");
                metadata.setValue(
                        protocolMDprefix + ProtocolResponse.TRIMMED_RESPONSE_REASON_KEY,
                        reason.toString().toLowerCase(Locale.ROOT));
            }

            nextRecord(offset, metadata); // proceed and calculate length

            _collector.emit(new Values(url, content, metadata), url);

            return;
        }

        nextRecord(offset, metadata); // proceed and calculate length

        // redirects, 404s, etc.
        _collector.emit(Constants.StatusStreamName, new Values(url, metadata, status), url);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declareStream(Constants.StatusStreamName, new Fields("url", "metadata", "status"));
        declarer.declare(new Fields("url", "content", "metadata"));
    }

    @Override
    public void fail(Object msgId) {
        LOG.error("Failed - unable to replay WARC record of: {}", msgId);
    }
}

Parse Compilation Unit:
package com.digitalpebble.stormcrawler.warc;

import static com.digitalpebble.stormcrawler.protocol.ProtocolResponse.REQUEST_HEADERS_KEY;
import static com.digitalpebble.stormcrawler.protocol.ProtocolResponse.RESPONSE_IP_KEY;

import com.digitalpebble.stormcrawler.Metadata;
import java.net.URI;
import java.nio.ByteBuffer;
import java.nio.charset.StandardCharsets;
import java.util.UUID;
import org.apache.commons.lang.StringUtils;
import org.apache.storm.tuple.Tuple;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Generate a byte representation of a WARC request record from a tuple if the request HTTP headers
 * are present. The request record ID is stored in the metadata so that a WARC response record
 * (created later) can refer to it.
 */
@SuppressWarnings("serial")
public class WARCRequestRecordFormat extends WARCRecordFormat {

    private static final Logger LOG = LoggerFactory.getLogger(WARCRequestRecordFormat.class);

    public WARCRequestRecordFormat(String protocolMDprefix) {
        super(protocolMDprefix);
    }

    @Override
    public byte[] format(Tuple tuple) {

        String url = tuple.getStringByField("url");
        Metadata metadata = (Metadata) tuple.getValueByField("metadata");

        String headersVerbatim = metadata.getFirstValue(REQUEST_HEADERS_KEY, this.protocolMDprefix);
        byte[] httpheaders = new byte[0];
        if (StringUtils.isBlank(headersVerbatim)) {
            // no request header: return empty record
            LOG.warn("No request header for {}", url);
            return new byte[] {};
        } else {
            // check that header ends with an empty line
            while (!headersVerbatim.endsWith(CRLF + CRLF)) {
                headersVerbatim += CRLF;
            }
            httpheaders = headersVerbatim.getBytes();
        }

        StringBuilder buffer = new StringBuilder();
        buffer.append(WARC_VERSION);
        buffer.append(CRLF);
        buffer.append("WARC-Type: ").append(WARC_TYPE_REQUEST).append(CRLF);

        // "WARC-IP-Address" if present
        String IP = metadata.getFirstValue(RESPONSE_IP_KEY, this.protocolMDprefix);
        if (StringUtils.isNotBlank(IP)) {
            buffer.append("WARC-IP-Address: ").append(IP).append(CRLF);
        }

        String mainID = UUID.randomUUID().toString();
        buffer.append("WARC-Record-ID: ")
                .append("<urn:uuid:")
                .append(mainID)
                .append(">")
                .append(CRLF);
        /*
         * The request record ID is stored in the metadata so that a WARC
         * response record can later refer to it. Deactivated because of
         * https://github.com/DigitalPebble/storm-crawler/issues/721
         */
        // metadata.setValue("_request.warc_record_id_", mainID);

        int contentLength = httpheaders.length;
        buffer.append("Content-Length: ").append(Integer.toString(contentLength)).append(CRLF);

        String blockDigest = getDigestSha1(httpheaders);

        String captureTime = getCaptureTime(metadata);
        buffer.append("WARC-Date: ").append(captureTime).append(CRLF);

        // must be a valid URI
        try {
            String normalised = url.replaceAll(" ", "%20");
            String targetURI = URI.create(normalised).toASCIIString();
            buffer.append("WARC-Target-URI: ").append(targetURI).append(CRLF);
        } catch (Exception e) {
            LOG.warn("Incorrect URI: {}", url);
            return new byte[] {};
        }

        buffer.append("Content-Type: application/http; msgtype=request").append(CRLF);
        buffer.append("WARC-Block-Digest: ").append(blockDigest).append(CRLF);

        byte[] buffasbytes = buffer.toString().getBytes(StandardCharsets.UTF_8);

        int capacity = 6 + buffasbytes.length + httpheaders.length;

        ByteBuffer bytebuffer = ByteBuffer.allocate(capacity);
        bytebuffer.put(buffasbytes);
        bytebuffer.put(CRLF_BYTES);
        bytebuffer.put(httpheaders);
        bytebuffer.put(CRLF_BYTES);
        bytebuffer.put(CRLF_BYTES);

        return bytebuffer.array();
    }
}

Parse Compilation Unit:
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license
 * agreements. See the NOTICE file distributed with this work for additional information regarding
 * copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance with the License. You may obtain a
 * copy of the License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.warc;

import org.apache.storm.hdfs.bolt.rotation.FileRotationPolicy;
import org.apache.storm.hdfs.bolt.rotation.FileSizeRotationPolicy;
import org.apache.storm.tuple.Tuple;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/** Rotates a file based on size or optionally time, whichever occurs first * */
@SuppressWarnings("serial")
public class FileTimeSizeRotationPolicy implements FileRotationPolicy {

    private static final Logger LOG = LoggerFactory.getLogger(FileSizeRotationPolicy.class);

    public static enum Units {
        KB((long) Math.pow(2, 10)),
        MB((long) Math.pow(2, 20)),
        GB((long) Math.pow(2, 30)),
        TB((long) Math.pow(2, 40));

        private long byteCount;

        private Units(long byteCount) {
            this.byteCount = byteCount;
        }

        public long getByteCount() {
            return byteCount;
        }
    }

    public static enum TimeUnit {
        SECONDS((long) 1000),
        MINUTES((long) 1000 * 60),
        HOURS((long) 1000 * 60 * 60),
        DAYS((long) 1000 * 60 * 60 * 24);

        private long milliSeconds;

        private TimeUnit(long milliSeconds) {
            this.milliSeconds = milliSeconds;
        }

        public long getMilliSeconds() {
            return milliSeconds;
        }
    }

    private long interval = -1;

    private long maxBytes;

    private long lastOffset = 0;
    private long currentBytesWritten = 0;

    private long timeStarted = System.currentTimeMillis();

    public FileTimeSizeRotationPolicy(float count, Units units) {
        this.maxBytes = (long) (count * units.getByteCount());
    }

    public void setTimeRotationInterval(float count, TimeUnit units) {
        this.interval = (long) (count * units.getMilliSeconds());
    }

    @Override
    public boolean mark(Tuple tuple, long offset) {
        // check based on time first
        if (interval != -1) {
            long now = System.currentTimeMillis();
            if (now >= timeStarted + interval) {
                LOG.info(
                        "Rotating file based on time : started {} interval {}",
                        timeStarted,
                        interval);
                return true;
            }
        }

        long diff = offset - this.lastOffset;
        this.currentBytesWritten += diff;
        this.lastOffset = offset;
        boolean size = this.currentBytesWritten >= this.maxBytes;
        if (size) {
            LOG.info(
                    "Rotating file based on size : currentBytesWritten {} maxBytes {}",
                    currentBytesWritten,
                    maxBytes);
        }
        return size;
    }

    @Override
    public void reset() {
        this.currentBytesWritten = 0;
        this.lastOffset = 0;
        this.timeStarted = System.currentTimeMillis();
    }

    @Override
    public FileRotationPolicy copy() {
        FileTimeSizeRotationPolicy copy = new FileTimeSizeRotationPolicy(1.0f, Units.GB);
        copy.maxBytes = this.maxBytes;
        copy.interval = this.interval;
        return copy;
    }
}

Parse Compilation Unit:
package com.digitalpebble.stormcrawler.warc;

import java.text.SimpleDateFormat;
import java.util.Date;
import java.util.Map;
import java.util.TimeZone;
import org.apache.storm.hdfs.bolt.format.FileNameFormat;
import org.apache.storm.task.TopologyContext;

/**
 * From the WARC specs It is helpful to use practices within an institution that make it unlikely or
 * impossible to duplicate aggregate WARC file names. The convention used inside the Internet
 * Archive with ARC files is to name files according to the following pattern:
 * Prefix-Timestamp-Serial-Crawlhost.warc.gz Prefix is an abbreviation usually reflective of the
 * project or crawl that created this file. Timestamp is a 14- digit GMT timestamp indicating the
 * time the file was initially begun. Serial is an increasing serial-number within the process
 * creating the files, often (but not necessarily) unique with regard to the Prefix. Crawlhost is
 * the domain name or IP address of the machine creating the file.
 */
@SuppressWarnings("serial")
public class WARCFileNameFormat implements FileNameFormat {

    private int taskIndex;
    private String path = "/";
    private String prefix = "crawl";

    private final String extension = ".warc.gz";

    /**
     * Overrides the default prefix.
     *
     * @param prefix
     * @return
     */
    public FileNameFormat withPrefix(String prefix) {
        this.prefix = prefix;
        return this;
    }

    public FileNameFormat withPath(String path) {
        this.path = path;
        return this;
    }

    @Override
    public void prepare(Map conf, TopologyContext topologyContext) {
        this.taskIndex = topologyContext.getThisTaskIndex();
        int totalTasks =
                topologyContext.getComponentTasks(topologyContext.getThisComponentId()).size();
        // single task? let's not bother with the task index in the file name
        if (totalTasks == 1) {
            this.taskIndex = -1;
        }
    }

    @Override
    public String getName(long rotation, long timeStamp) {
        SimpleDateFormat fileDate = new SimpleDateFormat("yyyyMMddHHmmss");
        fileDate.setTimeZone(TimeZone.getTimeZone("GMT"));
        String taskindexString = "";
        if (this.taskIndex != -1) {
            taskindexString = String.format("%02d", this.taskIndex) + "-";
        }
        return this.prefix
                + "-"
                + fileDate.format(new Date(timeStamp))
                + "-"
                + taskindexString
                + String.format("%05d", rotation)
                + this.extension;
    }

    public String getPath() {
        return this.path;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.sql;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.persistence.AbstractStatusUpdaterBolt;
import com.digitalpebble.stormcrawler.persistence.Status;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import com.digitalpebble.stormcrawler.util.URLPartitioner;
import java.sql.Connection;
import java.sql.PreparedStatement;
import java.sql.SQLException;
import java.util.Date;
import java.util.HashMap;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.Optional;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;
import org.apache.storm.metric.api.MultiCountMetric;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.tuple.Tuple;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Status updater for SQL backend. Discovered URLs are sent as a batch, whereas updates are atomic.
 */
@SuppressWarnings("serial")
public class StatusUpdaterBolt extends AbstractStatusUpdaterBolt {

    public static final Logger LOG = LoggerFactory.getLogger(StatusUpdaterBolt.class);

    private MultiCountMetric eventCounter;

    private Connection connection;
    private String tableName;

    private URLPartitioner partitioner;
    private int maxNumBuckets = -1;

    private int batchMaxSize = 1000;
    private float batchMaxIdleMsec = 2000;

    private int currentBatchSize = 0;

    private PreparedStatement insertPreparedStmt = null;

    private long lastInsertBatchTime = -1;

    private String updateQuery;
    private String insertQuery;

    private final Map<String, List<Tuple>> waitingAck = new HashMap<>();

    public StatusUpdaterBolt(int maxNumBuckets) {
        this.maxNumBuckets = maxNumBuckets;
    }

    /** Does not shard based on the total number of queues * */
    public StatusUpdaterBolt() {}

    @SuppressWarnings({"rawtypes", "unchecked"})
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
        super.prepare(stormConf, context, collector);

        partitioner = new URLPartitioner();
        partitioner.configure(stormConf);

        this.eventCounter = context.registerMetric("counter", new MultiCountMetric(), 10);

        tableName = ConfUtils.getString(stormConf, Constants.SQL_STATUS_TABLE_PARAM_NAME, "urls");

        batchMaxSize =
                ConfUtils.getInt(stormConf, Constants.SQL_UPDATE_BATCH_SIZE_PARAM_NAME, 1000);

        try {
            connection = SQLUtil.getConnection(stormConf);
        } catch (SQLException ex) {
            LOG.error(ex.getMessage(), ex);
            throw new RuntimeException(ex);
        }

        String query =
                tableName
                        + " (url, status, nextfetchdate, metadata, bucket, host)"
                        + " values (?, ?, ?, ?, ?, ?)";

        updateQuery = "REPLACE INTO " + query;
        insertQuery = "INSERT IGNORE INTO " + query;

        try {
            insertPreparedStmt = connection.prepareStatement(insertQuery);
        } catch (SQLException e) {
            LOG.error(e.getMessage(), e);
        }

        ScheduledExecutorService executor = Executors.newSingleThreadScheduledExecutor();
        executor.scheduleAtFixedRate(
                () -> {
                    try {
                        checkExecuteBatch();
                    } catch (SQLException ex) {
                        LOG.error(ex.getMessage(), ex);
                        throw new RuntimeException(ex);
                    }
                },
                0,
                1,
                TimeUnit.SECONDS);
    }

    @Override
    public synchronized void store(
            String url, Status status, Metadata metadata, Optional<Date> nextFetch, Tuple t)
            throws Exception {
        // check whether the batch needs sending
        checkExecuteBatch();

        boolean isUpdate = !status.equals(Status.DISCOVERED);

        // already have an entry for this DISCOVERED URL
        if (!isUpdate && waitingAck.containsKey(url)) {
            List<Tuple> list = waitingAck.get(url);
            // add the tuple to the list for that url
            list.add(t);
            return;
        }

        StringBuilder mdAsString = new StringBuilder();
        for (String mdKey : metadata.keySet()) {
            String[] vals = metadata.getValues(mdKey);
            for (String v : vals) {
                mdAsString.append("\t").append(mdKey).append("=").append(v);
            }
        }

        int partition = 0;
        String partitionKey = partitioner.getPartition(url, metadata);
        if (maxNumBuckets > 1) {
            // determine which shard to send to based on the host / domain /
            // IP
            partition = Math.abs(partitionKey.hashCode() % maxNumBuckets);
        }

        PreparedStatement preparedStmt = this.insertPreparedStmt;

        // create in table if does not already exist
        if (isUpdate) {
            preparedStmt = connection.prepareStatement(updateQuery);
        }

        preparedStmt.setString(1, url);
        preparedStmt.setString(2, status.toString());
        if (nextFetch.isPresent()) preparedStmt.setObject(3, nextFetch.get());
        preparedStmt.setString(4, mdAsString.toString());
        preparedStmt.setInt(5, partition);
        preparedStmt.setString(6, partitionKey);

        // updates are not batched
        if (isUpdate) {
            preparedStmt.executeUpdate();
            preparedStmt.close();
            eventCounter.scope("sql_updates_number").incrBy(1);
            super.ack(t, url);
            return;
        }

        // code below is for inserts i.e. DISCOVERED URLs
        preparedStmt.addBatch();

        if (lastInsertBatchTime == -1) {
            lastInsertBatchTime = System.currentTimeMillis();
        }

        // URL gets added to the cache in method ack
        // once this method has returned
        waitingAck.put(url, new LinkedList<Tuple>());

        currentBatchSize++;

        eventCounter.scope("sql_inserts_number").incrBy(1);
    }

    private synchronized void checkExecuteBatch() throws SQLException {
        if (currentBatchSize == 0) {
            return;
        }
        long now = System.currentTimeMillis();
        // check whether the insert batches need executing
        if ((currentBatchSize == batchMaxSize)) {
            LOG.info("About to execute batch - triggered by size");
        } else if (lastInsertBatchTime + (long) batchMaxIdleMsec < System.currentTimeMillis()) {
            LOG.info(
                    "About to execute batch - triggered by time. Due {}, now {}",
                    lastInsertBatchTime + (long) batchMaxIdleMsec,
                    now);
        } else {
            return;
        }

        try {
            long start = System.currentTimeMillis();
            insertPreparedStmt.executeBatch();
            long end = System.currentTimeMillis();

            LOG.info("Batched {} inserts executed in {} msec", currentBatchSize, end - start);
            waitingAck.forEach(
                    (k, v) -> {
                        for (Tuple t : v) {
                            super.ack(t, k);
                        }
                    });
        } catch (SQLException e) {
            LOG.error(e.getMessage(), e);
            // fail the entire batch
            waitingAck.forEach(
                    (k, v) -> {
                        for (Tuple t : v) {
                            super._collector.fail(t);
                        }
                    });
        }

        lastInsertBatchTime = System.currentTimeMillis();
        currentBatchSize = 0;
        waitingAck.clear();

        insertPreparedStmt.close();
        insertPreparedStmt = connection.prepareStatement(insertQuery);
    }

    @Override
    public void cleanup() {
        if (connection != null)
            try {
                connection.close();
            } catch (SQLException e) {
            }
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.sql.metrics;

import com.digitalpebble.stormcrawler.sql.Constants;
import com.digitalpebble.stormcrawler.sql.SQLUtil;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import java.sql.Connection;
import java.sql.PreparedStatement;
import java.sql.SQLException;
import java.util.Collection;
import java.util.Date;
import java.util.Iterator;
import java.util.Map;
import java.util.Map.Entry;
import org.apache.storm.metric.api.IMetricsConsumer;
import org.apache.storm.task.IErrorReporter;
import org.apache.storm.task.TopologyContext;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class MetricsConsumer implements IMetricsConsumer {

    private final Logger LOG = LoggerFactory.getLogger(getClass());
    private Connection connection;
    private String query;

    @Override
    public void prepare(
            Map stormConf,
            Object registrationArgument,
            TopologyContext context,
            IErrorReporter errorReporter) {
        final String tableName =
                ConfUtils.getString(stormConf, Constants.SQL_METRICS_TABLE_PARAM_NAME, "metrics");
        query =
                "INSERT INTO "
                        + tableName
                        + " (srcComponentId, srcTaskId, srcWorkerHost, srcWorkerPort, name, value, timestamp)"
                        + " values (?, ?, ?, ?, ?, ?, ?)";
        try {
            connection = SQLUtil.getConnection(stormConf);
        } catch (SQLException ex) {
            LOG.error(ex.getMessage(), ex);
            throw new RuntimeException(ex);
        }
    }

    @Override
    public void handleDataPoints(TaskInfo taskInfo, Collection<DataPoint> dataPoints) {
        final Date now = new Date();
        try {
            PreparedStatement preparedStmt = connection.prepareStatement(query);
            for (DataPoint dataPoint : dataPoints) {
                handleDataPoints(preparedStmt, taskInfo, dataPoint.name, dataPoint.value, now);
            }
            preparedStmt.executeBatch();
            preparedStmt.close();
        } catch (SQLException ex) {
            LOG.error(ex.getMessage(), ex);
            throw new RuntimeException(ex);
        }
    }

    private void handleDataPoints(
            final PreparedStatement preparedStmt,
            final TaskInfo taskInfo,
            final String nameprefix,
            final Object value,
            final Date now) {
        if (value instanceof Number) {
            try {
                indexDataPoint(
                        preparedStmt, taskInfo, now, nameprefix, ((Number) value).doubleValue());
            } catch (SQLException e) {
                LOG.error("Exception while indexing datapoint", e);
            }
        } else if (value instanceof Map) {
            Iterator<Entry> keyValiter = ((Map) value).entrySet().iterator();
            while (keyValiter.hasNext()) {
                Entry entry = keyValiter.next();
                String newnameprefix = nameprefix + "." + entry.getKey();
                handleDataPoints(preparedStmt, taskInfo, newnameprefix, entry.getValue(), now);
            }
        } else if (value instanceof Collection) {
            for (Object collectionObj : (Collection) value) {
                handleDataPoints(preparedStmt, taskInfo, nameprefix, collectionObj, now);
            }
        } else {
            LOG.warn("Found data point value {} of {}", nameprefix, value.getClass().toString());
        }
    }

    private void indexDataPoint(
            final PreparedStatement preparedStmt,
            TaskInfo taskInfo,
            Date timestamp,
            String name,
            double value)
            throws SQLException {
        preparedStmt.setString(1, taskInfo.srcComponentId);
        preparedStmt.setInt(2, taskInfo.srcTaskId);
        preparedStmt.setString(3, taskInfo.srcWorkerHost);
        preparedStmt.setInt(4, taskInfo.srcWorkerPort);
        preparedStmt.setString(5, name);
        preparedStmt.setDouble(6, value);
        preparedStmt.setObject(7, timestamp);
        preparedStmt.addBatch();
    }

    @Override
    public void cleanup() {
        try {
            connection.close();
        } catch (SQLException e) {
        }
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.sql;

import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.SQLException;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Properties;

public class SQLUtil {

    private SQLUtil() {}

    @SuppressWarnings({"rawtypes", "unchecked"})
    public static Connection getConnection(Map stormConf) throws SQLException {
        // SQL connection details
        Map<String, String> sqlConf = (Map) stormConf.get("sql.connection");

        if (sqlConf == null) {
            throw new RuntimeException(
                    "Missing SQL connection config, add a section 'sql.connection' to the configuration");
        }

        String url = sqlConf.get("url");
        if (url == null) {
            throw new RuntimeException(
                    "Missing SQL url, add an entry 'url' to the section 'sql.connection' of the configuration");
        }

        Properties props = new Properties();

        for (Entry<String, String> entry : sqlConf.entrySet()) {
            props.setProperty(entry.getKey(), entry.getValue());
        }

        return DriverManager.getConnection(url, props);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.sql;

public class Constants {

    public static final String SQL_STATUS_TABLE_PARAM_NAME = "sql.status.table";
    public static final String SQL_MAX_DOCS_BUCKET_PARAM_NAME = "sql.max.urls.per.bucket";
    public static final String SQL_MAXRESULTS_PARAM_NAME = "sql.spout.max.results";

    public static final String SQL_UPDATE_BATCH_SIZE_PARAM_NAME = "sql.update.batch.size";

    public static final String SQL_METRICS_TABLE_PARAM_NAME = "sql.metrics.table";

    private Constants() {}
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.sql;

import static com.digitalpebble.stormcrawler.Constants.StatusStreamName;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.indexing.AbstractIndexerBolt;
import com.digitalpebble.stormcrawler.persistence.Status;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import java.sql.Connection;
import java.sql.PreparedStatement;
import java.sql.SQLException;
import java.util.Map;
import org.apache.commons.lang.StringUtils;
import org.apache.storm.metric.api.MultiCountMetric;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.tuple.Values;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/** Stores URL and selected metadata into a SQL table * */
public class IndexerBolt extends AbstractIndexerBolt {

    private static final Logger LOG = LoggerFactory.getLogger(IndexerBolt.class);

    public static final String SQL_INDEX_TABLE_PARAM_NAME = "sql.index.table";

    private OutputCollector _collector;

    private MultiCountMetric eventCounter;

    private Connection connection;

    private String tableName;

    private Map conf;

    @SuppressWarnings({"unchecked", "rawtypes"})
    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
        super.prepare(conf, context, collector);
        _collector = collector;

        this.eventCounter = context.registerMetric("SQLIndexer", new MultiCountMetric(), 10);

        this.tableName = ConfUtils.getString(conf, SQL_INDEX_TABLE_PARAM_NAME);

        this.conf = conf;
    }

    @Override
    public void execute(Tuple tuple) {
        String url = tuple.getStringByField("url");

        // Distinguish the value used for indexing
        // from the one used for the status
        String normalisedurl = valueForURL(tuple);

        Metadata metadata = (Metadata) tuple.getValueByField("metadata");
        String text = tuple.getStringByField("text");

        boolean keep = filterDocument(metadata);
        if (!keep) {
            eventCounter.scope("Filtered").incrBy(1);
            // treat it as successfully processed even if
            // we do not index it
            _collector.emit(StatusStreamName, tuple, new Values(url, metadata, Status.FETCHED));
            _collector.ack(tuple);
            return;
        }

        try {

            // which metadata to display?
            Map<String, String[]> keyVals = filterMetadata(metadata);

            StringBuilder query =
                    new StringBuilder(" insert into ")
                            .append(tableName)
                            .append(" (")
                            .append(fieldNameForURL());

            Object[] keys = keyVals.keySet().toArray();

            for (int i = 0; i < keys.length; i++) {
                query.append(", ").append((String) keys[i]);
            }

            query.append(") values(?");

            for (int i = 0; i < keys.length; i++) {
                query.append(", ?");
            }

            query.append(")");

            query.append(" ON DUPLICATE KEY UPDATE ");
            for (int i = 0; i < keys.length; i++) {
                String key = (String) keys[i];
                if (i > 0) {
                    query.append(", ");
                }
                query.append(key).append("=VALUES(").append(key).append(")");
            }

            if (connection == null) {
                try {
                    connection = SQLUtil.getConnection(conf);
                } catch (SQLException ex) {
                    LOG.error(ex.getMessage(), ex);
                    throw new RuntimeException(ex);
                }
            }

            LOG.debug("PreparedStatement => {}", query);

            // create the mysql insert preparedstatement
            PreparedStatement preparedStmt = connection.prepareStatement(query.toString());

            // TODO store the text of the document?
            if (StringUtils.isNotBlank(fieldNameForText())) {
                // builder.field(fieldNameForText(), trimText(text));
            }

            // send URL as field?
            if (fieldNameForURL() != null) {
                preparedStmt.setString(1, normalisedurl);
            }

            for (int i = 0; i < keys.length; i++) {
                insert(preparedStmt, i + 2, (String) keys[i], keyVals);
            }

            preparedStmt.executeUpdate();

            eventCounter.scope("Indexed").incrBy(1);

            _collector.emit(StatusStreamName, tuple, new Values(url, metadata, Status.FETCHED));
            _collector.ack(tuple);

        } catch (Exception e) {
            // do not send to status stream so that it gets replayed
            LOG.error("Error inserting into SQL", e);
            _collector.fail(tuple);
            if (connection != null) {
                // reset the connection
                try {
                    connection.close();
                } catch (SQLException e1) {
                }
                connection = null;
            }
        }
    }

    private void insert(
            PreparedStatement preparedStmt,
            int position,
            String label,
            Map<String, String[]> keyVals)
            throws SQLException {
        String[] values = keyVals.get(label);
        String value = "";
        if (values == null || values.length == 0) {
            LOG.info("No values found for label {}", label);
        } else if (values.length > 1) {
            LOG.info("More than one value found for label {}", label);
            value = values[0];
        } else {
            value = values[0];
        }
        preparedStmt.setString(position, value);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.sql;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.persistence.AbstractQueryingSpout;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import com.digitalpebble.stormcrawler.util.StringTabScheme;
import java.nio.ByteBuffer;
import java.sql.Connection;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.sql.Statement;
import java.sql.Timestamp;
import java.time.Instant;
import java.util.List;
import java.util.Map;
import org.apache.storm.spout.Scheme;
import org.apache.storm.spout.SpoutOutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

@SuppressWarnings("serial")
public class SQLSpout extends AbstractQueryingSpout {

    public static final Logger LOG = LoggerFactory.getLogger(SQLSpout.class);

    private static final Scheme SCHEME = new StringTabScheme();

    private String tableName;

    private Connection connection;

    /**
     * if more than one instance of the spout exist, each one is in charge of a separate bucket
     * value. This is used to ensure a good diversity of URLs.
     */
    private int bucketNum = -1;

    /** Used to distinguish between instances in the logs * */
    protected String logIdprefix = "";

    private int maxDocsPerBucket;

    private int maxNumResults;

    private Instant lastNextFetchDate = null;

    @SuppressWarnings({"rawtypes", "unchecked"})
    @Override
    public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) {

        super.open(conf, context, collector);

        maxDocsPerBucket = ConfUtils.getInt(conf, Constants.SQL_MAX_DOCS_BUCKET_PARAM_NAME, 5);

        tableName = ConfUtils.getString(conf, Constants.SQL_STATUS_TABLE_PARAM_NAME, "urls");

        maxNumResults = ConfUtils.getInt(conf, Constants.SQL_MAXRESULTS_PARAM_NAME, 100);

        try {
            connection = SQLUtil.getConnection(conf);
        } catch (SQLException ex) {
            LOG.error(ex.getMessage(), ex);
            throw new RuntimeException(ex);
        }

        // determine bucket this spout instance will be in charge of
        int totalTasks = context.getComponentTasks(context.getThisComponentId()).size();
        if (totalTasks > 1) {
            logIdprefix =
                    "[" + context.getThisComponentId() + " #" + context.getThisTaskIndex() + "] ";
            bucketNum = context.getThisTaskIndex();
        }
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(SCHEME.getOutputFields());
    }

    @Override
    protected void populateBuffer() {

        if (lastNextFetchDate == null) {
            lastNextFetchDate = Instant.now();
            lastTimeResetToNOW = Instant.now();
        } else if (resetFetchDateAfterNSecs != -1) {
            Instant changeNeededOn =
                    Instant.ofEpochMilli(
                            lastTimeResetToNOW.toEpochMilli() + (resetFetchDateAfterNSecs * 1000));
            if (Instant.now().isAfter(changeNeededOn)) {
                LOG.info(
                        "lastDate reset based on resetFetchDateAfterNSecs {}",
                        resetFetchDateAfterNSecs);
                lastNextFetchDate = Instant.now();
            }
        }

        // select entries from mysql
        // https://mariadb.com/kb/en/library/window-functions-overview/
        // http://www.mysqltutorial.org/mysql-window-functions/mysql-rank-function/

        String query =
                "SELECT * from (select rank() over (partition by host order by nextfetchdate desc, url) as ranking, url, metadata, nextfetchdate from "
                        + tableName;

        query +=
                " WHERE nextfetchdate <= '" + new Timestamp(lastNextFetchDate.toEpochMilli()) + "'";

        // constraint on bucket num
        if (bucketNum >= 0) {
            query += " AND bucket = '" + bucketNum + "'";
        }

        query +=
                ") as urls_ranks where (urls_ranks.ranking <= "
                        + maxDocsPerBucket
                        + ") order by ranking";

        if (maxNumResults != -1) {
            query += " LIMIT " + this.maxNumResults;
        }

        int alreadyprocessed = 0;
        int numhits = 0;

        long timeStartQuery = System.currentTimeMillis();

        // create the java statement
        Statement st = null;
        ResultSet rs = null;
        try {
            st = this.connection.createStatement();

            // dump query to log
            LOG.debug("{} SQL query {}", logIdprefix, query);

            // execute the query, and get a java resultset
            rs = st.executeQuery(query);

            long timeTaken = System.currentTimeMillis() - timeStartQuery;
            queryTimes.addMeasurement(timeTaken);

            // iterate through the java resultset
            while (rs.next()) {
                String url = rs.getString("url");
                numhits++;
                // already processed? skip
                if (beingProcessed.containsKey(url)) {
                    alreadyprocessed++;
                    continue;
                }
                String metadata = rs.getString("metadata");
                if (metadata == null) {
                    metadata = "";
                } else if (!metadata.startsWith("\t")) {
                    metadata = "\t" + metadata;
                }
                String URLMD = url + metadata;
                List<Object> v = SCHEME.deserialize(ByteBuffer.wrap(URLMD.getBytes()));
                buffer.add(url, (Metadata) v.get(1));
            }

            // no results? reset the date
            if (numhits == 0) {
                lastNextFetchDate = null;
            }

            eventCounter.scope("already_being_processed").incrBy(alreadyprocessed);
            eventCounter.scope("queries").incrBy(1);
            eventCounter.scope("docs").incrBy(numhits);

            LOG.info(
                    "{} SQL query returned {} hits in {} msec with {} already being processed",
                    logIdprefix,
                    numhits,
                    timeTaken,
                    alreadyprocessed);

        } catch (SQLException e) {
            LOG.error("Exception while querying table", e);
        } finally {
            try {
                if (rs != null) rs.close();
            } catch (SQLException e) {
                LOG.error("Exception closing resultset", e);
            }
            try {
                if (st != null) st.close();
            } catch (SQLException e) {
                LOG.error("Exception closing statement", e);
            }
        }
    }

    @Override
    public void ack(Object msgId) {
        LOG.debug("{}  Ack for {}", logIdprefix, msgId);
        super.ack(msgId);
    }

    @Override
    public void fail(Object msgId) {
        LOG.info("{}  Fail for {}", logIdprefix, msgId);
        super.fail(msgId);
    }

    @Override
    public void close() {
        super.close();
        try {
            connection.close();
        } catch (SQLException e) {
            LOG.error("Exception caught while closing SQL connection", e);
        }
    }
}

Parse Compilation Unit:
#set($symbol_pound='#')#set($symbol_dollar='$')#set($symbol_escape='\')

/**
 * Licensed to DigitalPebble Ltd under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package ${package};

import org.apache.storm.metric.LoggingMetricsConsumer;
import org.apache.storm.topology.TopologyBuilder;
import org.apache.storm.tuple.Fields;

import com.digitalpebble.stormcrawler.ConfigurableTopology;
import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.bolt.FetcherBolt;
import com.digitalpebble.stormcrawler.bolt.JSoupParserBolt;
import com.digitalpebble.stormcrawler.bolt.SiteMapParserBolt;
import com.digitalpebble.stormcrawler.bolt.URLFilterBolt;
import com.digitalpebble.stormcrawler.bolt.URLPartitionerBolt;
import com.digitalpebble.stormcrawler.elasticsearch.bolt.DeletionBolt;
import com.digitalpebble.stormcrawler.elasticsearch.bolt.IndexerBolt;
import com.digitalpebble.stormcrawler.elasticsearch.metrics.MetricsConsumer;
import com.digitalpebble.stormcrawler.elasticsearch.metrics.StatusMetricsBolt;
import com.digitalpebble.stormcrawler.elasticsearch.persistence.AggregationSpout;
import com.digitalpebble.stormcrawler.elasticsearch.persistence.StatusUpdaterBolt;
import com.digitalpebble.stormcrawler.spout.FileSpout;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import com.digitalpebble.stormcrawler.util.URLStreamGrouping;
import com.digitalpebble.stormcrawler.tika.ParserBolt;
import com.digitalpebble.stormcrawler.tika.RedirectionBolt;

/**
 * Dummy topology to play with the spouts and bolts on ElasticSearch
 */
public class ESCrawlTopology extends ConfigurableTopology {

	public static void main(String[] args) throws Exception {
		ConfigurableTopology.start(new ESCrawlTopology(), args);
	}

	@Override
	protected int run(String[] args) {
		TopologyBuilder builder = new TopologyBuilder();

		int numWorkers = ConfUtils.getInt(getConf(), "topology.workers", 1);

		if (args.length == 0) {
			System.err.println("ESCrawlTopology seed_dir file_filter");
			return -1;
		}

		// set to the real number of shards ONLY if es.status.routing is set to
		// true in the configuration
		int numShards = 1;

		builder.setSpout("filespout", new FileSpout(args[0], args[1], true));

		Fields key = new Fields("url");

		builder.setBolt("filter", new URLFilterBolt()).fieldsGrouping("filespout", Constants.StatusStreamName, key);

		builder.setSpout("spout", new AggregationSpout(), numShards);

		builder.setBolt("status_metrics", new StatusMetricsBolt()).shuffleGrouping("spout");

		builder.setBolt("partitioner", new URLPartitionerBolt(), numWorkers).shuffleGrouping("spout");

		builder.setBolt("fetch", new FetcherBolt(), numWorkers).fieldsGrouping("partitioner", new Fields("key"));

		builder.setBolt("sitemap", new SiteMapParserBolt(), numWorkers).localOrShuffleGrouping("fetch");

		builder.setBolt("parse", new JSoupParserBolt(), numWorkers).localOrShuffleGrouping("sitemap");

		builder.setBolt("shunt", new RedirectionBolt()).localOrShuffleGrouping("parse");

		builder.setBolt("tika", new ParserBolt()).localOrShuffleGrouping("shunt", "tika");

		builder.setBolt("indexer", new IndexerBolt(), numWorkers).localOrShuffleGrouping("shunt")
				.localOrShuffleGrouping("tika");

		builder.setBolt("status", new StatusUpdaterBolt(), numWorkers)
				.fieldsGrouping("fetch", Constants.StatusStreamName, key)
				.fieldsGrouping("sitemap", Constants.StatusStreamName, key)
				.fieldsGrouping("parse", Constants.StatusStreamName, key)
				.fieldsGrouping("tika", Constants.StatusStreamName, key)
				.fieldsGrouping("indexer", Constants.StatusStreamName, key)
				.customGrouping("filter", Constants.StatusStreamName, new URLStreamGrouping());

		builder.setBolt("deleter", new DeletionBolt(), numWorkers).localOrShuffleGrouping("status",
				Constants.DELETION_STREAM_NAME);

		conf.registerMetricsConsumer(MetricsConsumer.class);
		conf.registerMetricsConsumer(LoggingMetricsConsumer.class);

		return submit("crawl", conf, builder);
	}
}

Indexing exception: java.lang.RuntimeException: could not find it!
Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.elasticsearch.parse.filter;

import com.digitalpebble.stormcrawler.JSONResource;
import com.digitalpebble.stormcrawler.elasticsearch.ElasticSearchConnection;
import com.digitalpebble.stormcrawler.parse.ParseFilter;
import com.digitalpebble.stormcrawler.parse.ParseResult;
import com.fasterxml.jackson.databind.JsonNode;
import java.io.ByteArrayInputStream;
import java.util.Map;
import java.util.Timer;
import java.util.TimerTask;
import org.elasticsearch.action.get.GetRequest;
import org.elasticsearch.action.get.GetResponse;
import org.elasticsearch.client.RequestOptions;
import org.elasticsearch.client.RestHighLevelClient;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.w3c.dom.DocumentFragment;

/**
 * Wraps a ParseFilter whose resources are in a JSON file that can be stored in ES. The benefit of
 * doing this is that the resources can be refreshed automatically and modified without having to
 * recompile the jar and restart the topology. The connection to ES is done via the config and uses
 * a new bolt type 'config'.
 *
 * <p>The configuration of the delegate is done in the parsefilters.json as usual.
 *
 * <pre>
 *  {
 *     "class": "com.digitalpebble.stormcrawler.elasticsearch.parse.filter.JSONResourceWrapper",
 *     "name": "ESCollectionTagger",
 *     "params": {
 *         "refresh": "60",
 *         "delegate": {
 *             "class": "com.digitalpebble.stormcrawler.parse.filter.CollectionTagger",
 *             "params": {
 *                 "file": "collections.json"
 *             }
 *         }
 *     }
 *  }
 * </pre>
 *
 * The resource file can be pushed to ES with
 *
 * <pre>
 *  curl -XPUT 'localhost:9200/config/config/collections.json?pretty' -H 'Content-Type: application/json' -d @collections.json
 * </pre>
 */
public class JSONResourceWrapper extends ParseFilter {

    private static final Logger LOG = LoggerFactory.getLogger(JSONResourceWrapper.class);

    private ParseFilter delegatedParseFilter;

    public void configure(@SuppressWarnings("rawtypes") Map stormConf, JsonNode filterParams) {

        String parsefilterclass = null;

        JsonNode delegateNode = filterParams.get("delegate");
        if (delegateNode == null) {
            throw new RuntimeException("delegateNode undefined!");
        }

        JsonNode node = delegateNode.get("class");
        if (node != null && node.isTextual()) {
            parsefilterclass = node.asText();
        }

        if (parsefilterclass == null) {
            throw new RuntimeException("parsefilter.class undefined!");
        }

        // load an instance of the delegated parsefilter
        try {
            Class<?> filterClass = Class.forName(parsefilterclass);

            boolean subClassOK = ParseFilter.class.isAssignableFrom(filterClass);
            if (!subClassOK) {
                throw new RuntimeException(
                        "Filter " + parsefilterclass + " does not extend ParseFilter");
            }

            delegatedParseFilter = (ParseFilter) filterClass.newInstance();

            // check that it implements JSONResource
            if (!JSONResource.class.isInstance(delegatedParseFilter)) {
                throw new RuntimeException(
                        "Filter " + parsefilterclass + " does not implement JSONResource");
            }

        } catch (Exception e) {
            LOG.error("Can't setup {}: {}", parsefilterclass, e);
            throw new RuntimeException("Can't setup " + parsefilterclass, e);
        }

        // configure it
        node = delegateNode.get("params");

        delegatedParseFilter.configure(stormConf, node);

        int refreshRate = 600;

        node = filterParams.get("refresh");
        if (node != null && node.isInt()) {
            refreshRate = node.asInt(refreshRate);
        }

        final JSONResource resource = (JSONResource) delegatedParseFilter;

        new Timer()
                .schedule(
                        new TimerTask() {
                            private RestHighLevelClient esClient;

                            public void run() {
                                if (esClient == null) {
                                    try {
                                        esClient =
                                                ElasticSearchConnection.getClient(
                                                        stormConf, "config");
                                    } catch (Exception e) {
                                        LOG.error("Exception while creating ES connection", e);
                                    }
                                }
                                if (esClient != null) {
                                    LOG.info("Reloading json resources from ES");
                                    try {
                                        GetResponse response =
                                                esClient.get(
                                                        new GetRequest(
                                                                "config",
                                                                resource.getResourceFile()),
                                                        RequestOptions.DEFAULT);
                                        resource.loadJSONResources(
                                                new ByteArrayInputStream(
                                                        response.getSourceAsBytes()));
                                    } catch (Exception e) {
                                        LOG.error("Can't load config from ES", e);
                                    }
                                }
                            }
                        },
                        0,
                        refreshRate * 1000);
    }

    @Override
    public void filter(String URL, byte[] content, DocumentFragment doc, ParseResult parse) {
        delegatedParseFilter.filter(URL, content, doc, parse);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.elasticsearch.persistence;

import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.elasticsearch.ElasticSearchConnection;
import com.digitalpebble.stormcrawler.persistence.AbstractStatusUpdaterBolt;
import com.digitalpebble.stormcrawler.persistence.Status;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import com.digitalpebble.stormcrawler.util.URLPartitioner;
import com.github.benmanes.caffeine.cache.Cache;
import com.github.benmanes.caffeine.cache.Caffeine;
import com.github.benmanes.caffeine.cache.RemovalCause;
import com.github.benmanes.caffeine.cache.RemovalListener;
import java.util.Date;
import java.util.Iterator;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.Optional;
import java.util.concurrent.TimeUnit;
import org.apache.commons.lang.StringUtils;
import org.apache.storm.metric.api.MultiCountMetric;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.tuple.Tuple;
import org.checkerframework.checker.nullness.qual.NonNull;
import org.checkerframework.checker.nullness.qual.Nullable;
import org.elasticsearch.action.DocWriteRequest;
import org.elasticsearch.action.bulk.BulkItemResponse;
import org.elasticsearch.action.bulk.BulkProcessor;
import org.elasticsearch.action.bulk.BulkRequest;
import org.elasticsearch.action.bulk.BulkResponse;
import org.elasticsearch.action.index.IndexRequest;
import org.elasticsearch.common.xcontent.XContentBuilder;
import org.elasticsearch.rest.RestStatus;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Simple bolt which stores the status of URLs into ElasticSearch. Takes the tuples coming from the
 * 'status' stream. To be used in combination with a Spout to read from the index.
 */
@SuppressWarnings("serial")
public class StatusUpdaterBolt extends AbstractStatusUpdaterBolt
        implements RemovalListener<String, List<Tuple>>, BulkProcessor.Listener {

    private static final Logger LOG = LoggerFactory.getLogger(StatusUpdaterBolt.class);

    private String ESBoltType = "status";

    private static final String ESStatusIndexNameParamName = "es.%s.index.name";
    private static final String ESStatusRoutingParamName = "es.%s.routing";
    private static final String ESStatusRoutingFieldParamName = "es.%s.routing.fieldname";

    private boolean routingFieldNameInMetadata = false;

    private String indexName;

    private URLPartitioner partitioner;

    /** whether to apply the same partitioning logic used for politeness for routing, e.g byHost */
    private boolean doRouting;

    /** Store the key used for routing explicitly as a field in metadata * */
    private String fieldNameForRoutingKey = null;

    private ElasticSearchConnection connection;

    private Cache<String, List<Tuple>> waitAck;

    private MultiCountMetric eventCounter;

    public StatusUpdaterBolt() {
        super();
    }

    /**
     * Loads the configuration using a substring different from the default value 'status' in order
     * to distinguish it from the spout configurations
     */
    public StatusUpdaterBolt(String boltType) {
        super();
        ESBoltType = boltType;
    }

    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {

        super.prepare(stormConf, context, collector);

        indexName =
                ConfUtils.getString(
                        stormConf,
                        String.format(StatusUpdaterBolt.ESStatusIndexNameParamName, ESBoltType),
                        "status");

        doRouting =
                ConfUtils.getBoolean(
                        stormConf,
                        String.format(StatusUpdaterBolt.ESStatusRoutingParamName, ESBoltType),
                        false);

        partitioner = new URLPartitioner();
        partitioner.configure(stormConf);

        fieldNameForRoutingKey =
                ConfUtils.getString(
                        stormConf,
                        String.format(StatusUpdaterBolt.ESStatusRoutingFieldParamName, ESBoltType));
        if (StringUtils.isNotBlank(fieldNameForRoutingKey)) {
            if (fieldNameForRoutingKey.startsWith("metadata.")) {
                routingFieldNameInMetadata = true;
                fieldNameForRoutingKey = fieldNameForRoutingKey.substring("metadata.".length());
            }
            // periods are not allowed in ES2 - replace with %2E
            fieldNameForRoutingKey = fieldNameForRoutingKey.replaceAll("\\.", "%2E");
        }

        waitAck =
                Caffeine.newBuilder()
                        .expireAfterWrite(60, TimeUnit.SECONDS)
                        .removalListener(this)
                        .build();

        // create gauge for waitAck
        context.registerMetric(
                "waitAck",
                () -> {
                    return waitAck.estimatedSize();
                },
                10);

        try {
            connection = ElasticSearchConnection.getConnection(stormConf, ESBoltType, this);
        } catch (Exception e1) {
            LOG.error("Can't connect to ElasticSearch", e1);
            throw new RuntimeException(e1);
        }

        this.eventCounter = context.registerMetric("counters", new MultiCountMetric(), 30);
    }

    @Override
    public void cleanup() {
        if (connection != null) connection.close();
    }

    @Override
    public void store(
            String url, Status status, Metadata metadata, Optional<Date> nextFetch, Tuple tuple)
            throws Exception {

        String sha256hex = org.apache.commons.codec.digest.DigestUtils.sha256Hex(url);

        // need to synchronize: otherwise it might get added to the cache
        // without having been sent to ES
        synchronized (waitAck) {
            // check that the same URL is not being sent to ES
            List<Tuple> alreadySent = waitAck.getIfPresent(sha256hex);
            if (alreadySent != null && status.equals(Status.DISCOVERED)) {
                // if this object is discovered - adding another version of it
                // won't make any difference
                LOG.debug(
                        "Already being sent to ES {} with status {} and ID {}",
                        url,
                        status,
                        sha256hex);
                // ack straight away!
                eventCounter.scope("acked").incrBy(1);
                super.ack(tuple, url);
                return;
            }
        }

        XContentBuilder builder = jsonBuilder().startObject();
        builder.field("url", url);
        builder.field("status", status);

        // check that we don't overwrite an existing entry
        // When create is used, the index operation will fail if a document
        // by that id already exists in the index.
        boolean create = status.equals(Status.DISCOVERED);

        builder.startObject("metadata");
        Iterator<String> mdKeys = metadata.keySet().iterator();
        while (mdKeys.hasNext()) {
            String mdKey = mdKeys.next();
            String[] values = metadata.getValues(mdKey);
            // periods are not allowed in ES2 - replace with %2E
            mdKey = mdKey.replaceAll("\\.", "%2E");
            builder.array(mdKey, values);
        }

        String partitionKey = partitioner.getPartition(url, metadata);
        if (partitionKey == null) {
            partitionKey = "_DEFAULT_";
        }

        // store routing key in metadata?
        if (StringUtils.isNotBlank(fieldNameForRoutingKey) && routingFieldNameInMetadata) {
            builder.field(fieldNameForRoutingKey, partitionKey);
        }

        builder.endObject();

        // store routing key outside metadata?
        if (StringUtils.isNotBlank(fieldNameForRoutingKey) && !routingFieldNameInMetadata) {
            builder.field(fieldNameForRoutingKey, partitionKey);
        }

        if (nextFetch.isPresent()) {
            builder.timeField("nextFetchDate", nextFetch.get());
        }

        builder.endObject();

        IndexRequest request = new IndexRequest(getIndexName(metadata));
        request.source(builder).id(sha256hex).create(create);

        if (doRouting) {
            request.routing(partitionKey);
        }

        synchronized (waitAck) {
            List<Tuple> tt = waitAck.getIfPresent(sha256hex);
            if (tt == null) {
                tt = new LinkedList<>();
                waitAck.put(sha256hex, tt);
            }
            tt.add(tuple);
            LOG.debug("Added to waitAck {} with ID {} total {}", url, sha256hex, tt.size());
        }

        LOG.debug("Sending to ES buffer {} with ID {}", url, sha256hex);

        connection.getProcessor().add(request);
    }

    @Override
    public void onRemoval(
            @Nullable String key, @Nullable List<Tuple> value, @NonNull RemovalCause cause) {
        if (!cause.wasEvicted()) return;
        LOG.error("Purged from waitAck {} with {} values", key, value.size());
        for (Tuple t : value) {
            eventCounter.scope("failed").incrBy(1);
            _collector.fail(t);
        }
    }

    @Override
    public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {
        LOG.debug("afterBulk [{}] with {} responses", executionId, request.numberOfActions());
        long msec = response.getTook().getMillis();
        eventCounter.scope("bulks_received").incrBy(1);
        eventCounter.scope("bulk_msec").incrBy(msec);
        Iterator<BulkItemResponse> bulkitemiterator = response.iterator();
        int itemcount = 0;
        int acked = 0;
        int failurecount = 0;

        synchronized (waitAck) {
            while (bulkitemiterator.hasNext()) {
                BulkItemResponse bir = bulkitemiterator.next();
                itemcount++;
                String id = bir.getId();
                BulkItemResponse.Failure f = bir.getFailure();
                boolean failed = false;
                if (f != null) {
                    // already discovered
                    if (f.getStatus().equals(RestStatus.CONFLICT)) {
                        eventCounter.scope("doc_conflicts").incrBy(1);
                        LOG.debug("Doc conflict ID {}", id);
                    } else {
                        LOG.error("Update ID {}, failure: {}", id, f);
                        failed = true;
                    }
                }
                List<Tuple> xx = waitAck.getIfPresent(id);
                if (xx != null) {
                    LOG.debug("Acked {} tuple(s) for ID {}", xx.size(), id);
                    for (Tuple x : xx) {
                        if (!failed) {
                            String url = x.getStringByField("url");
                            acked++;
                            // ack and put in cache
                            LOG.debug("Acked {} with ID {}", url, id);
                            eventCounter.scope("acked").incrBy(1);
                            super.ack(x, url);
                        } else {
                            failurecount++;
                            eventCounter.scope("failed").incrBy(1);
                            _collector.fail(x);
                        }
                    }
                    waitAck.invalidate(id);
                } else {
                    LOG.warn("Could not find unacked tuple for {}", id);
                }
            }

            LOG.info(
                    "Bulk response [{}] : items {}, waitAck {}, acked {}, failed {}",
                    executionId,
                    itemcount,
                    waitAck.estimatedSize(),
                    acked,
                    failurecount);
            if (waitAck.estimatedSize() > 0 && LOG.isDebugEnabled()) {
                for (String kinaw : waitAck.asMap().keySet()) {
                    LOG.debug(
                            "Still in wait ack after bulk response [{}] => {}", executionId, kinaw);
                }
            }
        }
    }

    @Override
    public void afterBulk(long executionId, BulkRequest request, Throwable throwable) {
        eventCounter.scope("bulks_received").incrBy(1);
        LOG.error("Exception with bulk {} - failing the whole lot ", executionId, throwable);
        synchronized (waitAck) {
            // WHOLE BULK FAILED
            // mark all the docs as fail
            Iterator<DocWriteRequest<?>> itreq = request.requests().iterator();
            while (itreq.hasNext()) {
                DocWriteRequest bir = itreq.next();
                String id = bir.id();
                List<Tuple> xx = waitAck.getIfPresent(id);
                if (xx != null) {
                    LOG.debug("Failed {} tuple(s) for ID {}", xx.size(), id);
                    for (Tuple x : xx) {
                        // fail it
                        eventCounter.scope("failed").incrBy(1);
                        _collector.fail(x);
                    }
                    waitAck.invalidate(id);
                } else {
                    LOG.warn("Could not find unacked tuple for {}", id);
                }
            }
        }
    }

    @Override
    public void beforeBulk(long executionId, BulkRequest request) {
        LOG.debug("beforeBulk {} with {} actions", executionId, request.numberOfActions());
        eventCounter.scope("bulks_received").incrBy(1);
    }

    /**
     * Must be overridden for implementing custom index names based on some metadata information By
     * Default, indexName coming from config is used
     */
    protected String getIndexName(Metadata m) {
        return indexName;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.elasticsearch.persistence;

import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.persistence.AbstractStatusUpdaterBolt;
import com.digitalpebble.stormcrawler.persistence.Status;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.Queue;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.tuple.Fields;
import org.apache.storm.tuple.Values;
import org.apache.storm.utils.Utils;
import org.elasticsearch.action.ActionListener;
import org.elasticsearch.action.search.SearchRequest;
import org.elasticsearch.action.search.SearchResponse;
import org.elasticsearch.action.search.SearchScrollRequest;
import org.elasticsearch.client.RequestOptions;
import org.elasticsearch.common.unit.TimeValue;
import org.elasticsearch.index.query.QueryBuilders;
import org.elasticsearch.search.SearchHit;
import org.elasticsearch.search.SearchHits;
import org.elasticsearch.search.builder.SearchSourceBuilder;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Reads all the documents from a shard and emits them on the status stream. Used for copying an
 * index.
 */
public class ScrollSpout extends AbstractSpout implements ActionListener<SearchResponse> {

    private String scrollId = null;
    private boolean hasFinished = false;

    private Queue<Values> queue = new LinkedList<>();

    private static final Logger LOG = LoggerFactory.getLogger(ScrollSpout.class);

    @Override
    // simplified version of the super method so that we can store the fields in
    // the
    // map of things being processed
    public void nextTuple() {
        synchronized (queue) {
            if (!queue.isEmpty()) {
                List<Object> fields = queue.remove();
                String url = fields.get(0).toString();
                _collector.emit(Constants.StatusStreamName, fields, url);
                beingProcessed.put(url, fields);
                eventCounter.scope("emitted").incrBy(1);
                LOG.debug("{} emitted {}", logIdprefix, url);
                return;
            }
        }

        if (isInQuery.get()) {
            // sleep for a bit but not too much in order to give ack/fail a
            // chance
            Utils.sleep(10);
            return;
        }

        // re-populate the buffer
        populateBuffer();
    }

    @Override
    protected void populateBuffer() {
        if (hasFinished) {
            Utils.sleep(10);
            return;
        }

        // initial request
        if (scrollId == null) {
            SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();
            searchSourceBuilder.query(QueryBuilders.matchAllQuery());
            searchSourceBuilder.size(maxURLsPerBucket * maxBucketNum);
            SearchRequest searchRequest = new SearchRequest(indexName);
            searchRequest.source(searchSourceBuilder);
            searchRequest.scroll(TimeValue.timeValueMinutes(5L));

            // specific shard but ideally a local copy of it
            if (shardID != -1) {
                searchRequest.preference("_shards:" + shardID + "|_local");
            }

            isInQuery.set(true);
            client.searchAsync(searchRequest, RequestOptions.DEFAULT, this);

            // dump query to log
            LOG.debug("{} ES query {}", logIdprefix, searchRequest.toString());
            return;
        }

        SearchScrollRequest scrollRequest = new SearchScrollRequest(scrollId);
        scrollRequest.scroll(TimeValue.timeValueMinutes(5L));

        isInQuery.set(true);
        client.scrollAsync(scrollRequest, RequestOptions.DEFAULT, this);
        // dump query to log
        LOG.debug("{} ES query {}", logIdprefix, scrollRequest.toString());
    }

    @Override
    public void onResponse(SearchResponse response) {
        SearchHits hits = response.getHits();
        LOG.info(
                "{} ES query returned {} hits in {} msec",
                logIdprefix,
                hits.getHits().length,
                response.getTook().getMillis());
        hasFinished = hits.getHits().length == 0;
        synchronized (this.queue) {
            // Unlike standard spouts, the scroll queries should never return
            // the same
            // document twice -> no need to look in the buffer or cache
            for (SearchHit hit : hits) {
                Map<String, Object> keyValues = hit.getSourceAsMap();
                String url = (String) keyValues.get("url");
                String status = (String) keyValues.get("status");
                String nextFetchDate = (String) keyValues.get("nextFetchDate");
                Metadata metadata = fromKeyValues(keyValues);
                metadata.setValue(
                        AbstractStatusUpdaterBolt.AS_IS_NEXTFETCHDATE_METADATA, nextFetchDate);
                this.queue.add(new Values(url, metadata, Status.valueOf(status)));
            }
        }
        scrollId = response.getScrollId();
        // remove lock
        markQueryReceivedNow();
    }

    @Override
    public void onFailure(Exception e) {
        LOG.error("{} Exception with ES query", logIdprefix, e);
        markQueryReceivedNow();
    }

    @Override
    public void fail(Object msgId) {
        LOG.info("{}  Fail for {}", logIdprefix, msgId);
        eventCounter.scope("failed").incrBy(1);
        // retrieve the values from being processed and send them back to the
        // queue
        Values v = (Values) beingProcessed.remove(msgId);
        queue.add(v);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declareStream(Constants.StatusStreamName, new Fields("url", "metadata", "status"));
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.elasticsearch.persistence;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.elasticsearch.ElasticSearchConnection;
import com.digitalpebble.stormcrawler.persistence.AbstractQueryingSpout;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import java.io.IOException;
import java.util.Date;
import java.util.Iterator;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import org.apache.storm.spout.SpoutOutputCollector;
import org.apache.storm.task.TopologyContext;
import org.elasticsearch.client.RestHighLevelClient;
import org.elasticsearch.search.SearchHit;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public abstract class AbstractSpout extends AbstractQueryingSpout {

    private static final Logger LOG = LoggerFactory.getLogger(AbstractSpout.class);

    protected static final String ESBoltType = "status";
    protected static final String ESStatusIndexNameParamName = "es.status.index.name";

    /** Field name to use for aggregating * */
    protected static final String ESStatusBucketFieldParamName = "es.status.bucket.field";

    protected static final String ESStatusMaxBucketParamName = "es.status.max.buckets";
    protected static final String ESStatusMaxURLsParamName = "es.status.max.urls.per.bucket";

    /** Field name to use for sorting the URLs within a bucket, not used if empty or null. */
    protected static final String ESStatusBucketSortFieldParamName = "es.status.bucket.sort.field";

    /** Field name to use for sorting the buckets, not used if empty or null. */
    protected static final String ESStatusGlobalSortFieldParamName = "es.status.global.sort.field";

    protected static final String ESStatusFilterParamName = "es.status.filterQuery";

    protected static final String ESStatusQueryTimeoutParamName = "es.status.query.timeout";

    /** Query to use as a positive filter, set by es.status.filterQuery */
    protected List<String> filterQueries = null;

    protected String indexName;

    protected static RestHighLevelClient client;

    /**
     * when using multiple instances - each one is in charge of a specific shard useful when
     * sharding based on host or domain to guarantee a good mix of URLs
     */
    protected int shardID = -1;

    /** Used to distinguish between instances in the logs * */
    protected String logIdprefix = "";

    /** Field name used for field collapsing e.g. key * */
    protected String partitionField;

    protected int maxURLsPerBucket = 10;

    protected int maxBucketNum = 10;

    protected List<String> bucketSortField = new LinkedList<>();

    protected String totalSortField = "";

    protected Date queryDate;

    protected int queryTimeout = -1;

    @Override
    public void open(Map stormConf, TopologyContext context, SpoutOutputCollector collector) {

        super.open(stormConf, context, collector);

        indexName = ConfUtils.getString(stormConf, ESStatusIndexNameParamName, "status");

        // one ES client per JVM
        synchronized (AbstractSpout.class) {
            try {
                if (client == null) {
                    client = ElasticSearchConnection.getClient(stormConf, ESBoltType);
                }
            } catch (Exception e1) {
                LOG.error("Can't connect to ElasticSearch", e1);
                throw new RuntimeException(e1);
            }
        }

        // if more than one instance is used we expect their number to be the
        // same as the number of shards
        int totalTasks = context.getComponentTasks(context.getThisComponentId()).size();
        if (totalTasks > 1) {
            logIdprefix =
                    "[" + context.getThisComponentId() + " #" + context.getThisTaskIndex() + "] ";

            // determine the number of shards so that we can restrict the
            // search

            // TODO use the admin API when it gets available
            // TODO or the low level one with
            // https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-shards-stores.html
            // TODO identify local shards and use those if possible

            // ClusterSearchShardsRequest request = new
            // ClusterSearchShardsRequest(
            // indexName);
            // ClusterSearchShardsResponse shardresponse = client.admin()
            // .cluster().searchShards(request).actionGet();
            // ClusterSearchShardsGroup[] shardgroups =
            // shardresponse.getGroups();
            // if (totalTasks != shardgroups.length) {
            // throw new RuntimeException(
            // "Number of ES spout instances should be the same as number of
            // shards ("
            // + shardgroups.length + ") but is " + totalTasks);
            // }
            // shardID = shardgroups[context.getThisTaskIndex()].getShardId()
            // .getId();

            // TEMPORARY simply use the task index as shard index
            shardID = context.getThisTaskIndex();
            LOG.info("{} assigned shard ID {}", logIdprefix, shardID);
        }

        partitionField = ConfUtils.getString(stormConf, ESStatusBucketFieldParamName, "key");

        bucketSortField = ConfUtils.loadListFromConf(ESStatusBucketSortFieldParamName, stormConf);

        totalSortField = ConfUtils.getString(stormConf, ESStatusGlobalSortFieldParamName);

        maxURLsPerBucket = ConfUtils.getInt(stormConf, ESStatusMaxURLsParamName, 1);
        maxBucketNum = ConfUtils.getInt(stormConf, ESStatusMaxBucketParamName, 10);

        queryTimeout = ConfUtils.getInt(stormConf, ESStatusQueryTimeoutParamName, -1);

        filterQueries = ConfUtils.loadListFromConf(ESStatusFilterParamName, stormConf);
    }

    /** Builds a query and use it retrieve the results from ES * */
    protected abstract void populateBuffer();

    protected final boolean addHitToBuffer(SearchHit hit) {
        Map<String, Object> keyValues = hit.getSourceAsMap();
        String url = (String) keyValues.get("url");
        // is already being processed - skip it!
        if (beingProcessed.containsKey(url)) {
            return false;
        }
        return buffer.add(url, fromKeyValues(keyValues));
    }

    protected final Metadata fromKeyValues(Map<String, Object> keyValues) {
        Map<String, List<String>> mdAsMap = (Map<String, List<String>>) keyValues.get("metadata");
        Metadata metadata = new Metadata();
        if (mdAsMap != null) {
            Iterator<Entry<String, List<String>>> mdIter = mdAsMap.entrySet().iterator();
            while (mdIter.hasNext()) {
                Entry<String, List<String>> mdEntry = mdIter.next();
                String key = mdEntry.getKey();
                // periods are not allowed in ES2 - replace with %2E
                key = key.replaceAll("%2E", "\\.");
                Object mdValObj = mdEntry.getValue();
                // single value
                if (mdValObj instanceof String) {
                    metadata.addValue(key, (String) mdValObj);
                }
                // multi valued
                else {
                    metadata.addValues(key, (List<String>) mdValObj);
                }
            }
        }
        return metadata;
    }

    @Override
    public void ack(Object msgId) {
        LOG.debug("{}  Ack for {}", logIdprefix, msgId);
        super.ack(msgId);
    }

    @Override
    public void fail(Object msgId) {
        LOG.info("{}  Fail for {}", logIdprefix, msgId);
        super.fail(msgId);
    }

    @Override
    public void close() {
        if (client != null)
            try {
                client.close();
            } catch (IOException e) {
            }
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.elasticsearch.persistence;

import static org.elasticsearch.index.query.QueryBuilders.boolQuery;

import com.digitalpebble.stormcrawler.util.ConfUtils;
import java.time.Instant;
import java.util.Date;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.concurrent.TimeUnit;
import org.apache.commons.lang.StringUtils;
import org.apache.storm.spout.SpoutOutputCollector;
import org.apache.storm.task.TopologyContext;
import org.elasticsearch.action.ActionListener;
import org.elasticsearch.action.search.SearchRequest;
import org.elasticsearch.action.search.SearchResponse;
import org.elasticsearch.client.RequestOptions;
import org.elasticsearch.common.unit.TimeValue;
import org.elasticsearch.index.query.BoolQueryBuilder;
import org.elasticsearch.index.query.InnerHitBuilder;
import org.elasticsearch.index.query.QueryBuilders;
import org.elasticsearch.search.SearchHit;
import org.elasticsearch.search.SearchHits;
import org.elasticsearch.search.builder.SearchSourceBuilder;
import org.elasticsearch.search.collapse.CollapseBuilder;
import org.elasticsearch.search.sort.FieldSortBuilder;
import org.elasticsearch.search.sort.SortBuilder;
import org.elasticsearch.search.sort.SortBuilders;
import org.elasticsearch.search.sort.SortOrder;
import org.joda.time.format.ISODateTimeFormat;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Spout which pulls URL from an ES index. Use a single instance unless you use 'es.status.routing'
 * with the StatusUpdaterBolt, in which case you need to have exactly the same number of spout
 * instances as ES shards. Collapses results to implement politeness and ensure a good diversity of
 * sources.
 */
public class CollapsingSpout extends AbstractSpout implements ActionListener<SearchResponse> {

    private static final Logger LOG = LoggerFactory.getLogger(CollapsingSpout.class);

    /** Used to avoid deep paging * */
    private static final String ESMaxStartOffsetParamName = "es.status.max.start.offset";

    private int lastStartOffset = 0;
    private int maxStartOffset = -1;

    @Override
    public void open(Map stormConf, TopologyContext context, SpoutOutputCollector collector) {
        maxStartOffset = ConfUtils.getInt(stormConf, ESMaxStartOffsetParamName, -1);
        super.open(stormConf, context, collector);
    }

    @Override
    protected void populateBuffer() {
        // not used yet or returned empty results
        if (queryDate == null) {
            queryDate = new Date();
            lastTimeResetToNOW = Instant.now();
            lastStartOffset = 0;
        }
        // been running same query for too long and paging deep?
        else if (maxStartOffset != -1 && lastStartOffset > maxStartOffset) {
            LOG.info("Reached max start offset {}", lastStartOffset);
            lastStartOffset = 0;
        }

        String formattedLastDate = ISODateTimeFormat.dateTimeNoMillis().print(queryDate.getTime());

        LOG.info("{} Populating buffer with nextFetchDate <= {}", logIdprefix, formattedLastDate);

        BoolQueryBuilder queryBuilder =
                boolQuery()
                        .filter(QueryBuilders.rangeQuery("nextFetchDate").lte(formattedLastDate));

        if (filterQueries != null) {
            for (String filterQuery : filterQueries) {
                queryBuilder.filter(QueryBuilders.queryStringQuery(filterQuery));
            }
        }

        SearchRequest request = new SearchRequest(indexName);

        SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();
        sourceBuilder.query(queryBuilder);
        sourceBuilder.from(lastStartOffset);
        sourceBuilder.size(maxBucketNum);
        sourceBuilder.explain(false);
        sourceBuilder.trackTotalHits(false);

        // https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-preference.html
        // _shards:2,3
        // specific shard but ideally a local copy of it
        if (shardID != -1) {
            request.preference("_shards:" + shardID + "|_local");
        }

        if (queryTimeout != -1) {
            sourceBuilder.timeout(new TimeValue(queryTimeout, TimeUnit.SECONDS));
        }

        if (StringUtils.isNotBlank(totalSortField)) {
            sourceBuilder.sort(new FieldSortBuilder(totalSortField).order(SortOrder.ASC));
        }

        CollapseBuilder collapse = new CollapseBuilder(partitionField);

        // group expansion -> sends sub queries for each bucket
        if (maxURLsPerBucket > 1) {
            InnerHitBuilder ihb = new InnerHitBuilder();
            ihb.setSize(maxURLsPerBucket);
            ihb.setName("urls_per_bucket");
            List<SortBuilder<?>> sorts = new LinkedList<>();
            // sort within a bucket
            for (String bsf : bucketSortField) {
                FieldSortBuilder bucketsorter = SortBuilders.fieldSort(bsf).order(SortOrder.ASC);
                sorts.add(bucketsorter);
            }
            if (!sorts.isEmpty()) {
                ihb.setSorts(sorts);
            }
            collapse.setInnerHits(ihb);
        }

        sourceBuilder.collapse(collapse);

        request.source(sourceBuilder);

        // dump query to log
        LOG.debug("{} ES query {}", logIdprefix, request.toString());

        isInQuery.set(true);
        client.searchAsync(request, RequestOptions.DEFAULT, this);
    }

    @Override
    public void onFailure(Exception e) {
        LOG.error("{} Exception with ES query", logIdprefix, e);
        markQueryReceivedNow();
    }

    @Override
    public void onResponse(SearchResponse response) {
        long timeTaken = System.currentTimeMillis() - getTimeLastQuerySent();

        SearchHit[] hits = response.getHits().getHits();
        int numBuckets = hits.length;

        int alreadyprocessed = 0;
        int numDocs = 0;

        for (SearchHit hit : hits) {
            Map<String, SearchHits> innerHits = hit.getInnerHits();
            // wanted just one per bucket : no inner hits
            if (innerHits == null) {
                numDocs++;
                if (!addHitToBuffer(hit)) {
                    alreadyprocessed++;
                }
                continue;
            }
            // more than one per bucket
            SearchHits inMyBucket = innerHits.get("urls_per_bucket");
            for (SearchHit subHit : inMyBucket.getHits()) {
                numDocs++;
                if (!addHitToBuffer(subHit)) {
                    alreadyprocessed++;
                }
            }
        }

        queryTimes.addMeasurement(timeTaken);
        // could be derived from the count of query times above
        eventCounter.scope("ES_queries").incrBy(1);
        eventCounter.scope("ES_docs").incrBy(numDocs);
        eventCounter.scope("already_being_processed").incrBy(alreadyprocessed);

        LOG.info(
                "{} ES query returned {} hits from {} buckets in {} msec with {} already being processed.Took {} msec per doc on average.",
                logIdprefix,
                numDocs,
                numBuckets,
                timeTaken,
                alreadyprocessed,
                ((float) timeTaken / numDocs));

        // reset the value for next fetch date if the previous one is too old
        if (resetFetchDateAfterNSecs != -1) {
            Instant changeNeededOn =
                    Instant.ofEpochMilli(
                            lastTimeResetToNOW.toEpochMilli() + (resetFetchDateAfterNSecs * 1000));
            if (Instant.now().isAfter(changeNeededOn)) {
                LOG.info(
                        "queryDate reset based on resetFetchDateAfterNSecs {}",
                        resetFetchDateAfterNSecs);
                queryDate = null;
                lastStartOffset = 0;
            }
        }

        // no more results?
        if (numBuckets == 0) {
            queryDate = null;
            lastStartOffset = 0;
        }
        // still got some results but paging won't help
        else if (numBuckets < maxBucketNum) {
            lastStartOffset = 0;
        } else {
            lastStartOffset += numBuckets;
        }

        // remove lock
        markQueryReceivedNow();
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.elasticsearch.persistence;

import static org.elasticsearch.index.query.QueryBuilders.boolQuery;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import java.time.Instant;
import java.util.Calendar;
import java.util.Date;
import java.util.HashSet;
import java.util.Iterator;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.TimeUnit;
import org.apache.commons.lang.StringUtils;
import org.apache.storm.spout.SpoutOutputCollector;
import org.apache.storm.task.TopologyContext;
import org.elasticsearch.action.ActionListener;
import org.elasticsearch.action.search.SearchRequest;
import org.elasticsearch.action.search.SearchResponse;
import org.elasticsearch.client.RequestOptions;
import org.elasticsearch.common.unit.TimeValue;
import org.elasticsearch.index.query.BoolQueryBuilder;
import org.elasticsearch.index.query.QueryBuilders;
import org.elasticsearch.search.SearchHit;
import org.elasticsearch.search.aggregations.AggregationBuilders;
import org.elasticsearch.search.aggregations.Aggregations;
import org.elasticsearch.search.aggregations.BucketOrder;
import org.elasticsearch.search.aggregations.bucket.SingleBucketAggregation;
import org.elasticsearch.search.aggregations.bucket.sampler.DiversifiedAggregationBuilder;
import org.elasticsearch.search.aggregations.bucket.terms.Terms;
import org.elasticsearch.search.aggregations.bucket.terms.Terms.Bucket;
import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregationBuilder;
import org.elasticsearch.search.aggregations.metrics.TopHits;
import org.elasticsearch.search.builder.SearchSourceBuilder;
import org.elasticsearch.search.sort.FieldSortBuilder;
import org.elasticsearch.search.sort.SortBuilders;
import org.elasticsearch.search.sort.SortOrder;
import org.joda.time.format.ISODateTimeFormat;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Spout which pulls URL from an ES index. Use a single instance unless you use 'es.status.routing'
 * with the StatusUpdaterBolt, in which case you need to have exactly the same number of spout
 * instances as ES shards. Guarantees a good mix of URLs by aggregating them by an arbitrary field
 * e.g. key.
 */
@SuppressWarnings("serial")
public class AggregationSpout extends AbstractSpout implements ActionListener<SearchResponse> {

    private static final Logger LOG = LoggerFactory.getLogger(AggregationSpout.class);

    private static final String ESStatusSampleParamName = "es.status.sample";
    private static final String ESMostRecentDateIncreaseParamName = "es.status.recentDate.increase";
    private static final String ESMostRecentDateMinGapParamName = "es.status.recentDate.min.gap";

    private boolean sample = false;

    private int recentDateIncrease = -1;
    private int recentDateMinGap = -1;

    protected Set<String> currentBuckets;

    @Override
    public void open(Map stormConf, TopologyContext context, SpoutOutputCollector collector) {
        sample = ConfUtils.getBoolean(stormConf, ESStatusSampleParamName, sample);
        recentDateIncrease =
                ConfUtils.getInt(stormConf, ESMostRecentDateIncreaseParamName, recentDateIncrease);
        recentDateMinGap =
                ConfUtils.getInt(stormConf, ESMostRecentDateMinGapParamName, recentDateMinGap);
        super.open(stormConf, context, collector);
        currentBuckets = new HashSet<>();
    }

    @Override
    protected void populateBuffer() {

        if (queryDate == null) {
            queryDate = new Date();
            lastTimeResetToNOW = Instant.now();
        }

        String formattedQueryDate = ISODateTimeFormat.dateTimeNoMillis().print(queryDate.getTime());

        LOG.info("{} Populating buffer with nextFetchDate <= {}", logIdprefix, formattedQueryDate);

        BoolQueryBuilder queryBuilder =
                boolQuery()
                        .filter(QueryBuilders.rangeQuery("nextFetchDate").lte(formattedQueryDate));

        if (filterQueries != null) {
            for (String filterQuery : filterQueries) {
                queryBuilder.filter(QueryBuilders.queryStringQuery(filterQuery));
            }
        }

        SearchRequest request = new SearchRequest(indexName);

        SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();
        sourceBuilder.query(queryBuilder);
        sourceBuilder.from(0);
        sourceBuilder.size(0);
        sourceBuilder.explain(false);
        sourceBuilder.trackTotalHits(false);

        if (queryTimeout != -1) {
            sourceBuilder.timeout(new TimeValue(queryTimeout, TimeUnit.SECONDS));
        }

        TermsAggregationBuilder aggregations =
                AggregationBuilders.terms("partition").field(partitionField).size(maxBucketNum);

        org.elasticsearch.search.aggregations.metrics.TopHitsAggregationBuilder tophits =
                AggregationBuilders.topHits("docs").size(maxURLsPerBucket).explain(false);

        // sort within a bucket
        for (String bsf : bucketSortField) {
            FieldSortBuilder sorter = SortBuilders.fieldSort(bsf).order(SortOrder.ASC);
            tophits.sort(sorter);
        }

        aggregations.subAggregation(tophits);

        // sort between buckets
        if (StringUtils.isNotBlank(totalSortField)) {
            org.elasticsearch.search.aggregations.metrics.MinAggregationBuilder minBuilder =
                    AggregationBuilders.min("top_hit").field(totalSortField);
            aggregations.subAggregation(minBuilder);
            aggregations.order(BucketOrder.aggregation("top_hit", true));
        }

        if (sample) {
            DiversifiedAggregationBuilder sab = new DiversifiedAggregationBuilder("sample");
            sab.field(partitionField).maxDocsPerValue(maxURLsPerBucket);
            sab.shardSize(maxURLsPerBucket * maxBucketNum);
            sab.subAggregation(aggregations);
            sourceBuilder.aggregation(sab);
        } else {
            sourceBuilder.aggregation(aggregations);
        }

        request.source(sourceBuilder);

        // https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-preference.html
        // _shards:2,3
        // specific shard but ideally a local copy of it
        if (shardID != -1) {
            request.preference("_shards:" + shardID + "|_local");
        }

        // dump query to log
        LOG.debug("{} ES query {}", logIdprefix, request);

        isInQuery.set(true);
        client.searchAsync(request, RequestOptions.DEFAULT, this);
    }

    @Override
    public void onFailure(Exception arg0) {
        LOG.error("{} Exception with ES query", logIdprefix, arg0);
        markQueryReceivedNow();
    }

    @Override
    public void onResponse(SearchResponse response) {
        long timeTaken = System.currentTimeMillis() - getTimeLastQuerySent();

        Aggregations aggregs = response.getAggregations();

        if (aggregs == null) {
            markQueryReceivedNow();
            return;
        }

        SingleBucketAggregation sample = aggregs.get("sample");
        if (sample != null) {
            aggregs = sample.getAggregations();
        }

        Terms agg = aggregs.get("partition");

        int numhits = 0;
        int numBuckets = 0;
        int alreadyprocessed = 0;

        Instant mostRecentDateFound = null;

        currentBuckets.clear();

        // For each entry
        Iterator<Terms.Bucket> iterator = (Iterator<Bucket>) agg.getBuckets().iterator();
        while (iterator.hasNext()) {
            Terms.Bucket entry = iterator.next();
            String key = (String) entry.getKey(); // bucket key

            currentBuckets.add(key);

            long docCount = entry.getDocCount(); // Doc count

            int hitsForThisBucket = 0;

            SearchHit lastHit = null;

            // filter results so that we don't include URLs we are already
            // being processed
            TopHits topHits = entry.getAggregations().get("docs");
            for (SearchHit hit : topHits.getHits().getHits()) {
                hitsForThisBucket++;

                lastHit = hit;

                Map<String, Object> keyValues = hit.getSourceAsMap();
                String url = (String) keyValues.get("url");
                LOG.debug(
                        "{} -> id [{}], _source [{}]",
                        logIdprefix,
                        hit.getId(),
                        hit.getSourceAsString());

                // consider only the first document of the last bucket
                // for optimising the nextFetchDate
                if (hitsForThisBucket == 1 && !iterator.hasNext()) {
                    String strDate = (String) keyValues.get("nextFetchDate");
                    try {
                        mostRecentDateFound = Instant.parse(strDate);
                    } catch (Exception e) {
                        throw new RuntimeException("can't parse date :" + strDate);
                    }
                }

                // is already being processed or in buffer - skip it!
                if (beingProcessed.containsKey(url)) {
                    LOG.debug("{} -> already processed: {}", logIdprefix, url);
                    alreadyprocessed++;
                    continue;
                }

                Metadata metadata = fromKeyValues(keyValues);
                boolean added = buffer.add(url, metadata);
                if (!added) {
                    LOG.debug("{} -> already in buffer: {}", logIdprefix, url);
                    alreadyprocessed++;
                    continue;
                }
                LOG.debug("{} -> added to buffer : {}", logIdprefix, url);
            }

            if (lastHit != null) {
                sortValuesForKey(key, lastHit.getSortValues());
            }

            if (hitsForThisBucket > 0) numBuckets++;

            numhits += hitsForThisBucket;

            LOG.debug(
                    "{} key [{}], hits[{}], doc_count [{}]",
                    logIdprefix,
                    key,
                    hitsForThisBucket,
                    docCount,
                    alreadyprocessed);
        }

        LOG.info(
                "{} ES query returned {} hits from {} buckets in {} msec with {} already being processed. Took {} msec per doc on average.",
                logIdprefix,
                numhits,
                numBuckets,
                timeTaken,
                alreadyprocessed,
                ((float) timeTaken / numhits));

        queryTimes.addMeasurement(timeTaken);
        eventCounter.scope("already_being_processed").incrBy(alreadyprocessed);
        eventCounter.scope("ES_queries").incrBy(1);
        eventCounter.scope("ES_docs").incrBy(numhits);

        // optimise the nextFetchDate by getting the most recent value
        // returned in the query and add to it, unless the previous value is
        // within n mins in which case we'll keep it
        if (mostRecentDateFound != null && recentDateIncrease >= 0) {
            Calendar potentialNewDate = Calendar.getInstance();
            potentialNewDate.setTimeInMillis(mostRecentDateFound.getEpochSecond());
            potentialNewDate.add(Calendar.MINUTE, recentDateIncrease);
            Date oldDate = null;
            // check boundaries
            if (this.recentDateMinGap > 0) {
                Calendar low = Calendar.getInstance();
                low.setTime(queryDate);
                low.add(Calendar.MINUTE, -recentDateMinGap);
                Calendar high = Calendar.getInstance();
                high.setTime(queryDate);
                high.add(Calendar.MINUTE, recentDateMinGap);
                if (high.before(potentialNewDate) || low.after(potentialNewDate)) {
                    oldDate = queryDate;
                }
            } else {
                oldDate = queryDate;
            }
            if (oldDate != null) {
                queryDate = potentialNewDate.getTime();
                LOG.info(
                        "{} queryDate changed from {} to {} based on mostRecentDateFound {}",
                        logIdprefix,
                        oldDate,
                        queryDate,
                        mostRecentDateFound);
            } else {
                LOG.info(
                        "{} queryDate kept at {} based on mostRecentDateFound {}",
                        logIdprefix,
                        queryDate,
                        mostRecentDateFound);
            }
        }

        // reset the value for next fetch date if the previous one is too old
        if (resetFetchDateAfterNSecs != -1) {
            Instant changeNeededOn =
                    Instant.ofEpochMilli(
                            lastTimeResetToNOW.toEpochMilli() + (resetFetchDateAfterNSecs * 1000));
            if (Instant.now().isAfter(changeNeededOn)) {
                LOG.info(
                        "{} queryDate set to null based on resetFetchDateAfterNSecs {}",
                        logIdprefix,
                        resetFetchDateAfterNSecs);
                queryDate = null;
            }
        }

        // change the date if we don't get any results at all
        if (numBuckets == 0) {
            queryDate = null;
        }

        // remove lock
        markQueryReceivedNow();
    }

    protected void sortValuesForKey(String key, Object[] sortValues) {}
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.elasticsearch.persistence;

import static org.elasticsearch.index.query.QueryBuilders.boolQuery;

import com.digitalpebble.stormcrawler.persistence.EmptyQueueListener;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import com.github.benmanes.caffeine.cache.Cache;
import com.github.benmanes.caffeine.cache.Caffeine;
import java.time.Instant;
import java.util.Date;
import java.util.List;
import java.util.Map;
import org.apache.storm.spout.SpoutOutputCollector;
import org.apache.storm.task.TopologyContext;
import org.elasticsearch.action.search.SearchRequest;
import org.elasticsearch.action.search.SearchResponse;
import org.elasticsearch.client.RequestOptions;
import org.elasticsearch.index.query.BoolQueryBuilder;
import org.elasticsearch.index.query.QueryBuilders;
import org.elasticsearch.search.SearchHit;
import org.elasticsearch.search.builder.SearchSourceBuilder;
import org.elasticsearch.search.sort.FieldSortBuilder;
import org.elasticsearch.search.sort.SortBuilders;
import org.elasticsearch.search.sort.SortOrder;
import org.joda.time.format.ISODateTimeFormat;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Uses collapsing spouts to get an initial set of URLs and keys to query for and gets emptyQueue
 * notifications from the URLBuffer to query ES for a specific key.
 *
 * @since 1.15
 */
public class HybridSpout extends AggregationSpout implements EmptyQueueListener {

    private static final Logger LOG = LoggerFactory.getLogger(HybridSpout.class);

    protected static final String RELOADPARAMNAME = "es.status.max.urls.per.reload";

    private int bufferReloadSize = 10;

    private Cache<String, Object[]> searchAfterCache;

    @Override
    public void open(Map stormConf, TopologyContext context, SpoutOutputCollector collector) {
        super.open(stormConf, context, collector);
        bufferReloadSize = ConfUtils.getInt(stormConf, RELOADPARAMNAME, maxURLsPerBucket);
        buffer.setEmptyQueueListener(this);
        searchAfterCache = Caffeine.newBuilder().build();
    }

    @Override
    public void emptyQueue(String queueName) {

        LOG.info("{} Emptied buffer queue for {}", logIdprefix, queueName);

        if (!currentBuckets.contains(queueName)) {
            // not interested in this one any more
            return;
        }

        // reloading the aggregs - searching now
        // would just overload ES and yield
        // mainly duplicates
        if (isInQuery.get()) {
            return;
        }

        LOG.info("{} Querying for more docs for {}", logIdprefix, queueName);

        if (queryDate == null) {
            queryDate = new Date();
            lastTimeResetToNOW = Instant.now();
        }

        String formattedQueryDate = ISODateTimeFormat.dateTimeNoMillis().print(queryDate.getTime());

        BoolQueryBuilder queryBuilder =
                boolQuery()
                        .filter(QueryBuilders.rangeQuery("nextFetchDate").lte(formattedQueryDate));

        queryBuilder.filter(QueryBuilders.termQuery(partitionField, queueName));

        SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();
        sourceBuilder.query(queryBuilder);
        sourceBuilder.from(0);
        sourceBuilder.size(bufferReloadSize);
        sourceBuilder.explain(false);
        sourceBuilder.trackTotalHits(false);

        // sort within a bucket
        for (String bsf : bucketSortField) {
            FieldSortBuilder sorter = SortBuilders.fieldSort(bsf).order(SortOrder.ASC);
            sourceBuilder.sort(sorter);
        }

        // do we have a search after for this one?
        Object[] searchAfterValues = searchAfterCache.getIfPresent(queueName);
        if (searchAfterValues != null) {
            sourceBuilder.searchAfter(searchAfterValues);
        }

        SearchRequest request = new SearchRequest(indexName);

        request.source(sourceBuilder);

        // https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-preference.html
        // _shards:2,3
        // specific shard but ideally a local copy of it
        if (shardID != -1) {
            request.preference("_shards:" + shardID + "|_local");
        }

        // dump query to log
        LOG.debug("{} ES query {} - {}", logIdprefix, queueName, request.toString());

        client.searchAsync(request, RequestOptions.DEFAULT, this);
    }

    /** gets the results for a specific host */
    public void onResponse(SearchResponse response) {

        // aggregations? process with the super class
        if (response.getAggregations() != null) {
            // delete all entries from the searchAfterCache when
            // we get the results from the aggregation spouts
            searchAfterCache.invalidateAll();
            super.onResponse(response);
            return;
        }

        int alreadyprocessed = 0;
        int numDocs = 0;

        SearchHit[] hits = response.getHits().getHits();

        Object[] sortValues = null;

        // retrieve the key for these results
        String key = null;

        for (SearchHit hit : hits) {
            numDocs++;
            String pfield = partitionField;
            Map<String, Object> sourceAsMap = hit.getSourceAsMap();
            if (pfield.startsWith("metadata.")) {
                sourceAsMap = (Map<String, Object>) sourceAsMap.get("metadata");
                pfield = pfield.substring(9);
            }
            Object key_as_object = sourceAsMap.get(pfield);
            if (key_as_object instanceof List) {
                if (((List) (key_as_object)).size() == 1)
                    key = (String) ((List) key_as_object).get(0);
            } else {
                key = key_as_object.toString();
            }

            sortValues = hit.getSortValues();
            if (!addHitToBuffer(hit)) {
                alreadyprocessed++;
            }
        }

        // no key if no results have been found
        if (key != null) {
            this.searchAfterCache.put(key, sortValues);
        }

        eventCounter.scope("ES_queries_host").incrBy(1);
        eventCounter.scope("ES_docs_host").incrBy(numDocs);
        eventCounter.scope("already_being_processed_host").incrBy(alreadyprocessed);

        LOG.info(
                "{} ES term query returned {} hits  in {} msec with {} already being processed for {}",
                logIdprefix,
                numDocs,
                response.getTook().getMillis(),
                alreadyprocessed,
                key);
    }

    /** A failure caused by an exception at some phase of the task. */
    public void onFailure(Exception e) {
        LOG.error("Exception with ES query", e);
    }

    @Override
    /** The aggregation kindly told us where to start from * */
    protected void sortValuesForKey(String key, Object[] sortValues) {
        if (sortValues != null && sortValues.length > 0) this.searchAfterCache.put(key, sortValues);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.elasticsearch.bolt;

import static com.digitalpebble.stormcrawler.Constants.StatusStreamName;
import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;

import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.elasticsearch.ElasticSearchConnection;
import com.digitalpebble.stormcrawler.indexing.AbstractIndexerBolt;
import com.digitalpebble.stormcrawler.persistence.Status;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import com.digitalpebble.stormcrawler.util.PerSecondReducer;
import com.github.benmanes.caffeine.cache.Cache;
import com.github.benmanes.caffeine.cache.Caffeine;
import com.github.benmanes.caffeine.cache.RemovalCause;
import com.github.benmanes.caffeine.cache.RemovalListener;
import java.io.IOException;
import java.util.Iterator;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.concurrent.TimeUnit;
import org.apache.commons.lang.StringUtils;
import org.apache.storm.metric.api.MultiCountMetric;
import org.apache.storm.metric.api.MultiReducedMetric;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.tuple.Values;
import org.checkerframework.checker.nullness.qual.NonNull;
import org.checkerframework.checker.nullness.qual.Nullable;
import org.elasticsearch.action.DocWriteRequest;
import org.elasticsearch.action.bulk.BulkItemResponse;
import org.elasticsearch.action.bulk.BulkProcessor;
import org.elasticsearch.action.bulk.BulkRequest;
import org.elasticsearch.action.bulk.BulkResponse;
import org.elasticsearch.action.index.IndexRequest;
import org.elasticsearch.common.xcontent.XContentBuilder;
import org.elasticsearch.rest.RestStatus;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Sends documents to ElasticSearch. Indexes all the fields from the tuples or a Map
 * &lt;String,Object&gt; from a named field.
 */
@SuppressWarnings("serial")
public class IndexerBolt extends AbstractIndexerBolt
        implements RemovalListener<String, List<Tuple>>, BulkProcessor.Listener {

    private static final Logger LOG = LoggerFactory.getLogger(IndexerBolt.class);

    private static final String ESBoltType = "indexer";

    static final String ESIndexNameParamName = "es.indexer.index.name";
    private static final String ESCreateParamName = "es.indexer.create";
    private static final String ESIndexPipelineParamName = "es.indexer.pipeline";

    private OutputCollector _collector;

    private String indexName;

    private String pipeline;

    // whether the document will be created only if it does not exist or
    // overwritten
    private boolean create = false;

    private MultiCountMetric eventCounter;

    private ElasticSearchConnection connection;

    private MultiReducedMetric perSecMetrics;

    private Cache<String, List<Tuple>> waitAck;

    public IndexerBolt() {}

    /** Sets the index name instead of taking it from the configuration. * */
    public IndexerBolt(String indexName) {
        this.indexName = indexName;
    }

    @SuppressWarnings({"unchecked", "rawtypes"})
    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
        super.prepare(conf, context, collector);
        _collector = collector;
        if (indexName == null) {
            indexName = ConfUtils.getString(conf, IndexerBolt.ESIndexNameParamName, "content");
        }

        create = ConfUtils.getBoolean(conf, IndexerBolt.ESCreateParamName, false);
        pipeline = ConfUtils.getString(conf, IndexerBolt.ESIndexPipelineParamName);

        try {
            connection = ElasticSearchConnection.getConnection(conf, ESBoltType, this);
        } catch (Exception e1) {
            LOG.error("Can't connect to ElasticSearch", e1);
            throw new RuntimeException(e1);
        }

        this.eventCounter =
                context.registerMetric("ElasticSearchIndexer", new MultiCountMetric(), 10);

        this.perSecMetrics =
                context.registerMetric(
                        "Indexer_average_persec",
                        new MultiReducedMetric(new PerSecondReducer()),
                        10);

        waitAck =
                Caffeine.newBuilder()
                        .expireAfterWrite(60, TimeUnit.SECONDS)
                        .removalListener(this)
                        .build();

        context.registerMetric("waitAck", () -> waitAck.estimatedSize(), 10);
    }

    public void onRemoval(
            @Nullable String key, @Nullable List<Tuple> value, @NonNull RemovalCause cause) {
        if (!cause.wasEvicted()) return;
        LOG.error("Purged from waitAck {} with {} values", key, value.size());
        for (Tuple t : value) {
            _collector.fail(t);
        }
    }

    @Override
    public void cleanup() {
        if (connection != null) connection.close();
    }

    @Override
    public void execute(Tuple tuple) {

        String url = tuple.getStringByField("url");

        // Distinguish the value used for indexing
        // from the one used for the status
        String normalisedurl = valueForURL(tuple);

        LOG.info("Indexing {} as {}", url, normalisedurl);

        Metadata metadata = (Metadata) tuple.getValueByField("metadata");

        boolean keep = filterDocument(metadata);
        if (!keep) {
            LOG.info("Filtered {}", url);
            eventCounter.scope("Filtered").incrBy(1);
            // treat it as successfully processed even if
            // we do not index it
            _collector.emit(StatusStreamName, tuple, new Values(url, metadata, Status.FETCHED));
            _collector.ack(tuple);
            return;
        }

        String docID = org.apache.commons.codec.digest.DigestUtils.sha256Hex(normalisedurl);

        try {
            XContentBuilder builder = jsonBuilder().startObject();

            // display text of the document?
            if (StringUtils.isNotBlank(fieldNameForText())) {
                String text = tuple.getStringByField("text");
                builder.field(fieldNameForText(), trimText(text));
            }

            // send URL as field?
            if (StringUtils.isNotBlank(fieldNameForURL())) {
                builder.field(fieldNameForURL(), normalisedurl);
            }

            // which metadata to display?
            Map<String, String[]> keyVals = filterMetadata(metadata);

            Iterator<String> iterator = keyVals.keySet().iterator();
            while (iterator.hasNext()) {
                String fieldName = iterator.next();
                String[] values = keyVals.get(fieldName);
                if (values.length == 1) {
                    builder.field(fieldName, values[0]);
                } else if (values.length > 1) {
                    builder.array(fieldName, values);
                }
            }

            builder.endObject();

            IndexRequest indexRequest =
                    new IndexRequest(getIndexName(metadata)).source(builder).id(docID);

            DocWriteRequest.OpType optype = DocWriteRequest.OpType.INDEX;

            if (create) {
                optype = DocWriteRequest.OpType.CREATE;
            }

            indexRequest.opType(optype);

            if (pipeline != null) {
                indexRequest.setPipeline(pipeline);
            }

            connection.getProcessor().add(indexRequest);

            eventCounter.scope("Indexed").incrBy(1);
            perSecMetrics.scope("Indexed").update(1);

            synchronized (waitAck) {
                List<Tuple> tt = waitAck.getIfPresent(docID);
                if (tt == null) {
                    tt = new LinkedList<>();
                    waitAck.put(docID, tt);
                }
                tt.add(tuple);
                LOG.debug("Added to waitAck {} with ID {} total {}", url, docID, tt.size());
            }

        } catch (IOException e) {
            LOG.error("Error building document for ES", e);
            // do not send to status stream so that it gets replayed
            _collector.fail(tuple);
            if (docID != null) {
                synchronized (waitAck) {
                    waitAck.invalidate(docID);
                }
            }
        }
    }

    /**
     * Must be overridden for implementing custom index names based on some metadata information By
     * Default, indexName coming from config is used
     */
    protected String getIndexName(Metadata m) {
        return indexName;
    }

    @Override
    public void beforeBulk(long executionId, BulkRequest request) {
        eventCounter.scope("bulks_sent").incrBy(1);
    }

    @Override
    public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {
        long msec = response.getTook().getMillis();
        eventCounter.scope("bulks_received").incrBy(1);
        eventCounter.scope("bulk_msec").incrBy(msec);
        Iterator<BulkItemResponse> bulkitemiterator = response.iterator();
        int itemcount = 0;
        int acked = 0;
        int failurecount = 0;

        synchronized (waitAck) {
            while (bulkitemiterator.hasNext()) {
                BulkItemResponse bir = bulkitemiterator.next();
                itemcount++;
                String id = bir.getId();
                BulkItemResponse.Failure f = bir.getFailure();
                boolean failed = false;
                if (f != null) {
                    if (f.getStatus().equals(RestStatus.CONFLICT)) {
                        eventCounter.scope("doc_conflicts").incrBy(1);
                    } else {
                        failed = true;
                    }
                }

                List<Tuple> xx = waitAck.getIfPresent(id);
                if (xx == null) {
                    LOG.warn("Could not find unacked tuples for {}", id);
                    continue;
                }

                LOG.debug("Found {} tuple(s) for ID {}", xx.size(), id);

                for (Tuple t : xx) {
                    String u = (String) t.getValueByField("url");

                    Metadata metadata = (Metadata) t.getValueByField("metadata");

                    if (!failed) {
                        acked++;
                        _collector.emit(
                                StatusStreamName, t, new Values(u, metadata, Status.FETCHED));
                        _collector.ack(t);
                    } else {
                        failurecount++;
                        LOG.error("update ID {}, URL {}, failure: {}", id, u, f);
                        // there is something wrong with the content we should
                        // treat
                        // it as an ERROR
                        if (f.getStatus().equals(RestStatus.BAD_REQUEST)) {
                            metadata.setValue(Constants.STATUS_ERROR_SOURCE, "ES indexing");
                            metadata.setValue(Constants.STATUS_ERROR_MESSAGE, "invalid content");
                            _collector.emit(
                                    StatusStreamName, t, new Values(u, metadata, Status.ERROR));
                            _collector.ack(t);
                            LOG.debug("Acked {} with ID {}", u, id);
                        } else {
                            failurecount++;
                            LOG.error("update ID {}, URL {}, failure: {}", id, u, f);
                            // there is something wrong with the content we
                            // should
                            // treat
                            // it as an ERROR
                            if (f.getStatus().equals(RestStatus.BAD_REQUEST)) {
                                metadata.setValue(Constants.STATUS_ERROR_SOURCE, "ES indexing");
                                metadata.setValue(
                                        Constants.STATUS_ERROR_MESSAGE, "invalid content");
                                _collector.emit(
                                        StatusStreamName, t, new Values(u, metadata, Status.ERROR));
                                _collector.ack(t);
                            }
                            // otherwise just fail it
                            else {
                                _collector.fail(t);
                            }
                        }
                    }
                }
                waitAck.invalidate(id);
            }

            LOG.info(
                    "Bulk response [{}] : items {}, waitAck {}, acked {}, failed {}",
                    executionId,
                    itemcount,
                    waitAck.estimatedSize(),
                    acked,
                    failurecount);

            if (waitAck.estimatedSize() > 0 && LOG.isDebugEnabled()) {
                for (String kinaw : waitAck.asMap().keySet()) {
                    LOG.debug(
                            "Still in wait ack after bulk response [{}] => {}", executionId, kinaw);
                }
            }
        }
    }

    @Override
    public void afterBulk(long executionId, BulkRequest request, Throwable failure) {
        eventCounter.scope("bulks_received").incrBy(1);
        LOG.error("Exception with bulk {} - failing the whole lot ", executionId, failure);
        synchronized (waitAck) {
            // WHOLE BULK FAILED
            // mark all the docs as fail
            Iterator<DocWriteRequest<?>> itreq = request.requests().iterator();
            while (itreq.hasNext()) {
                String id = itreq.next().id();

                List<Tuple> xx = waitAck.getIfPresent(id);
                if (xx != null) {
                    LOG.debug("Failed {} tuple(s) for ID {}", xx.size(), id);
                    for (Tuple x : xx) {
                        // fail it
                        _collector.fail(x);
                    }
                    waitAck.invalidate(id);
                } else {
                    LOG.warn("Could not find unacked tuple for {}", id);
                }
            }
        }
    }
}

Parse Compilation Unit:
package com.digitalpebble.stormcrawler.elasticsearch.bolt;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.elasticsearch.ElasticSearchConnection;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import java.io.IOException;
import java.lang.invoke.MethodHandles;
import java.util.Map;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.topology.base.BaseRichBolt;
import org.apache.storm.tuple.Tuple;
import org.elasticsearch.action.delete.DeleteRequest;
import org.elasticsearch.client.RequestOptions;
import org.elasticsearch.client.RestHighLevelClient;
import org.slf4j.LoggerFactory;

/**
 * Deletes documents to ElasticSearch. This should be connected to the StatusUpdaterBolt via the
 * 'deletion' stream and will remove the documents with a status of ERROR one by one. Note that this
 * component will also try to delete documents even though they were never indexed and it currently
 * won't delete documents which were indexed under the canonical URL.
 */
public class DeletionBolt extends BaseRichBolt {

    static final org.slf4j.Logger LOG =
            LoggerFactory.getLogger(MethodHandles.lookup().lookupClass());

    private static final String ESBoltType = "indexer";

    private OutputCollector _collector;

    private String indexName;

    private RestHighLevelClient client;

    public DeletionBolt() {}

    /** Sets the index name instead of taking it from the configuration. * */
    public DeletionBolt(String indexName) {
        this.indexName = indexName;
    }

    @SuppressWarnings({"unchecked", "rawtypes"})
    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
        _collector = collector;
        if (indexName == null) {
            indexName = ConfUtils.getString(conf, IndexerBolt.ESIndexNameParamName, "content");
        }
        client = ElasticSearchConnection.getClient(conf, ESBoltType);
    }

    @Override
    public void cleanup() {
        if (client != null)
            try {
                client.close();
            } catch (IOException e) {
            }
    }

    @Override
    public void execute(Tuple tuple) {
        String url = tuple.getStringByField("url");
        Metadata metadata = (Metadata) tuple.getValueByField("metadata");

        // keep it simple for now and ignore cases where the canonical URL was
        // used
        String sha256hex = org.apache.commons.codec.digest.DigestUtils.sha256Hex(url);
        DeleteRequest dr = new DeleteRequest(getIndexName(metadata), sha256hex);
        try {
            client.delete(dr, RequestOptions.DEFAULT);
        } catch (IOException e) {
            _collector.fail(tuple);
            LOG.error("Exception caught while deleting", e);
            return;
        }
        _collector.ack(tuple);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer arg0) {
        // none
    }

    /**
     * Must be overridden for implementing custom index names based on some metadata information By
     * Default, indexName coming from config is used
     */
    protected String getIndexName(Metadata m) {
        return indexName;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.elasticsearch;

import static org.elasticsearch.client.RestClientBuilder.DEFAULT_CONNECT_TIMEOUT_MILLIS;
import static org.elasticsearch.client.RestClientBuilder.DEFAULT_SOCKET_TIMEOUT_MILLIS;

import com.digitalpebble.stormcrawler.util.ConfUtils;
import java.io.IOException;
import java.net.URI;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.concurrent.TimeUnit;
import org.apache.http.HttpHost;
import org.apache.http.auth.AuthScope;
import org.apache.http.auth.UsernamePasswordCredentials;
import org.apache.http.client.CredentialsProvider;
import org.apache.http.impl.client.BasicCredentialsProvider;
import org.apache.http.impl.nio.client.HttpAsyncClientBuilder;
import org.apache.storm.shade.org.apache.commons.lang.StringUtils;
import org.elasticsearch.action.bulk.BulkProcessor;
import org.elasticsearch.action.bulk.BulkRequest;
import org.elasticsearch.action.bulk.BulkResponse;
import org.elasticsearch.client.Node;
import org.elasticsearch.client.NodeSelector;
import org.elasticsearch.client.RequestOptions;
import org.elasticsearch.client.RestClient;
import org.elasticsearch.client.RestClientBuilder;
import org.elasticsearch.client.RestHighLevelClient;
import org.elasticsearch.client.sniff.Sniffer;
import org.elasticsearch.common.unit.TimeValue;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/** Utility class to instantiate an ES client and bulkprocessor based on the configuration. */
public class ElasticSearchConnection {

    private static final Logger LOG = LoggerFactory.getLogger(ElasticSearchConnection.class);

    private RestHighLevelClient client;

    private BulkProcessor processor;

    private Sniffer sniffer;

    private ElasticSearchConnection(RestHighLevelClient c, BulkProcessor p) {
        this(c, p, null);
    }

    private ElasticSearchConnection(RestHighLevelClient c, BulkProcessor p, Sniffer s) {
        processor = p;
        client = c;
        sniffer = s;
    }

    public RestHighLevelClient getClient() {
        return client;
    }

    public BulkProcessor getProcessor() {
        return processor;
    }

    public static RestHighLevelClient getClient(Map stormConf, String boltType) {

        List<String> confighosts =
                ConfUtils.loadListFromConf("es." + boltType + ".addresses", stormConf);

        List<HttpHost> hosts = new ArrayList<>();

        for (String host : confighosts) {
            // no port specified? use default one
            int port = 9200;
            String scheme = "http";
            // no scheme specified? use http
            if (!host.startsWith(scheme)) {
                host = "http://" + host;
            }
            URI uri = URI.create(host);
            if (uri.getHost() == null) {
                throw new RuntimeException("host undefined " + host);
            }
            if (uri.getPort() != -1) {
                port = uri.getPort();
            }
            if (uri.getScheme() != null) {
                scheme = uri.getScheme();
            }
            hosts.add(new HttpHost(uri.getHost(), port, scheme));
        }

        RestClientBuilder builder = RestClient.builder(hosts.toArray(new HttpHost[hosts.size()]));

        // authentication via user / password
        String user = ConfUtils.getString(stormConf, "es." + boltType + ".user");
        String password = ConfUtils.getString(stormConf, "es." + boltType + ".password");

        String proxyhost = ConfUtils.getString(stormConf, "es." + boltType + ".proxy.host");

        int proxyport = ConfUtils.getInt(stormConf, "es." + boltType + ".proxy.port", -1);

        String proxyscheme =
                ConfUtils.getString(stormConf, "es." + boltType + ".proxy.scheme", "http");

        boolean needsUser = StringUtils.isNotBlank(user) && StringUtils.isNotBlank(password);
        boolean needsProxy = StringUtils.isNotBlank(proxyhost) && proxyport != -1;

        if (needsUser || needsProxy) {
            builder.setHttpClientConfigCallback(
                    new RestClientBuilder.HttpClientConfigCallback() {
                        @Override
                        public HttpAsyncClientBuilder customizeHttpClient(
                                HttpAsyncClientBuilder httpClientBuilder) {
                            if (needsUser) {
                                final CredentialsProvider credentialsProvider =
                                        new BasicCredentialsProvider();
                                credentialsProvider.setCredentials(
                                        AuthScope.ANY,
                                        new UsernamePasswordCredentials(user, password));
                                httpClientBuilder.setDefaultCredentialsProvider(
                                        credentialsProvider);
                            }
                            if (needsProxy) {
                                httpClientBuilder.setProxy(
                                        new HttpHost(proxyhost, proxyport, proxyscheme));
                            }
                            return httpClientBuilder;
                        }
                    });
        }

        int connectTimeout =
                ConfUtils.getInt(
                        stormConf,
                        "es." + boltType + ".connect.timeout",
                        DEFAULT_CONNECT_TIMEOUT_MILLIS);
        int socketTimeout =
                ConfUtils.getInt(
                        stormConf,
                        "es." + boltType + ".socket.timeout",
                        DEFAULT_SOCKET_TIMEOUT_MILLIS);
        // timeout until connection is established
        builder.setRequestConfigCallback(
                requestConfigBuilder ->
                        requestConfigBuilder
                                .setConnectTimeout(connectTimeout)
                                .setSocketTimeout(socketTimeout) // Timeout when waiting
                // for data
                );

        // TODO check if this has gone somewhere else in ES 7
        // int maxRetryTimeout = ConfUtils.getInt(stormConf, "es." + boltType +
        // ".max.retry.timeout",
        // DEFAULT_MAX_RETRY_TIMEOUT_MILLIS);
        // builder.setMaxRetryTimeoutMillis(maxRetryTimeout);

        // TODO configure headers etc...
        // Map<String, String> configSettings = (Map) stormConf
        // .get("es." + boltType + ".settings");
        // if (configSettings != null) {
        // configSettings.forEach((k, v) -> settings.put(k, v));
        // }

        // use node selector only to log nodes listed in the config
        // and/or discovered through sniffing
        builder.setNodeSelector(
                new NodeSelector() {
                    @Override
                    public void select(Iterable<Node> nodes) {
                        for (Node node : nodes) {
                            LOG.debug(
                                    "Connected to ES node {} [{}] for {}",
                                    node.getName(),
                                    node.getHost(),
                                    boltType);
                        }
                    }
                });

        return new RestHighLevelClient(builder);
    }

    /**
     * Creates a connection with a default listener. The values for bolt type are
     * [indexer,status,metrics]
     */
    public static ElasticSearchConnection getConnection(Map stormConf, String boltType) {
        BulkProcessor.Listener listener =
                new BulkProcessor.Listener() {
                    @Override
                    public void afterBulk(long arg0, BulkRequest arg1, BulkResponse arg2) {}

                    @Override
                    public void afterBulk(long arg0, BulkRequest arg1, Throwable arg2) {}

                    @Override
                    public void beforeBulk(long arg0, BulkRequest arg1) {}
                };
        return getConnection(stormConf, boltType, listener);
    }

    public static ElasticSearchConnection getConnection(
            Map stormConf, String boltType, BulkProcessor.Listener listener) {

        String flushIntervalString =
                ConfUtils.getString(stormConf, "es." + boltType + ".flushInterval", "5s");

        TimeValue flushInterval =
                TimeValue.parseTimeValue(
                        flushIntervalString, TimeValue.timeValueSeconds(5), "flushInterval");

        int bulkActions = ConfUtils.getInt(stormConf, "es." + boltType + ".bulkActions", 50);

        int concurrentRequests =
                ConfUtils.getInt(stormConf, "es." + boltType + ".concurrentRequests", 1);

        RestHighLevelClient client = getClient(stormConf, boltType);

        boolean sniff = ConfUtils.getBoolean(stormConf, "es." + boltType + ".sniff", true);
        Sniffer sniffer = null;
        if (sniff) {
            sniffer = Sniffer.builder(client.getLowLevelClient()).build();
        }

        BulkProcessor bulkProcessor =
                BulkProcessor.builder(
                                (request, bulkListener) ->
                                        client.bulkAsync(
                                                request, RequestOptions.DEFAULT, bulkListener),
                                listener)
                        .setFlushInterval(flushInterval)
                        .setBulkActions(bulkActions)
                        .setConcurrentRequests(concurrentRequests)
                        .build();

        return new ElasticSearchConnection(client, bulkProcessor, sniffer);
    }

    public void close() {
        // First, close the BulkProcessor ensuring pending actions are flushed
        if (processor != null) {
            try {
                boolean success = processor.awaitClose(60, TimeUnit.SECONDS);
                if (!success) {
                    throw new RuntimeException(
                            "Failed to flush pending actions when closing BulkProcessor");
                }
            } catch (InterruptedException e) {
                throw new RuntimeException(e);
            }
        }

        if (sniffer != null) {
            sniffer.close();
        }

        // Now close the actual client
        if (client != null) {
            try {
                client.close();
            } catch (IOException e) {
                // ignore silently
            }
        }
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.elasticsearch.filtering;

import com.digitalpebble.stormcrawler.JSONResource;
import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.elasticsearch.ElasticSearchConnection;
import com.digitalpebble.stormcrawler.filtering.URLFilter;
import com.fasterxml.jackson.databind.JsonNode;
import java.io.ByteArrayInputStream;
import java.net.URL;
import java.util.Map;
import java.util.Timer;
import java.util.TimerTask;
import org.elasticsearch.action.get.GetRequest;
import org.elasticsearch.action.get.GetResponse;
import org.elasticsearch.client.RequestOptions;
import org.elasticsearch.client.RestHighLevelClient;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Wraps a URLFilter whose resources are in a JSON file that can be stored in ES. The benefit of
 * doing this is that the resources can be refreshed automatically and modified without having to
 * recompile the jar and restart the topology. The connection to ES is done via the config and uses
 * a new bolt type 'config'.
 *
 * <p>The configuration of the delegate is done in the urlfilters.json as usual.
 *
 * <pre>
 *  {
 *     "class": "com.digitalpebble.stormcrawler.elasticsearch.filtering.JSONURLFilterWrapper",
 *     "name": "ESFastURLFilter",
 *     "params": {
 *         "refresh": "60",
 *         "delegate": {
 *             "class": "com.digitalpebble.stormcrawler.filtering.regex.FastURLFilter",
 *             "params": {
 *                 "file": "fast.urlfilter.json"
 *             }
 *         }
 *     }
 *  }
 * </pre>
 *
 * The resource file can be pushed to ES with
 *
 * <pre>
 *  curl -XPUT 'localhost:9200/config/config/fast.urlfilter.json?pretty' -H 'Content-Type: application/json' -d @fast.urlfilter.json
 * </pre>
 */
public class JSONURLFilterWrapper implements URLFilter {

    private static final Logger LOG = LoggerFactory.getLogger(JSONURLFilterWrapper.class);

    private URLFilter delegatedURLFilter;

    public void configure(@SuppressWarnings("rawtypes") Map stormConf, JsonNode filterParams) {

        String urlfilterclass = null;

        JsonNode delegateNode = filterParams.get("delegate");
        if (delegateNode == null) {
            throw new RuntimeException("delegateNode undefined!");
        }

        JsonNode node = delegateNode.get("class");
        if (node != null && node.isTextual()) {
            urlfilterclass = node.asText();
        }

        if (urlfilterclass == null) {
            throw new RuntimeException("urlfilter.class undefined!");
        }

        // load an instance of the delegated parsefilter
        try {
            Class<?> filterClass = Class.forName(urlfilterclass);

            boolean subClassOK = URLFilter.class.isAssignableFrom(filterClass);
            if (!subClassOK) {
                throw new RuntimeException(
                        "Filter " + urlfilterclass + " does not extend URLFilter");
            }

            delegatedURLFilter = (URLFilter) filterClass.newInstance();

            // check that it implements JSONResource
            if (!JSONResource.class.isInstance(delegatedURLFilter)) {
                throw new RuntimeException(
                        "Filter " + urlfilterclass + " does not implement JSONResource");
            }

        } catch (Exception e) {
            LOG.error("Can't setup {}: {}", urlfilterclass, e);
            throw new RuntimeException("Can't setup " + urlfilterclass, e);
        }

        // configure it
        node = delegateNode.get("params");

        delegatedURLFilter.configure(stormConf, node);

        int refreshRate = 600;

        node = filterParams.get("refresh");
        if (node != null && node.isInt()) {
            refreshRate = node.asInt(refreshRate);
        }

        final JSONResource resource = (JSONResource) delegatedURLFilter;

        new Timer()
                .schedule(
                        new TimerTask() {
                            private RestHighLevelClient esClient;

                            public void run() {
                                if (esClient == null) {
                                    try {
                                        esClient =
                                                ElasticSearchConnection.getClient(
                                                        stormConf, "config");
                                    } catch (Exception e) {
                                        LOG.error("Exception while creating ES connection", e);
                                    }
                                }
                                if (esClient != null) {
                                    LOG.info("Reloading json resources from ES");
                                    try {
                                        GetResponse response =
                                                esClient.get(
                                                        new GetRequest(
                                                                "config",
                                                                "config",
                                                                resource.getResourceFile()),
                                                        RequestOptions.DEFAULT);
                                        resource.loadJSONResources(
                                                new ByteArrayInputStream(
                                                        response.getSourceAsBytes()));
                                    } catch (Exception e) {
                                        LOG.error("Can't load config from ES", e);
                                    }
                                }
                            }
                        },
                        0,
                        refreshRate * 1000);
    }

    @Override
    public String filter(URL sourceUrl, Metadata sourceMetadata, String urlToFilter) {
        return delegatedURLFilter.filter(sourceUrl, sourceMetadata, urlToFilter);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.elasticsearch.metrics;

import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;

import com.digitalpebble.stormcrawler.elasticsearch.ElasticSearchConnection;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import java.text.SimpleDateFormat;
import java.util.Collection;
import java.util.Date;
import java.util.Iterator;
import java.util.Map;
import java.util.Map.Entry;
import org.apache.storm.metric.api.IMetricsConsumer;
import org.apache.storm.task.IErrorReporter;
import org.apache.storm.task.TopologyContext;
import org.elasticsearch.action.index.IndexRequest;
import org.elasticsearch.common.xcontent.XContentBuilder;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Sends metrics to an Elasticsearch index. The ES details are set in the configuration; an optional
 * argument sets a date format to append to the index name.
 *
 * <pre>
 *   topology.metrics.consumer.register:
 *        - class: "com.digitalpebble.stormcrawler.elasticsearch.metrics.MetricsConsumer"
 *          parallelism.hint: 1
 *          argument: "yyyy-MM-dd"
 * </pre>
 */
public class MetricsConsumer implements IMetricsConsumer {

    private final Logger LOG = LoggerFactory.getLogger(getClass());

    private static final String ESBoltType = "metrics";

    /** name of the index to use for the metrics (default : metrics) * */
    private static final String ESMetricsIndexNameParamName = "es." + ESBoltType + ".index.name";

    private String indexName;

    private ElasticSearchConnection connection;

    private String stormID;

    /** optional date format passed as argument, must be parsable as a SimpleDateFormat */
    private SimpleDateFormat dateFormat;

    @Override
    public void prepare(
            Map stormConf,
            Object registrationArgument,
            TopologyContext context,
            IErrorReporter errorReporter) {
        indexName = ConfUtils.getString(stormConf, ESMetricsIndexNameParamName, "metrics");
        stormID = context.getStormId();
        if (registrationArgument != null) {
            dateFormat = new SimpleDateFormat((String) registrationArgument);
            LOG.info("Using date format {}", registrationArgument);
        }
        try {
            connection = ElasticSearchConnection.getConnection(stormConf, ESBoltType);
        } catch (Exception e1) {
            LOG.error("Can't connect to ElasticSearch", e1);
            throw new RuntimeException(e1);
        }
    }

    @Override
    public void cleanup() {
        if (connection != null) connection.close();
    }

    @Override
    public void handleDataPoints(TaskInfo taskInfo, Collection<DataPoint> dataPoints) {
        final Date now = new Date();
        for (DataPoint dataPoint : dataPoints) {
            handleDataPoints(taskInfo, dataPoint.name, dataPoint.value, now);
        }
    }

    private void handleDataPoints(
            final TaskInfo taskInfo, final String nameprefix, final Object value, final Date now) {
        if (value instanceof Number) {
            indexDataPoint(taskInfo, now, nameprefix, ((Number) value).doubleValue());
        } else if (value instanceof Map) {
            Iterator<Entry> keyValiter = ((Map) value).entrySet().iterator();
            while (keyValiter.hasNext()) {
                Entry entry = keyValiter.next();
                String newnameprefix = nameprefix + "." + entry.getKey();
                handleDataPoints(taskInfo, newnameprefix, entry.getValue(), now);
            }
        } else if (value instanceof Collection) {
            for (Object collectionObj : (Collection) value) {
                handleDataPoints(taskInfo, nameprefix, collectionObj, now);
            }
        } else {
            LOG.warn("Found data point value {} of {}", nameprefix, value.getClass().toString());
        }
    }

    /**
     * Returns the name of the index that metrics will be written to.
     *
     * @return elastic index name
     */
    private String getIndexName(Date timestamp) {
        if (dateFormat == null) return indexName;

        StringBuilder sb = new StringBuilder(indexName);
        sb.append("-").append(dateFormat.format(timestamp));
        return sb.toString();
    }

    private void indexDataPoint(TaskInfo taskInfo, Date timestamp, String name, double value) {
        try {
            XContentBuilder builder = jsonBuilder().startObject();
            builder.field("stormId", stormID);
            builder.field("srcComponentId", taskInfo.srcComponentId);
            builder.field("srcTaskId", taskInfo.srcTaskId);
            builder.field("srcWorkerHost", taskInfo.srcWorkerHost);
            builder.field("srcWorkerPort", taskInfo.srcWorkerPort);
            builder.field("name", name);
            builder.field("value", value);
            builder.field("timestamp", timestamp);
            builder.endObject();

            IndexRequest indexRequest = new IndexRequest(getIndexName(timestamp)).source(builder);
            connection.getProcessor().add(indexRequest);
        } catch (Exception e) {
            LOG.error("problem when building request for ES", e);
        }
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.elasticsearch.metrics;

import com.digitalpebble.stormcrawler.elasticsearch.ElasticSearchConnection;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import java.util.HashMap;
import java.util.Map;
import org.apache.storm.Config;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.topology.base.BaseRichBolt;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.utils.TupleUtils;
import org.elasticsearch.action.ActionListener;
import org.elasticsearch.client.RequestOptions;
import org.elasticsearch.client.core.CountRequest;
import org.elasticsearch.client.core.CountResponse;
import org.elasticsearch.index.query.QueryBuilders;
import org.elasticsearch.search.builder.SearchSourceBuilder;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Queries the status index periodically to get the count of URLs per status. This bolt can be
 * connected to the output of any other bolt and will not produce anything as output.
 */
public class StatusMetricsBolt extends BaseRichBolt {

    private static final Logger LOG = LoggerFactory.getLogger(StatusMetricsBolt.class);

    private static final String ESBoltType = "status";
    private static final String ESStatusIndexNameParamName = "es.status.index.name";

    private String indexName;

    private ElasticSearchConnection connection;

    private Map<String, Long> latestStatusCounts = new HashMap<>(6);

    private int freqStats = 60;

    private OutputCollector _collector;

    private transient StatusActionListener[] listeners;

    private class StatusActionListener implements ActionListener<CountResponse> {

        private final String name;

        private boolean ready = true;

        public boolean isReady() {
            return ready;
        }

        public void busy() {
            this.ready = false;
        }

        StatusActionListener(String statusName) {
            name = statusName;
        }

        @Override
        public void onResponse(CountResponse response) {
            ready = true;
            LOG.debug("Got {} counts for status:{}", response.getCount(), name);
            latestStatusCounts.put(name, response.getCount());
        }

        @Override
        public void onFailure(Exception e) {
            ready = true;
            LOG.error("Failure when getting counts for status:{}", name, e);
        }
    }

    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
        _collector = collector;
        indexName = ConfUtils.getString(stormConf, ESStatusIndexNameParamName, "status");
        try {
            connection = ElasticSearchConnection.getConnection(stormConf, ESBoltType);
        } catch (Exception e1) {
            LOG.error("Can't connect to ElasticSearch", e1);
            throw new RuntimeException(e1);
        }

        context.registerMetric(
                "status.count",
                () -> {
                    return latestStatusCounts;
                },
                freqStats);

        listeners = new StatusActionListener[6];

        listeners[0] = new StatusActionListener("DISCOVERED");
        listeners[1] = new StatusActionListener("FETCHED");
        listeners[2] = new StatusActionListener("FETCH_ERROR");
        listeners[3] = new StatusActionListener("REDIRECTION");
        listeners[4] = new StatusActionListener("ERROR");
        listeners[5] = new StatusActionListener("TOTAL");
    }

    @Override
    public Map<String, Object> getComponentConfiguration() {
        Config conf = new Config();
        conf.put(Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS, freqStats);
        return conf;
    }

    @Override
    public void execute(Tuple input) {
        _collector.ack(input);

        // this bolt can be connected to anything
        // we just want to trigger a new search when the input is a tick tuple
        if (!TupleUtils.isTick(input)) {
            return;
        }

        for (StatusActionListener listener : listeners) {
            // still waiting for results from previous request
            if (!listener.isReady()) {
                LOG.debug("Not ready to get counts for status {}", listener.name);
                continue;
            }
            CountRequest request = new CountRequest(indexName);
            if (!listener.name.equalsIgnoreCase("TOTAL")) {
                SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();
                sourceBuilder.query(QueryBuilders.termQuery("status", listener.name));
                request.source(sourceBuilder);
            }
            listener.busy();
            connection.getClient().countAsync(request, RequestOptions.DEFAULT, listener);
        }
    }

    @Override
    public void cleanup() {
        connection.close();
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        // NONE - THIS BOLT DOES NOT GET CONNECTED TO ANY OTHERS
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.elasticsearch.bolt;

import static org.junit.Assert.assertEquals;
import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.TestOutputCollector;
import com.digitalpebble.stormcrawler.TestUtil;
import com.digitalpebble.stormcrawler.indexing.AbstractIndexerBolt;
import java.util.HashMap;
import java.util.Map;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.tuple.Tuple;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.testcontainers.elasticsearch.ElasticsearchContainer;

public class IndexerBoltTest {

    private ElasticsearchContainer container;
    private IndexerBolt bolt;
    protected TestOutputCollector output;

    private static final Logger LOG = LoggerFactory.getLogger(IndexerBoltTest.class);

    @Before
    public void setupIndexerBolt() {

        String version = System.getProperty("elasticsearch-version");
        if (version == null) version = "7.5.0";
        LOG.info("Starting docker instance of Elasticsearch {}...", version);

        container =
                new ElasticsearchContainer(
                        "docker.elastic.co/elasticsearch/elasticsearch:" + version);
        container.start();

        bolt = new IndexerBolt("content");

        // give the indexer the port for connecting to ES

        Map conf = new HashMap();
        conf.put(AbstractIndexerBolt.urlFieldParamName, "url");
        conf.put(AbstractIndexerBolt.canonicalMetadataParamName, "canonical");
        conf.put("es.indexer.addresses", container.getHttpHostAddress());

        output = new TestOutputCollector();

        bolt.prepare(conf, TestUtil.getMockedTopologyContext(), new OutputCollector(output));
    }

    @After
    public void close() {
        LOG.info("Closing indexer bolt and ES container");
        bolt.cleanup();
        container.close();
        output = null;
    }

    private void index(String url, String text, Metadata metadata) {
        Tuple tuple = mock(Tuple.class);
        when(tuple.getStringByField("text")).thenReturn(text);
        when(tuple.getStringByField("url")).thenReturn(url);
        when(tuple.getValueByField("metadata")).thenReturn(metadata);
        bolt.execute(tuple);
    }

    @Test
    // https://github.com/DigitalPebble/storm-crawler/issues/832
    public void simultaneousCanonicals() {

        Metadata m1 = new Metadata();
        String url =
                "https://www.obozrevatel.com/ukr/dnipro/city/u-dnipri-ta-oblasti-ogolosili-shtormove-poperedzhennya.htm";
        m1.addValue("canonical", url);

        Metadata m2 = new Metadata();
        String url2 =
                "https://www.obozrevatel.com/ukr/dnipro/city/u-dnipri-ta-oblasti-ogolosili-shtormove-poperedzhennya/amp.htm";
        m2.addValue("canonical", url);

        index(url, "", m1);
        index(url2, "", m2);

        // check that something has been emitted out
        while (output.getEmitted(Constants.StatusStreamName).size() == 0) {
            try {
                Thread.sleep(100);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        }

        // should be two in status output
        assertEquals(2, output.getEmitted(Constants.StatusStreamName).size());

        // and 2 acked
        assertEquals(2, output.getAckedTuples().size());

        // TODO check output in ES?

    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.elasticsearch.bolt;

import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertTrue;
import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.TestOutputCollector;
import com.digitalpebble.stormcrawler.TestUtil;
import com.digitalpebble.stormcrawler.elasticsearch.persistence.StatusUpdaterBolt;
import com.digitalpebble.stormcrawler.persistence.Status;
import java.io.File;
import java.io.IOException;
import java.nio.charset.Charset;
import java.util.HashMap;
import java.util.Map;
import org.apache.http.HttpHost;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.tuple.Tuple;
import org.elasticsearch.action.get.GetRequest;
import org.elasticsearch.action.get.GetResponse;
import org.elasticsearch.client.RequestOptions;
import org.elasticsearch.client.RestClient;
import org.elasticsearch.client.RestClientBuilder;
import org.elasticsearch.client.RestHighLevelClient;
import org.elasticsearch.client.indices.CreateIndexRequest;
import org.elasticsearch.common.xcontent.XContentType;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.testcontainers.elasticsearch.ElasticsearchContainer;

public class StatusBoltTest {

    private ElasticsearchContainer container;
    private StatusUpdaterBolt bolt;
    protected TestOutputCollector output;

    protected RestHighLevelClient client;

    private static final Logger LOG = LoggerFactory.getLogger(StatusBoltTest.class);

    @Before
    public void setupStatusBolt() throws IOException {

        String version = System.getProperty("elasticsearch-version");
        if (version == null) version = "7.5.0";
        LOG.info("Starting docker instance of Elasticsearch {}...", version);

        container =
                new ElasticsearchContainer(
                        "docker.elastic.co/elasticsearch/elasticsearch:" + version);
        container.start();

        bolt = new StatusUpdaterBolt();

        // configure the status index

        RestClientBuilder builder =
                RestClient.builder(
                        new HttpHost(container.getHost(), container.getMappedPort(9200)));

        client = new RestHighLevelClient(builder);

        // TODO
        // https://www.elastic.co/guide/en/elasticsearch/client/java-rest/7.13/java-rest-high-put-mapping.html

        CreateIndexRequest request = new CreateIndexRequest("status");

        File file = new File(getClass().getClassLoader().getResource("status.mapping").getFile());

        String mappingSource =
                new String(
                        java.nio.file.Files.readAllBytes(file.toPath()), Charset.defaultCharset());

        request.source(mappingSource, XContentType.JSON);

        client.indices().create(request, RequestOptions.DEFAULT);

        // configure the status updater bolt

        Map conf = new HashMap();
        conf.put("es.status.routing.fieldname", "metadata.key");

        conf.put("es.status.addresses", container.getHttpHostAddress());

        conf.put("scheduler.class", "com.digitalpebble.stormcrawler.persistence.DefaultScheduler");

        conf.put("status.updater.cache.spec", "maximumSize=10000,expireAfterAccess=1h");

        conf.put("metadata.persist", "someKey");

        output = new TestOutputCollector();

        bolt.prepare(conf, TestUtil.getMockedTopologyContext(), new OutputCollector(output));
    }

    @After
    public void close() {
        LOG.info("Closing updater bolt and ES container");
        bolt.cleanup();
        container.close();
        output = null;
        try {
            client.close();
        } catch (IOException e) {
        }
    }

    private void store(String url, Status status, Metadata metadata) {
        Tuple tuple = mock(Tuple.class);
        when(tuple.getValueByField("status")).thenReturn(status);
        when(tuple.getStringByField("url")).thenReturn(url);
        when(tuple.getValueByField("metadata")).thenReturn(metadata);
        bolt.execute(tuple);
    }

    @Test
    // see https://github.com/DigitalPebble/storm-crawler/issues/885
    public void checkListKeyFromES() throws IOException {

        String url = "https://www.url.net/something";

        Metadata md = new Metadata();

        md.addValue("someKey", "someValue");

        store(url, Status.DISCOVERED, md);

        // check that something has been emitted out
        while (output.getAckedTuples().size() == 0) {
            try {
                Thread.sleep(100);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        }

        assertEquals(1, output.getAckedTuples().size());

        // check output in ES?

        String id = org.apache.commons.codec.digest.DigestUtils.sha256Hex(url);

        GetResponse result = client.get(new GetRequest("status", id), RequestOptions.DEFAULT);

        Map sourceAsMap = result.getSourceAsMap();

        String pfield = "metadata.someKey";

        if (pfield.startsWith("metadata.")) {
            sourceAsMap = (Map<String, Object>) sourceAsMap.get("metadata");
            pfield = pfield.substring(9);
        }
        Object key = sourceAsMap.get(pfield);

        assertTrue(key instanceof java.util.ArrayList);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.tika;

import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.TestUtil;
import com.digitalpebble.stormcrawler.parse.ParsingTester;
import com.digitalpebble.stormcrawler.persistence.Status;
import com.digitalpebble.stormcrawler.protocol.HttpHeaders;
import com.digitalpebble.stormcrawler.protocol.ProtocolResponse;
import java.io.IOException;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.tuple.Tuple;
import org.junit.Assert;
import org.junit.Before;
import org.junit.Test;

public class ParserBoltTest extends ParsingTester {

    @Before
    public void setupParserBolt() {
        bolt = new ParserBolt();
        setupParserBolt(bolt);
    }

    @Test
    /**
     * Checks that recursive docs are handled correctly
     *
     * @see https://issues.apache.org/jira/browse/TIKA-2096
     */
    public void testRecursiveDoc() throws IOException {

        Map conf = new HashMap();

        conf.put("parser.extract.embedded", true);

        bolt.prepare(conf, TestUtil.getMockedTopologyContext(), new OutputCollector(output));

        parse(
                "http://www.digitalpebble.com/test_recursive_embedded.docx",
                "test_recursive_embedded.docx");

        List<List<Object>> outTuples = output.getEmitted();

        // TODO could we get as many subdocs as embedded in the original one?
        // or just one for now? but should at least contain the text of the
        // subdocs

        Assert.assertEquals(1, outTuples.size());
        Assert.assertTrue(
                outTuples
                        .get(0)
                        .get(3)
                        .toString()
                        .contains("Life, Liberty and the pursuit of Happiness"));
    }

    @Test
    /**
     * Checks that the mimetype whitelists are handled correctly
     *
     * @see https://github.com/DigitalPebble/storm-crawler/issues/712
     */
    public void testMimeTypeWhileList() throws IOException {

        Map conf = new HashMap();

        conf.put("parser.mimetype.whitelist", "application/.+word.*");
        conf.put(ProtocolResponse.PROTOCOL_MD_PREFIX_PARAM, "http.");

        bolt.prepare(conf, TestUtil.getMockedTopologyContext(), new OutputCollector(output));

        String url = "http://thisisatest.com/adoc.pdf";
        Metadata metadata = new Metadata();
        metadata.addValue("http." + HttpHeaders.CONTENT_TYPE, "application/pdf");

        byte[] content = new byte[] {};
        Tuple tuple = mock(Tuple.class);
        when(tuple.getBinaryByField("content")).thenReturn(content);
        when(tuple.getStringByField("url")).thenReturn(url);
        when(tuple.getValueByField("metadata")).thenReturn(metadata);
        bolt.execute(tuple);

        List<List<Object>> outTuples = output.getEmitted(Constants.StatusStreamName);

        Assert.assertEquals(1, outTuples.size());
        Assert.assertTrue(outTuples.get(0).get(2).equals(Status.ERROR));

        outTuples.clear();

        metadata = new Metadata();
        metadata.addValue(
                "http." + HttpHeaders.CONTENT_TYPE,
                "application/vnd.openxmlformats-officedocument.wordprocessingml.document");

        parse(
                "http://www.digitalpebble.com/test_recursive_embedded.docx",
                "test_recursive_embedded.docx",
                metadata);

        outTuples = output.getEmitted();

        Assert.assertEquals(1, outTuples.size());
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.tika;

import com.digitalpebble.stormcrawler.Metadata;
import java.util.Map;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.topology.base.BaseRichBolt;
import org.apache.storm.tuple.Fields;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.tuple.Values;

/**
 * Uses Tika only if a document has not been parsed with anything else. Emits the tuples to be
 * processed with Tika on a stream of the same name ('tika').
 *
 * <p>Remember to set
 *
 * <pre>
 *   jsoup.treat.non.html.as.error: false
 * </pre>
 *
 * Use in your topologies as follows :
 *
 * <pre>
 * builder.setBolt(&quot;jsoup&quot;, new JSoupParserBolt()).localOrShuffleGrouping(
 *         &quot;sitemap&quot;);
 *
 * builder.setBolt(&quot;shunt&quot;, new RedirectionBolt()).localOrShuffleGrouping(&quot;jsoup&quot;);
 *
 * builder.setBolt(&quot;tika&quot;, new ParserBolt()).localOrShuffleGrouping(&quot;shunt&quot;,
 *         &quot;tika&quot;);
 *
 * builder.setBolt(&quot;indexer&quot;, new IndexingBolt(), numWorkers)
 *         .localOrShuffleGrouping(&quot;shunt&quot;).localOrShuffleGrouping(&quot;tika&quot;);
 * </pre>
 */
@SuppressWarnings("serial")
public class RedirectionBolt extends BaseRichBolt {

    private OutputCollector collector;

    public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
        this.collector = collector;
    }

    @Override
    public void execute(Tuple tuple) {
        String url = tuple.getStringByField("url");
        byte[] content = tuple.getBinaryByField("content");
        Metadata metadata = (Metadata) tuple.getValueByField("metadata");
        String text = tuple.getStringByField("text");

        Values v = new Values(url, content, metadata, text);

        if (metadata.getFirstValue("parsed.by") != null) {
            collector.emit(tuple, v);
        } else {
            collector.emit("tika", tuple, v);
        }

        collector.ack(tuple);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("url", "content", "metadata", "text"));
        declarer.declareStream("tika", new Fields("url", "content", "metadata", "text"));
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */

/*
 * XXX NUTCH COMMENT
 * XXX ab@apache.org: This class is copied verbatim from Xalan-J 2.6.0
 * XXX distribution, org.apache.xml.utils.DOMBuilder, in order to
 * avoid dependency on Xalan.
 */

package com.digitalpebble.stormcrawler.tika;

/**
 * Class used to verify whether the specified <var>ch</var> conforms to the XML 1.0 definition of
 * whitespace.
 */
class XMLCharacterRecognizer {

    private XMLCharacterRecognizer() {}

    /**
     * Returns whether the specified <var>ch</var> conforms to the XML 1.0 definition of whitespace.
     * Refer to <A href="http://www.w3.org/TR/1998/REC-xml-19980210#NT-S">the definition of <CODE>S
     * </CODE></A> for details.
     *
     * @param ch Character to check as XML whitespace.
     * @return =true if <var>ch</var> is XML whitespace; otherwise =false.
     */
    static boolean isWhiteSpace(char ch) {
        return (ch == 0x20) || (ch == 0x09) || (ch == 0xD) || (ch == 0xA);
    }

    /**
     * Tell if the string is whitespace.
     *
     * @param ch Character array to check as XML whitespace.
     * @param start Start index of characters in the array
     * @param length Number of characters in the array
     * @return True if the characters in the array are XML whitespace; otherwise, false.
     */
    static boolean isWhiteSpace(char ch[], int start, int length) {

        int end = start + length;

        for (int s = start; s < end; s++) {
            if (!isWhiteSpace(ch[s])) return false;
        }

        return true;
    }

    /**
     * Tell if the string is whitespace.
     *
     * @param buf StringBuffer to check as XML whitespace.
     * @return True if characters in buffer are XML whitespace, false otherwise
     */
    static boolean isWhiteSpace(StringBuffer buf) {

        int n = buf.length();

        for (int i = 0; i < n; i++) {
            if (!isWhiteSpace(buf.charAt(i))) return false;
        }

        return true;
    }

    /**
     * Tell if the string is whitespace.
     *
     * @param s String to check as XML whitespace.
     * @return True if characters in buffer are XML whitespace, false otherwise
     */
    static boolean isWhiteSpace(String s) {

        if (null != s) {
            int n = s.length();

            for (int i = 0; i < n; i++) {
                if (!isWhiteSpace(s.charAt(i))) return false;
            }
        }

        return true;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */

/*
 * XXX NUTCH COMMENT
 * XXX ab@apache.org: This class is copied verbatim from Xalan-J 2.6.0
 * XXX distribution, org.apache.xml.utils.DOMBuilder, in order to
 * avoid dependency on Xalan.
 */

package com.digitalpebble.stormcrawler.tika;

import java.util.Locale;
import java.util.Stack;
import org.w3c.dom.CDATASection;
import org.w3c.dom.Comment;
import org.w3c.dom.Document;
import org.w3c.dom.DocumentFragment;
import org.w3c.dom.Element;
import org.w3c.dom.Node;
import org.w3c.dom.Text;
import org.xml.sax.Attributes;
import org.xml.sax.ContentHandler;
import org.xml.sax.Locator;
import org.xml.sax.ext.LexicalHandler;

/**
 * This class takes SAX events (in addition to some extra events that SAX doesn't handle yet) and
 * adds the result to a document or document fragment.
 */
public class DOMBuilder implements ContentHandler, LexicalHandler {

    private boolean upperCaseElementNames = true;

    /** Root document */
    private Document m_doc;

    /** Current node */
    private Node m_currentNode = null;

    /** First node of document fragment or null if not a DocumentFragment */
    private DocumentFragment m_docFrag = null;

    /** Vector of element nodes */
    private Stack<Element> m_elemStack = new Stack<>();

    /** Element recorded with this namespace will be converted to Node without a namespace */
    private String defaultNamespaceURI = null;

    /**
     * DOMBuilder instance constructor... it will add the DOM nodes to the document fragment.
     *
     * @param doc Root document
     * @param node Current node
     */
    public DOMBuilder(Document doc, Node node) {
        m_doc = doc;
        m_currentNode = node;
    }

    /**
     * DOMBuilder instance constructor... it will add the DOM nodes to the document fragment.
     *
     * @param doc Root document
     * @param docFrag Document fragment
     */
    DOMBuilder(Document doc, DocumentFragment docFrag) {
        m_doc = doc;
        m_docFrag = docFrag;
    }

    /**
     * DOMBuilder instance constructor... it will add the DOM nodes to the document.
     *
     * @param doc Root document
     */
    DOMBuilder(Document doc) {
        m_doc = doc;
    }

    /**
     * Get the root node of the DOM being created. This is either a Document or a DocumentFragment.
     *
     * @return The root document or document fragment if not null
     */
    Node getRootNode() {
        return (null != m_docFrag) ? (Node) m_docFrag : (Node) m_doc;
    }

    /**
     * Get the node currently being processed.
     *
     * @return the current node being processed
     */
    Node getCurrentNode() {
        return m_currentNode;
    }

    /**
     * Return null since there is no Writer for this class.
     *
     * @return null
     */
    java.io.Writer getWriter() {
        return null;
    }

    /**
     * Append a node to the current container.
     *
     * @param newNode New node to append
     */
    protected void append(Node newNode) throws org.xml.sax.SAXException {

        Node currentNode = m_currentNode;

        if (null != currentNode) {
            currentNode.appendChild(newNode);

            // System.out.println(newNode.getNodeName());
        } else if (null != m_docFrag) {
            m_docFrag.appendChild(newNode);
        } else {
            boolean ok = true;
            short type = newNode.getNodeType();

            if (type == Node.TEXT_NODE) {
                String data = newNode.getNodeValue();

                if ((null != data) && (data.trim().length() > 0)) {
                    throw new org.xml.sax.SAXException(
                            "Warning: can't output text before document element!  Ignoring...");
                }

                ok = false;
            } else if (type == Node.ELEMENT_NODE) {
                if (m_doc.getDocumentElement() != null) {
                    throw new org.xml.sax.SAXException("Can't have more than one root on a DOM!");
                }
            }

            if (ok) {
                m_doc.appendChild(newNode);
            }
        }
    }

    /**
     * Receive an object for locating the origin of SAX document events.
     *
     * <p>SAX parsers are strongly encouraged (though not absolutely required) to supply a locator:
     * if it does so, it must supply the locator to the application by invoking this method before
     * invoking any of the other methods in the ContentHandler interface.
     *
     * <p>The locator allows the application to determine the end position of any document-related
     * event, even if the parser is not reporting an error. Typically, the application will use this
     * information for reporting its own errors (such as character content that does not match an
     * application's business rules). The information returned by the locator is probably not
     * sufficient for use with a search engine.
     *
     * <p>Note that the locator will return correct information only during the invocation of the
     * events in this interface. The application should not attempt to use it at any other time.
     *
     * @param locator An object that can return the location of any SAX document event.
     * @see org.xml.sax.Locator
     */
    @Override
    public void setDocumentLocator(Locator locator) {

        // No action for the moment.
    }

    /**
     * Receive notification of the beginning of a document.
     *
     * <p>The SAX parser will invoke this method only once, before any other methods in this
     * interface or in DTDHandler (except for setDocumentLocator).
     */
    @Override
    public void startDocument() throws org.xml.sax.SAXException {

        // No action for the moment.
    }

    /**
     * Receive notification of the end of a document.
     *
     * <p>The SAX parser will invoke this method only once, and it will be the last method invoked
     * during the parse. The parser shall not invoke this method until it has either abandoned
     * parsing (because of an unrecoverable error) or reached the end of input.
     */
    @Override
    public void endDocument() throws org.xml.sax.SAXException {

        // No action for the moment.
    }

    /**
     * Receive notification of the beginning of an element.
     *
     * <p>The Parser will invoke this method at the beginning of every element in the XML document;
     * there will be a corresponding endElement() event for every startElement() event (even when
     * the element is empty). All of the element's content will be reported, in order, before the
     * corresponding endElement() event.
     *
     * <p>If the element name has a namespace prefix, the prefix will still be attached. Note that
     * the attribute list provided will contain only attributes with explicit values (specified or
     * defaulted): #IMPLIED attributes will be omitted.
     *
     * @param ns The namespace of the node
     * @param localName The local part of the qualified name
     * @param name The element name.
     * @param atts The attributes attached to the element, if any.
     * @see #endElement
     * @see org.xml.sax.Attributes
     */
    @Override
    public void startElement(String ns, String localName, String name, Attributes atts)
            throws org.xml.sax.SAXException {

        Element elem;

        if (upperCaseElementNames) {
            name = name.toUpperCase(Locale.ROOT);
        }

        // Note that the namespace-aware call must be used to correctly
        // construct a Level 2 DOM, even for non-namespaced nodes.
        if ((null == ns) || (ns.length() == 0) || ns.equals(defaultNamespaceURI)) {
            elem = m_doc.createElementNS(null, name);
        } else {
            elem = m_doc.createElementNS(ns, name);
        }

        append(elem);

        try {
            int nAtts = atts.getLength();

            if (0 != nAtts) {
                for (int i = 0; i < nAtts; i++) {
                    // First handle a possible ID attribute
                    if (atts.getType(i).equalsIgnoreCase("ID")) {
                        setIDAttribute(atts.getValue(i), elem);
                    }

                    String attrNS = atts.getURI(i);

                    if ("".equals(attrNS)) {
                        attrNS = null; // DOM represents no-namespace as null
                    }

                    // System.out.println("attrNS: "+attrNS+", localName: "+atts.getQName(i)
                    // +", qname: "+atts.getQName(i)+", value: "+atts.getValue(i));
                    // Crimson won't let us set an xmlns: attribute on the DOM.
                    String attrQName = atts.getQName(i);

                    // In SAX, xmlns: attributes have an empty namespace, while
                    // in DOM they should have the xmlns namespace
                    if (attrQName.startsWith("xmlns:")) {
                        attrNS = "http://www.w3.org/2000/xmlns/";
                    }

                    // ALWAYS use the DOM Level 2 call!
                    elem.setAttributeNS(attrNS, attrQName, atts.getValue(i));
                }
            }

            // append(elem);

            m_elemStack.push(elem);

            m_currentNode = elem;

            // append(elem);
        } catch (java.lang.Exception de) {
            // de.printStackTrace();
            throw new org.xml.sax.SAXException(de);
        }
    }

    /**
     * Receive notification of the end of an element.
     *
     * <p>The SAX parser will invoke this method at the end of every element in the XML document;
     * there will be a corresponding startElement() event for every endElement() event (even when
     * the element is empty).
     *
     * <p>If the element name has a namespace prefix, the prefix will still be attached to the name.
     *
     * @param ns the namespace of the element
     * @param localName The local part of the qualified name of the element
     * @param name The element name
     */
    @Override
    public void endElement(String ns, String localName, String name)
            throws org.xml.sax.SAXException {
        m_elemStack.pop();
        m_currentNode = m_elemStack.isEmpty() ? null : (Node) m_elemStack.peek();
    }

    /**
     * Set an ID string to node association in the ID table.
     *
     * @param id The ID string.
     * @param elem The associated ID.
     */
    public void setIDAttribute(String id, Element elem) {

        // Do nothing. This method is meant to be overiden.
    }

    /**
     * Receive notification of character data.
     *
     * <p>The Parser will call this method to report each chunk of character data. SAX parsers may
     * return all contiguous character data in a single chunk, or they may split it into several
     * chunks; however, all of the characters in any single event must come from the same external
     * entity, so that the Locator provides useful information.
     *
     * <p>The application must not attempt to read from the array outside of the specified range.
     *
     * <p>Note that some parsers will report whitespace using the ignorableWhitespace() method
     * rather than this one (validating parsers must do so).
     *
     * @param ch The characters from the XML document.
     * @param start The start position in the array.
     * @param length The number of characters to read from the array.
     * @see #ignorableWhitespace
     * @see org.xml.sax.Locator
     */
    @Override
    public void characters(char ch[], int start, int length) throws org.xml.sax.SAXException {
        if (isOutsideDocElem() && XMLCharacterRecognizer.isWhiteSpace(ch, start, length)) {
            return; // avoid DOM006 Hierarchy request error
        }

        if (m_inCData) {
            cdata(ch, start, length);

            return;
        }

        String s = new String(ch, start, length);
        Node childNode;
        childNode = m_currentNode != null ? m_currentNode.getLastChild() : null;
        if (childNode != null && childNode.getNodeType() == Node.TEXT_NODE) {
            ((Text) childNode).appendData(s);
        } else {
            Text text = m_doc.createTextNode(s);
            append(text);
        }
    }

    /**
     * If available, when the disable-output-escaping attribute is used, output raw text without
     * escaping. A PI will be inserted in front of the node with the name "lotusxsl-next-is-raw" and
     * a value of "formatter-to-dom".
     *
     * @param ch Array containing the characters
     * @param start Index to start of characters in the array
     * @param length Number of characters in the array
     */
    public void charactersRaw(char ch[], int start, int length) throws org.xml.sax.SAXException {
        if (isOutsideDocElem() && XMLCharacterRecognizer.isWhiteSpace(ch, start, length)) {
            return; // avoid DOM006 Hierarchy request error
        }

        String s = new String(ch, start, length);

        append(m_doc.createProcessingInstruction("xslt-next-is-raw", "formatter-to-dom"));
        append(m_doc.createTextNode(s));
    }

    /**
     * Report the beginning of an entity.
     *
     * <p>The start and end of the document entity are not reported. The start and end of the
     * external DTD subset are reported using the pseudo-name "[dtd]". All other events must be
     * properly nested within start/end entity events.
     *
     * @param name The name of the entity. If it is a parameter entity, the name will begin with
     *     '%'.
     * @see #endEntity
     * @see org.xml.sax.ext.DeclHandler#internalEntityDecl
     * @see org.xml.sax.ext.DeclHandler#externalEntityDecl
     */
    @Override
    public void startEntity(String name) throws org.xml.sax.SAXException {

        // Almost certainly the wrong behavior...
        // entityReference(name);
    }

    /**
     * Report the end of an entity.
     *
     * @param name The name of the entity that is ending.
     * @see #startEntity
     */
    @Override
    public void endEntity(String name) throws org.xml.sax.SAXException {}

    /**
     * Receive notivication of a entityReference.
     *
     * @param name name of the entity reference
     */
    public void entityReference(String name) throws org.xml.sax.SAXException {
        append(m_doc.createEntityReference(name));
    }

    /**
     * Receive notification of ignorable whitespace in element content.
     *
     * <p>Validating Parsers must use this method to report each chunk of ignorable whitespace (see
     * the W3C XML 1.0 recommendation, section 2.10): non-validating parsers may also use this
     * method if they are capable of parsing and using content models.
     *
     * <p>SAX parsers may return all contiguous whitespace in a single chunk, or they may split it
     * into several chunks; however, all of the characters in any single event must come from the
     * same external entity, so that the Locator provides useful information.
     *
     * <p>The application must not attempt to read from the array outside of the specified range.
     *
     * @param ch The characters from the XML document.
     * @param start The start position in the array.
     * @param length The number of characters to read from the array.
     * @see #characters
     */
    @Override
    public void ignorableWhitespace(char ch[], int start, int length)
            throws org.xml.sax.SAXException {
        if (isOutsideDocElem()) {
            return; // avoid DOM006 Hierarchy request error
        }

        String s = new String(ch, start, length);

        append(m_doc.createTextNode(s));
    }

    /**
     * Tell if the current node is outside the document element.
     *
     * @return true if the current node is outside the document element.
     */
    private boolean isOutsideDocElem() {
        return (null == m_docFrag)
                && m_elemStack.size() == 0
                && (null == m_currentNode || m_currentNode.getNodeType() == Node.DOCUMENT_NODE);
    }

    /**
     * Receive notification of a processing instruction.
     *
     * <p>The Parser will invoke this method once for each processing instruction found: note that
     * processing instructions may occur before or after the main document element.
     *
     * <p>A SAX parser should never report an XML declaration (XML 1.0, section 2.8) or a text
     * declaration (XML 1.0, section 4.3.1) using this method.
     *
     * @param target The processing instruction target.
     * @param data The processing instruction data, or null if none was supplied.
     */
    @Override
    public void processingInstruction(String target, String data) throws org.xml.sax.SAXException {
        append(m_doc.createProcessingInstruction(target, data));
    }

    /**
     * Report an XML comment anywhere in the document.
     *
     * <p>This callback will be used for comments inside or outside the document element, including
     * comments in the external DTD subset (if read).
     *
     * @param ch An array holding the characters in the comment.
     * @param start The starting position in the array.
     * @param length The number of characters to use from the array.
     */
    @Override
    public void comment(char ch[], int start, int length) throws org.xml.sax.SAXException {
        // tagsoup sometimes submits invalid values here
        if (ch == null || start < 0 || length >= (ch.length - start) || length < 0) {
            return;
        }
        append(m_doc.createComment(new String(ch, start, length)));
    }

    /** Flag indicating that we are processing a CData section */
    protected boolean m_inCData = false;

    /**
     * Report the start of a CDATA section.
     *
     * @see #endCDATA
     */
    @Override
    public void startCDATA() throws org.xml.sax.SAXException {
        m_inCData = true;
        append(m_doc.createCDATASection(""));
    }

    /**
     * Report the end of a CDATA section.
     *
     * @see #startCDATA
     */
    @Override
    public void endCDATA() throws org.xml.sax.SAXException {
        m_inCData = false;
    }

    /**
     * Receive notification of cdata.
     *
     * <p>The Parser will call this method to report each chunk of character data. SAX parsers may
     * return all contiguous character data in a single chunk, or they may split it into several
     * chunks; however, all of the characters in any single event must come from the same external
     * entity, so that the Locator provides useful information.
     *
     * <p>The application must not attempt to read from the array outside of the specified range.
     *
     * <p>Note that some parsers will report whitespace using the ignorableWhitespace() method
     * rather than this one (validating parsers must do so).
     *
     * @param ch The characters from the XML document.
     * @param start The start position in the array.
     * @param length The number of characters to read from the array.
     * @see #ignorableWhitespace
     * @see org.xml.sax.Locator
     */
    public void cdata(char ch[], int start, int length) {
        if (isOutsideDocElem() && XMLCharacterRecognizer.isWhiteSpace(ch, start, length)) {
            return; // avoid DOM006 Hierarchy request error
        }

        String s = new String(ch, start, length);

        // XXX ab@apache.org: modified from the original, to accomodate TagSoup.
        Node n = m_currentNode.getLastChild();
        if (n instanceof CDATASection) {
            ((CDATASection) n).appendData(s);
        } else if (n instanceof Comment) {
            ((Comment) n).appendData(s);
        }
    }

    /**
     * Report the start of DTD declarations, if any.
     *
     * <p>Any declarations are assumed to be in the internal subset unless otherwise indicated.
     *
     * @param name The document type name.
     * @param publicId The declared public identifier for the external DTD subset, or null if none
     *     was declared.
     * @param systemId The declared system identifier for the external DTD subset, or null if none
     *     was declared.
     * @see #endDTD
     * @see #startEntity
     */
    @Override
    public void startDTD(String name, String publicId, String systemId)
            throws org.xml.sax.SAXException {

        // Do nothing for now.
    }

    /**
     * Report the end of DTD declarations.
     *
     * @see #startDTD
     */
    @Override
    public void endDTD() throws org.xml.sax.SAXException {

        // Do nothing for now.
    }

    /**
     * Begin the scope of a prefix-URI Namespace mapping.
     *
     * <p>The information from this event is not necessary for normal Namespace processing: the SAX
     * XML reader will automatically replace prefixes for element and attribute names when the
     * http://xml.org/sax/features/namespaces feature is true (the default).
     *
     * <p>There are cases, however, when applications need to use prefixes in character data or in
     * attribute values, where they cannot safely be expanded automatically; the
     * start/endPrefixMapping event supplies the information to the application to expand prefixes
     * in those contexts itself, if necessary.
     *
     * <p>Note that start/endPrefixMapping events are not guaranteed to be properly nested relative
     * to each-other: all startPrefixMapping events will occur before the corresponding startElement
     * event, and all endPrefixMapping events will occur after the corresponding endElement event,
     * but their order is not guaranteed.
     *
     * @param prefix The Namespace prefix being declared.
     * @param uri The Namespace URI the prefix is mapped to.
     * @see #endPrefixMapping
     * @see #startElement
     */
    @Override
    public void startPrefixMapping(String prefix, String uri) throws org.xml.sax.SAXException {

        /*
         * // Not sure if this is needed or wanted // Also, it fails in the
         * stree. if((null != m_currentNode) && (m_currentNode.getNodeType() ==
         * Node.ELEMENT_NODE)) { String qname; if(((null != prefix) &&
         * (prefix.length() == 0)) || (null == prefix)) qname = "xmlns"; else
         * qname = "xmlns:"+prefix;
         *
         * Element elem = (Element)m_currentNode; String val =
         * elem.getAttribute(qname); // Obsolete, should be DOM2...? if(val ==
         * null) { elem.setAttributeNS("http://www.w3.org/XML/1998/namespace",
         * qname, uri); } }
         */
    }

    /**
     * End the scope of a prefix-URI mapping.
     *
     * <p>See startPrefixMapping for details. This event will always occur after the corresponding
     * endElement event, but the order of endPrefixMapping events is not otherwise guaranteed.
     *
     * @param prefix The prefix that was being mapping.
     * @see #startPrefixMapping
     * @see #endElement
     */
    @Override
    public void endPrefixMapping(String prefix) throws org.xml.sax.SAXException {}

    /**
     * Receive notification of a skipped entity.
     *
     * <p>The Parser will invoke this method once for each entity skipped. Non-validating processors
     * may skip entities if they have not seen the declarations (because, for example, the entity
     * was declared in an external DTD subset). All processors may skip external entities, depending
     * on the values of the http://xml.org/sax/features/external-general-entities and the
     * http://xml.org/sax/features/external-parameter-entities properties.
     *
     * @param name The name of the skipped entity. If it is a parameter entity, the name will begin
     *     with '%'.
     */
    @Override
    public void skippedEntity(String name) throws org.xml.sax.SAXException {}

    public boolean isUpperCaseElementNames() {
        return upperCaseElementNames;
    }

    public void setUpperCaseElementNames(boolean upperCaseElementNames) {
        this.upperCaseElementNames = upperCaseElementNames;
    }

    public String getDefaultNamespaceURI() {
        return defaultNamespaceURI;
    }

    public void setDefaultNamespaceURI(String defaultNamespaceURI) {
        this.defaultNamespaceURI = defaultNamespaceURI;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.tika;

import static com.digitalpebble.stormcrawler.Constants.StatusStreamName;

import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.filtering.URLFilters;
import com.digitalpebble.stormcrawler.parse.Outlink;
import com.digitalpebble.stormcrawler.parse.ParseData;
import com.digitalpebble.stormcrawler.parse.ParseFilter;
import com.digitalpebble.stormcrawler.parse.ParseFilters;
import com.digitalpebble.stormcrawler.parse.ParseResult;
import com.digitalpebble.stormcrawler.persistence.Status;
import com.digitalpebble.stormcrawler.protocol.HttpHeaders;
import com.digitalpebble.stormcrawler.protocol.ProtocolResponse;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import com.digitalpebble.stormcrawler.util.MetadataTransfer;
import com.digitalpebble.stormcrawler.util.URLUtil;
import java.io.ByteArrayInputStream;
import java.io.IOException;
import java.net.MalformedURLException;
import java.net.URL;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import org.apache.commons.lang.StringUtils;
import org.apache.html.dom.HTMLDocumentImpl;
import org.apache.storm.metric.api.MultiCountMetric;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.topology.base.BaseRichBolt;
import org.apache.storm.tuple.Fields;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.tuple.Values;
import org.apache.tika.Tika;
import org.apache.tika.config.TikaConfig;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.Parser;
import org.apache.tika.parser.html.HtmlMapper;
import org.apache.tika.parser.html.IdentityHtmlMapper;
import org.apache.tika.sax.BodyContentHandler;
import org.apache.tika.sax.Link;
import org.apache.tika.sax.LinkContentHandler;
import org.apache.tika.sax.TeeContentHandler;
import org.apache.tika.sax.XHTMLContentHandler;
import org.slf4j.LoggerFactory;
import org.w3c.dom.DocumentFragment;
import org.xml.sax.ContentHandler;

/** Uses Tika to parse the output of a fetch and extract text + metadata */
@SuppressWarnings("serial")
public class ParserBolt extends BaseRichBolt {

    private Tika tika;

    private URLFilters urlFilters = null;
    private ParseFilter parseFilters = null;

    private OutputCollector collector;

    private static final org.slf4j.Logger LOG = LoggerFactory.getLogger(ParserBolt.class);

    private MultiCountMetric eventCounter;

    private boolean upperCaseElementNames = true;
    private Class<?> HTMLMapperClass = IdentityHtmlMapper.class;

    private boolean extractEmbedded = false;

    private MetadataTransfer metadataTransfer;
    private boolean emitOutlinks = true;

    /** regular expressions to apply to the mime-type * */
    private List<String> mimeTypeWhiteList = new LinkedList<>();

    private String protocolMDprefix;

    @SuppressWarnings({"rawtypes", "unchecked"})
    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollector collector) {

        emitOutlinks = ConfUtils.getBoolean(conf, "parser.emitOutlinks", true);

        urlFilters = URLFilters.fromConf(conf);

        parseFilters = ParseFilters.fromConf(conf);

        upperCaseElementNames = ConfUtils.getBoolean(conf, "parser.uppercase.element.names", true);

        extractEmbedded = ConfUtils.getBoolean(conf, "parser.extract.embedded", false);

        String htmlmapperClassName =
                ConfUtils.getString(
                        conf,
                        "parser.htmlmapper.classname",
                        "org.apache.tika.parser.html.IdentityHtmlMapper");

        try {
            HTMLMapperClass = Class.forName(htmlmapperClassName);
            boolean interfaceOK = HtmlMapper.class.isAssignableFrom(HTMLMapperClass);
            if (!interfaceOK) {
                throw new RuntimeException(
                        "Class " + htmlmapperClassName + " does not implement HtmlMapper");
            }
        } catch (ClassNotFoundException e) {
            LOG.error("Can't load class {}", htmlmapperClassName);
            throw new RuntimeException("Can't load class " + htmlmapperClassName);
        }

        mimeTypeWhiteList = ConfUtils.loadListFromConf("parser.mimetype.whitelist", conf);

        protocolMDprefix = ConfUtils.getString(conf, ProtocolResponse.PROTOCOL_MD_PREFIX_PARAM, "");

        tika = instantiateTika(conf);

        this.collector = collector;

        this.eventCounter =
                context.registerMetric(this.getClass().getSimpleName(), new MultiCountMetric(), 10);

        this.metadataTransfer = MetadataTransfer.getInstance(conf);
    }

    @Override
    public void execute(Tuple tuple) {
        eventCounter.scope("tuple_in").incrBy(1);

        byte[] content = tuple.getBinaryByField("content");

        String url = tuple.getStringByField("url");
        Metadata metadata = (Metadata) tuple.getValueByField("metadata");

        // check that the mimetype is in the whitelist
        if (mimeTypeWhiteList.size() > 0) {
            boolean mt_match = false;
            // see if a mimetype was guessed in JSOUPBolt
            String mimeType = metadata.getFirstValue("parse.Content-Type");
            // otherwise rely on what could have been obtained from HTTP
            if (mimeType == null) {
                mimeType = metadata.getFirstValue(HttpHeaders.CONTENT_TYPE, this.protocolMDprefix);
            }
            if (mimeType != null) {
                for (String mt : mimeTypeWhiteList) {
                    if (mimeType.matches(mt)) {
                        mt_match = true;
                        break;
                    }
                }
            }
            if (!mt_match) {
                handleException(url, null, metadata, tuple, "content type");
                return;
            }
        }

        // the document got trimmed during the fetching - no point in trying to
        // parse it
        if ("true"
                .equalsIgnoreCase(
                        metadata.getFirstValue(
                                ProtocolResponse.TRIMMED_RESPONSE_KEY, this.protocolMDprefix))) {
            handleException(url, null, metadata, tuple, "skipped_trimmed");
            return;
        }

        long start = System.currentTimeMillis();

        ByteArrayInputStream bais = new ByteArrayInputStream(content);
        org.apache.tika.metadata.Metadata md = new org.apache.tika.metadata.Metadata();

        // provide the mime-type as a clue for guessing
        String httpCT = metadata.getFirstValue(HttpHeaders.CONTENT_TYPE, this.protocolMDprefix);
        if (StringUtils.isNotBlank(httpCT)) {
            // pass content type from server as a clue
            md.set(org.apache.tika.metadata.Metadata.CONTENT_TYPE, httpCT);
        }

        // as well as the filename
        try {
            URL _url = new URL(url);
            md.set(TikaCoreProperties.RESOURCE_NAME_KEY, _url.getFile());
        } catch (MalformedURLException e1) {
            throw new IllegalStateException("Malformed URL", e1);
        }

        LinkContentHandler linkHandler = new LinkContentHandler();
        ContentHandler textHandler = new BodyContentHandler(-1);
        TeeContentHandler teeHandler = new TeeContentHandler(linkHandler, textHandler);
        ParseContext parseContext = new ParseContext();

        if (extractEmbedded) {
            parseContext.set(Parser.class, tika.getParser());
        }

        try {
            parseContext.set(HtmlMapper.class, (HtmlMapper) HTMLMapperClass.newInstance());
        } catch (Exception e) {
            LOG.error("Exception while specifying HTMLMapper {}", url, e);
        }

        // build a DOM if required by the parseFilters
        DocumentFragment root = null;
        if (parseFilters.needsDOM()) {
            HTMLDocumentImpl doc = new HTMLDocumentImpl();
            doc.setErrorChecking(false);
            root = doc.createDocumentFragment();
            DOMBuilder domhandler = new DOMBuilder(doc, root);
            domhandler.setUpperCaseElementNames(upperCaseElementNames);
            domhandler.setDefaultNamespaceURI(XHTMLContentHandler.XHTML);
            teeHandler = new TeeContentHandler(linkHandler, textHandler, domhandler);
        }

        // parse
        String text;
        try {
            tika.getParser().parse(bais, teeHandler, md, parseContext);
            text = textHandler.toString();
        } catch (Throwable e) {
            handleException(url, e, metadata, tuple, "parse error");
            return;
        } finally {
            try {
                bais.close();
            } catch (IOException e) {
                LOG.error("Exception while closing stream", e);
            }
        }

        // add parse md to metadata
        for (String k : md.names()) {
            String[] values = md.getValues(k);
            metadata.setValues("parse." + k, values);
        }

        long duration = System.currentTimeMillis() - start;

        LOG.info("Parsed {} in {} msec", url, duration);

        // filter and convert the outlinks
        List<Outlink> outlinks = toOutlinks(url, linkHandler.getLinks(), metadata);

        ParseResult parse = new ParseResult(outlinks);

        // parse data of the parent URL
        ParseData parseData = parse.get(url);
        parseData.setMetadata(metadata);
        parseData.setText(text);
        parseData.setContent(content);

        // apply the parse filters if any
        try {
            parseFilters.filter(url, content, root, parse);
        } catch (RuntimeException e) {
            handleException(url, e, metadata, tuple, "parse filters");
            return;
        }

        if (emitOutlinks) {
            for (Outlink outlink : parse.getOutlinks()) {
                collector.emit(
                        StatusStreamName,
                        tuple,
                        new Values(
                                outlink.getTargetURL(), outlink.getMetadata(), Status.DISCOVERED));
            }
        }

        // emit each document/subdocument in the ParseResult object
        // there should be at least one ParseData item for the "parent" URL

        for (Map.Entry<String, ParseData> doc : parse) {
            ParseData parseDoc = doc.getValue();

            collector.emit(
                    tuple,
                    new Values(
                            doc.getKey(),
                            parseDoc.getContent(),
                            parseDoc.getMetadata(),
                            parseDoc.getText()));
        }

        collector.ack(tuple);
        eventCounter.scope("tuple_success").incrBy(1);
    }

    private Tika instantiateTika(Map<String, Object> conf) {
        Tika tika = null;
        String tikaConfigFile =
                ConfUtils.getString(conf, "parser.tika.config.file", "tika-config.xml");
        long start = System.currentTimeMillis();
        URL tikaConfigUrl = getClass().getClassLoader().getResource(tikaConfigFile);
        if (tikaConfigUrl == null) {
            LOG.error("Tika configuration file {} not found on classpath", tikaConfigFile);
        } else {
            LOG.info("Instantiating Tika using custom configuration {}", tikaConfigUrl);
            try {
                TikaConfig tikaConfig = new TikaConfig(tikaConfigUrl, getClass().getClassLoader());
                tika = new Tika(tikaConfig);
            } catch (Exception e) {
                LOG.error(
                        "Failed to instantiate Tika using custom configuration {}",
                        tikaConfigUrl,
                        e);
            }
        }
        if (tika == null) {
            LOG.info("Instantiating Tika with default configuration");
            tika = new Tika();
        }
        long end = System.currentTimeMillis();
        LOG.debug("Tika loaded in {} msec", end - start);
        return tika;
    }

    private void handleException(
            String url, Throwable e, Metadata metadata, Tuple tuple, String errorType) {
        // real exception?
        if (e != null) {
            LOG.error("{} -> {}", errorType, url, e);
        } else {
            LOG.info("{} -> {}", errorType, url);
        }
        metadata.setValue(Constants.STATUS_ERROR_SOURCE, "TIKA");
        metadata.setValue(Constants.STATUS_ERROR_MESSAGE, errorType);
        collector.emit(StatusStreamName, tuple, new Values(url, metadata, Status.ERROR));
        collector.ack(tuple);
        // Increment metric that is context specific
        String s = "error_" + errorType.replaceAll(" ", "_");
        eventCounter.scope(s).incrBy(1);
        // Increment general metric
        eventCounter.scope("parse exception").incrBy(1);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("url", "content", "metadata", "text"));
        declarer.declareStream(StatusStreamName, new Fields("url", "metadata", "status"));
    }

    private List<Outlink> toOutlinks(String parentURL, List<Link> links, Metadata parentMetadata) {

        Map<String, Outlink> outlinks = new HashMap<String, Outlink>();

        URL url_;
        try {
            url_ = new URL(parentURL);
        } catch (MalformedURLException e1) {
            // we would have known by now as previous
            // components check whether the URL is valid
            LOG.error("MalformedURLException on {}", parentURL);
            eventCounter.scope("error_invalid_source_url").incrBy(1);
            return new LinkedList<Outlink>();
        }

        for (Link l : links) {
            if (StringUtils.isBlank(l.getUri())) {
                continue;
            }
            String urlOL;

            // build an absolute URL
            try {
                URL tmpURL = URLUtil.resolveURL(url_, l.getUri());
                urlOL = tmpURL.toExternalForm();
            } catch (MalformedURLException e) {
                LOG.debug("MalformedURLException on {}", l.getUri());
                eventCounter
                        .scope("error_outlink_parsing_" + e.getClass().getSimpleName())
                        .incrBy(1);
                continue;
            }

            // applies the URL filters
            if (urlFilters != null) {
                urlOL = urlFilters.filter(url_, parentMetadata, urlOL);
                if (urlOL == null) {
                    eventCounter.scope("outlink_filtered").incrBy(1);
                    continue;
                }
            }

            eventCounter.scope("outlink_kept").incrBy(1);

            Outlink ol = new Outlink(urlOL);
            // add the anchor
            ol.setAnchor(l.getText());

            // get the metadata for the outlink from the parent ones
            ol.setMetadata(metadataTransfer.getMetaForOutlink(urlOL, parentURL, parentMetadata));

            // keep only one instance of outlink per URL
            Outlink ol2 = outlinks.get(urlOL);
            if (ol2 == null) {
                outlinks.put(urlOL, ol);
            }
        }
        return new ArrayList<Outlink>(outlinks.values());
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.parse.filter;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.parse.ParseFilter;
import com.digitalpebble.stormcrawler.parse.ParseResult;
import com.fasterxml.jackson.databind.JsonNode;
import com.optimaize.langdetect.DetectedLanguage;
import com.optimaize.langdetect.LanguageDetector;
import com.optimaize.langdetect.LanguageDetectorBuilder;
import com.optimaize.langdetect.ngram.NgramExtractors;
import com.optimaize.langdetect.profiles.LanguageProfile;
import com.optimaize.langdetect.profiles.LanguageProfileReader;
import com.optimaize.langdetect.text.RemoveMinorityScriptsTextFilter;
import com.optimaize.langdetect.text.TextObject;
import com.optimaize.langdetect.text.TextObjectFactory;
import com.optimaize.langdetect.text.TextObjectFactoryBuilder;
import com.optimaize.langdetect.text.UrlTextFilter;
import java.io.IOException;
import java.util.List;
import java.util.Locale;
import java.util.Map;
import org.apache.commons.lang.StringUtils;
import org.w3c.dom.DocumentFragment;

/**
 * Language identification; the language codes gets stored in the metadata. <br>
 * To use it, just add the module as a dependency in your pom and include
 *
 * <p>```json { "class": "com.digitalpebble.stormcrawler.parse.filter.LanguageID", "name":
 * "LanguageID", "params": { "key": "lang" , "minProb": 0.99 , "extracted": "parse.lang"} } ```
 *
 * <p>in the parse filter config. Any value found in the metadata under the key specified by
 * _extracted_ will be normalised and stored in the metadata, otherwise the languages above the
 * probability will be used.
 */
public class LanguageID extends ParseFilter {

    private static LanguageDetector languageDetector;

    private static final int maxTextLength = 10000;

    private static final TextObjectFactory textObjectFactory =
            new TextObjectFactoryBuilder()
                    .maxTextLength(maxTextLength)
                    .withTextFilter(UrlTextFilter.getInstance())
                    .withTextFilter(RemoveMinorityScriptsTextFilter.forThreshold(0.3))
                    .build();

    private String mdKey = "lang";
    private float minProb = 0.999f;
    private String extractedKeyName = "parse.lang";

    static {
        try {
            // load all languages:
            List<LanguageProfile> languageProfiles = new LanguageProfileReader().readAllBuiltIn();
            // build language detector:
            languageDetector =
                    LanguageDetectorBuilder.create(NgramExtractors.standard())
                            .withProfiles(languageProfiles)
                            .build();
        } catch (IOException e) {
            throw new RuntimeException("Error while loading language profiles", e);
        }
    }

    @Override
    public void configure(Map stormConf, JsonNode filterParams) {
        JsonNode node = filterParams.get("key");
        if (node != null && node.isTextual()) {
            mdKey = node.asText("lang");
        }
        node = filterParams.get("minProb");
        if (node != null && node.isNumber()) {
            minProb = node.floatValue();
        }
        node = filterParams.get("extracted");
        if (node != null && node.isTextual()) {
            extractedKeyName = node.asText("parse.lang");
        }
    }

    @Override
    public void filter(String url, byte[] content, DocumentFragment doc, ParseResult parse) {

        // check whether the metadata already contains a lang value
        // in which case we normalise its value and use it
        Metadata m = parse.get(url).getMetadata();
        String extractedValue = m.getFirstValue(extractedKeyName);
        if (StringUtils.isNotBlank(extractedValue) && extractedValue.length() > 1) {
            extractedValue = extractedValue.substring(0, 2).toLowerCase(Locale.ENGLISH);
            LOG.info("Lang: {} extracted from page for {}", extractedValue, url);
            m.setValue(mdKey, extractedValue);
            return;
        }

        String text = parse.get(url).getText();
        if (StringUtils.isBlank(text)) {
            return;
        }

        if (text.length() > maxTextLength) {
            text = text.substring(0, maxTextLength);
        }

        TextObject textObject = textObjectFactory.forText(text);
        synchronized (languageDetector) {
            List<DetectedLanguage> probs = languageDetector.getProbabilities(textObject);
            if (probs == null || probs.size() == 0) {
                return;
            }
            for (DetectedLanguage lang : probs) {
                if (lang.getProbability() >= minProb) {
                    String code = lang.getLocale().getLanguage();
                    parse.get(url).getMetadata().addValue(mdKey, code);
                }
            }
        }
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.bolt;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.filtering.URLFilters;
import com.digitalpebble.stormcrawler.parse.Outlink;
import com.digitalpebble.stormcrawler.persistence.Status;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import com.digitalpebble.stormcrawler.util.MetadataTransfer;
import com.digitalpebble.stormcrawler.util.URLUtil;
import java.net.MalformedURLException;
import java.net.URL;
import java.util.Map;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.topology.base.BaseRichBolt;
import org.apache.storm.tuple.Fields;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.tuple.Values;

/**
 * Provides common functionalities for Bolts which emit tuples to the status stream, e.g. Fetchers,
 * Parsers. Encapsulates the logic of URL filtering and metadata transfer to outlinks.
 */
public abstract class StatusEmitterBolt extends BaseRichBolt {

    private URLFilters urlFilters;

    private MetadataTransfer metadataTransfer;

    private boolean allowRedirs;

    protected OutputCollector collector;

    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
        this.collector = collector;
        urlFilters = URLFilters.fromConf(stormConf);
        metadataTransfer = MetadataTransfer.getInstance(stormConf);
        allowRedirs =
                ConfUtils.getBoolean(
                        stormConf,
                        com.digitalpebble.stormcrawler.Constants.AllowRedirParamName,
                        true);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declareStream(
                com.digitalpebble.stormcrawler.Constants.StatusStreamName,
                new Fields("url", "metadata", "status"));
    }

    /**
     * Used for redirections or when discovering sitemap URLs. The custom key / values are added to
     * the target metadata post-filtering.
     */
    protected void emitOutlink(
            Tuple t, URL sURL, String newUrl, Metadata sourceMetadata, String... customKeyVals) {

        Outlink ol = filterOutlink(sURL, newUrl, sourceMetadata, customKeyVals);
        if (ol == null) return;

        collector.emit(
                com.digitalpebble.stormcrawler.Constants.StatusStreamName,
                t,
                new Values(ol.getTargetURL(), ol.getMetadata(), Status.DISCOVERED));
    }

    protected Outlink filterOutlink(
            URL sURL, String newUrl, Metadata sourceMetadata, String... customKeyVals) {
        // build an absolute URL
        try {
            URL tmpURL = URLUtil.resolveURL(sURL, newUrl);
            newUrl = tmpURL.toExternalForm();
        } catch (MalformedURLException e) {
            return null;
        }

        // apply URL filters
        newUrl = this.urlFilters.filter(sURL, sourceMetadata, newUrl);

        // filtered
        if (newUrl == null) {
            return null;
        }

        Metadata metadata =
                metadataTransfer.getMetaForOutlink(newUrl, sURL.toExternalForm(), sourceMetadata);

        for (int i = 0; i < customKeyVals.length; i = i + 2) {
            metadata.addValue(customKeyVals[i], customKeyVals[i + 1]);
        }

        Outlink l = new Outlink(newUrl);
        l.setMetadata(metadata);
        return l;
    }

    protected boolean allowRedirs() {
        return allowRedirs;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.bolt;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.filtering.URLFilters;
import com.digitalpebble.stormcrawler.persistence.Status;
import java.io.IOException;
import java.util.Map;
import org.apache.commons.lang.StringUtils;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.topology.base.BaseRichBolt;
import org.apache.storm.tuple.Fields;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.tuple.Values;
import org.apache.storm.utils.Utils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class URLFilterBolt extends BaseRichBolt {

    public static final Logger LOG = LoggerFactory.getLogger(URLFilterBolt.class);

    private URLFilters urlFilters;

    protected OutputCollector collector;

    private final String filterConfigFile;

    private final boolean discoveredOnly;

    private static final String _s = com.digitalpebble.stormcrawler.Constants.StatusStreamName;

    /**
     * Relies on the file defined in urlfilters.config.file and applied to all tuples regardless of
     * status
     */
    public URLFilterBolt() {
        this(false, null);
    }

    public URLFilterBolt(boolean discoveredOnly, String filterConfigFile) {
        this.discoveredOnly = discoveredOnly;
        this.filterConfigFile = filterConfigFile;
    }

    @Override
    public void execute(Tuple input) {
        // the input can come from the standard stream or the status one
        // we'll emit to whichever it came from
        String stream = input.getSourceStreamId();
        if (stream == null) stream = Utils.DEFAULT_STREAM_ID;

        // must have at least a URL and metadata, possibly a status
        String urlString = input.getStringByField("url");
        Metadata metadata = (Metadata) input.getValueByField("metadata");
        Status status = (Status) input.getValueByField("status");

        // not a status we want to filter
        if (discoveredOnly && !status.equals(Status.DISCOVERED)) {
            Values v = new Values(urlString, metadata, status);
            collector.emit(stream, v);
            collector.ack(input);
            return;
        }

        String filtered = urlFilters.filter(null, null, urlString);
        if (StringUtils.isBlank(filtered)) {
            LOG.debug("URL rejected: {}", urlString);
            collector.ack(input);
            return;
        }

        Values v = new Values(filtered, metadata, status);
        collector.emit(stream, v);
        collector.ack(input);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        Fields f = new Fields("url", "metadata", "status");
        declarer.declareStream(_s, f);
        declarer.declare(f);
    }

    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
        this.collector = collector;
        if (filterConfigFile != null) {
            try {
                urlFilters = new URLFilters(stormConf, filterConfigFile);
            } catch (IOException e) {
                throw new RuntimeException("Can't load filters from " + filterConfigFile);
            }
        } else {
            urlFilters = URLFilters.fromConf(stormConf);
        }
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.bolt;

import static com.digitalpebble.stormcrawler.Constants.StatusStreamName;

import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.parse.Outlink;
import com.digitalpebble.stormcrawler.parse.ParseFilter;
import com.digitalpebble.stormcrawler.parse.ParseFilters;
import com.digitalpebble.stormcrawler.parse.ParseResult;
import com.digitalpebble.stormcrawler.persistence.DefaultScheduler;
import com.digitalpebble.stormcrawler.persistence.Status;
import com.digitalpebble.stormcrawler.protocol.HttpHeaders;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import com.google.common.primitives.Bytes;
import crawlercommons.sitemaps.AbstractSiteMap;
import crawlercommons.sitemaps.Namespace;
import crawlercommons.sitemaps.SiteMap;
import crawlercommons.sitemaps.SiteMapIndex;
import crawlercommons.sitemaps.SiteMapParser;
import crawlercommons.sitemaps.SiteMapURL;
import crawlercommons.sitemaps.SiteMapURL.ChangeFrequency;
import crawlercommons.sitemaps.UnknownFormatException;
import crawlercommons.sitemaps.extension.Extension;
import crawlercommons.sitemaps.extension.ExtensionMetadata;
import java.io.IOException;
import java.net.URL;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Calendar;
import java.util.Collection;
import java.util.Date;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import org.apache.commons.lang.StringUtils;
import org.apache.storm.metric.api.MeanReducer;
import org.apache.storm.metric.api.ReducedMetric;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.tuple.Fields;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.tuple.Values;
import org.slf4j.LoggerFactory;

/**
 * Extracts URLs from a sitemap file. The parsing is triggered by sniffing the content and can also
 * be forced by 'isSitemap=true' in the metadata, otherwise the tuple are passed on to the default
 * stream, whereas any URLs extracted from the sitemaps are sent to the 'status' field with a
 * 'DISCOVERED' status.
 */
@SuppressWarnings("serial")
public class SiteMapParserBolt extends StatusEmitterBolt {

    public static final String isSitemapKey = "isSitemap";
    public static final String foundSitemapKey = "foundSitemap";

    private static final org.slf4j.Logger LOG = LoggerFactory.getLogger(SiteMapParserBolt.class);

    private static final byte[] clue = Namespace.SITEMAP.getBytes();

    private SiteMapParser parser;

    private ParseFilter parseFilters;
    private int filterHoursSinceModified = -1;

    private int maxOffsetGuess = 300;

    private ReducedMetric averagedMetrics;

    /** Delay in minutes used for scheduling sub-sitemaps * */
    private int scheduleSitemapsWithDelay = -1;

    private List<Extension> extensionsToParse;

    @Override
    public void execute(Tuple tuple) {
        Metadata metadata = (Metadata) tuple.getValueByField("metadata");
        byte[] content = tuple.getBinaryByField("content");
        String url = tuple.getStringByField("url");

        String ct = metadata.getFirstValue(HttpHeaders.CONTENT_TYPE);

        LOG.debug("Processing {}", url);

        boolean looksLikeSitemap = sniff(content);
        // can force the mimetype as we know it is XML
        if (looksLikeSitemap) {
            ct = "application/xml";
        }

        String isSitemap = metadata.getFirstValue(isSitemapKey);

        boolean treatAsSM = false;

        if ("true".equalsIgnoreCase(isSitemap)) {
            treatAsSM = true;
        }

        // doesn't have the key and want to rely on the clue
        else if (isSitemap == null && looksLikeSitemap) {
            LOG.info("{} detected as sitemap based on content", url);
            treatAsSM = true;
        }

        // decided that it is not a sitemap file
        if (!treatAsSM) {
            LOG.debug("Not a sitemap {}", url);
            // just pass it on
            metadata.setValue(isSitemapKey, "false");
            this.collector.emit(tuple, tuple.getValues());
            this.collector.ack(tuple);
            return;
        }

        List<Outlink> outlinks;
        try {
            outlinks = parseSiteMap(url, content, ct, metadata);
        } catch (Exception e) {
            // exception while parsing the sitemap
            String errorMessage = "Exception while parsing " + url + ": " + e;
            LOG.error(errorMessage);
            // send to status stream in case another component wants to update
            // its status
            metadata.setValue(Constants.STATUS_ERROR_SOURCE, "sitemap parsing");
            metadata.setValue(Constants.STATUS_ERROR_MESSAGE, errorMessage);
            collector.emit(
                    Constants.StatusStreamName, tuple, new Values(url, metadata, Status.ERROR));
            collector.ack(tuple);
            return;
        }

        // mark the current doc as a sitemap
        // as it won't have the k/v if it is a redirected sitemap
        metadata.setValue(isSitemapKey, "true");

        // apply the parse filters if any to the current document
        ParseResult parse = new ParseResult(outlinks);
        parse.set(url, metadata);

        // apply the parse filters if any
        try {
            parseFilters.filter(url, content, null, parse);
        } catch (RuntimeException e) {
            String errorMessage = "Exception while running parse filters on " + url + ": " + e;
            LOG.error(errorMessage);
            metadata.setValue(Constants.STATUS_ERROR_SOURCE, "content filtering");
            metadata.setValue(Constants.STATUS_ERROR_MESSAGE, errorMessage);
            collector.emit(StatusStreamName, tuple, new Values(url, metadata, Status.ERROR));
            collector.ack(tuple);
            return;
        }

        // send to status stream
        for (Outlink ol : parse.getOutlinks()) {
            Values v = new Values(ol.getTargetURL(), ol.getMetadata(), Status.DISCOVERED);
            collector.emit(Constants.StatusStreamName, tuple, v);
        }

        // marking the main URL as successfully fetched
        // regardless of whether we got a parse exception or not
        collector.emit(
                Constants.StatusStreamName, tuple, new Values(url, metadata, Status.FETCHED));
        collector.ack(tuple);
    }

    private List<Outlink> parseSiteMap(
            String url, byte[] content, String contentType, Metadata parentMetadata)
            throws UnknownFormatException, IOException {

        URL sURL = new URL(url);
        long start = System.currentTimeMillis();
        AbstractSiteMap siteMap;
        // let the parser guess what the mimetype is
        if (StringUtils.isBlank(contentType) || contentType.contains("octet-stream")) {
            siteMap = parser.parseSiteMap(content, sURL);
        } else {
            siteMap = parser.parseSiteMap(contentType, content, sURL);
        }
        long end = System.currentTimeMillis();
        averagedMetrics.update(end - start);

        List<Outlink> links = new ArrayList<>();

        if (siteMap.isIndex()) {
            SiteMapIndex smi = (SiteMapIndex) siteMap;
            Collection<AbstractSiteMap> subsitemaps = smi.getSitemaps();

            Calendar rightNow = Calendar.getInstance();
            rightNow.add(Calendar.HOUR, -filterHoursSinceModified);

            int delay = 0;

            // keep the subsitemaps as outlinks
            // they will be fetched and parsed in the following steps
            Iterator<AbstractSiteMap> iter = subsitemaps.iterator();
            while (iter.hasNext()) {
                AbstractSiteMap asm = iter.next();
                String target = asm.getUrl().toExternalForm();

                Date lastModified = asm.getLastModified();
                String lastModifiedValue = "";
                if (lastModified != null) {
                    // filter based on the published date
                    if (filterHoursSinceModified != -1) {
                        if (lastModified.before(rightNow.getTime())) {
                            LOG.info(
                                    "{} has a modified date {} which is more than {} hours old",
                                    target,
                                    lastModified.toString(),
                                    filterHoursSinceModified);
                            continue;
                        }
                    }
                    lastModifiedValue = lastModified.toString();
                }

                Outlink ol =
                        filterOutlink(
                                sURL,
                                target,
                                parentMetadata,
                                isSitemapKey,
                                "true",
                                "sitemap.lastModified",
                                lastModifiedValue);
                if (ol == null) {
                    continue;
                }

                // add a delay
                if (this.scheduleSitemapsWithDelay > 0) {
                    if (delay > 0) {
                        ol.getMetadata()
                                .setValue(DefaultScheduler.DELAY_METADATA, Integer.toString(delay));
                    }
                    delay += this.scheduleSitemapsWithDelay;
                }

                links.add(ol);
                LOG.debug("{} : [sitemap] {}", url, target);
            }
        }
        // sitemap files
        else {
            SiteMap sm = (SiteMap) siteMap;
            // TODO see what we can do with the LastModified info
            Collection<SiteMapURL> sitemapURLs = sm.getSiteMapUrls();
            Iterator<SiteMapURL> iter = sitemapURLs.iterator();
            while (iter.hasNext()) {
                SiteMapURL smurl = iter.next();

                // TODO handle priority in metadata
                double priority = smurl.getPriority();
                // TODO convert the frequency into a numerical value and handle
                // it in metadata
                ChangeFrequency freq = smurl.getChangeFrequency();

                String target = smurl.getUrl().toExternalForm();
                String lastModifiedValue = "";
                Date lastModified = smurl.getLastModified();
                if (lastModified != null) {
                    // filter based on the published date
                    if (filterHoursSinceModified != -1) {
                        Calendar rightNow = Calendar.getInstance();
                        rightNow.add(Calendar.HOUR, -filterHoursSinceModified);
                        if (lastModified.before(rightNow.getTime())) {
                            LOG.info(
                                    "{} has a modified date {} which is more than {} hours old",
                                    target,
                                    lastModified.toString(),
                                    filterHoursSinceModified);
                            continue;
                        }
                    }
                    lastModifiedValue = lastModified.toString();
                }

                Outlink ol =
                        filterOutlink(
                                sURL,
                                target,
                                parentMetadata,
                                isSitemapKey,
                                "false",
                                "sitemap.lastModified",
                                lastModifiedValue);

                if (ol == null) {
                    continue;
                }
                parseExtensionAttributes(smurl, ol.getMetadata());
                links.add(ol);
                LOG.debug("{} : [sitemap] {}", url, target);
            }
        }

        return links;
    }

    public void parseExtensionAttributes(SiteMapURL url, Metadata metadata) {

        for (Extension extension : extensionsToParse) {
            ExtensionMetadata[] extensionMetadata = url.getAttributesForExtension(extension);

            if (extensionMetadata != null) {

                for (ExtensionMetadata extensionMetadatum : extensionMetadata) {

                    for (Map.Entry<String, String[]> entry :
                            extensionMetadatum.asMap().entrySet()) {

                        if (entry.getValue() != null) {
                            metadata.addValues(
                                    extension.name() + "." + entry.getKey(),
                                    Arrays.asList(entry.getValue()));
                        }
                    }
                }
            }
        }
    }

    @Override
    @SuppressWarnings({"rawtypes", "unchecked"})
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
        super.prepare(stormConf, context, collector);
        parser = new SiteMapParser(false);
        filterHoursSinceModified =
                ConfUtils.getInt(stormConf, "sitemap.filter.hours.since.modified", -1);
        parseFilters = ParseFilters.fromConf(stormConf);
        maxOffsetGuess = ConfUtils.getInt(stormConf, "sitemap.offset.guess", 300);
        averagedMetrics =
                context.registerMetric(
                        "sitemap_average_processing_time",
                        new ReducedMetric(new MeanReducer()),
                        30);
        scheduleSitemapsWithDelay =
                ConfUtils.getInt(stormConf, "sitemap.schedule.delay", scheduleSitemapsWithDelay);
        List<String> extensionsStrings =
                ConfUtils.loadListFromConf("sitemap.extensions", stormConf);
        extensionsToParse = new ArrayList<>(extensionsStrings.size());

        for (String type : extensionsStrings) {
            Extension extension = Extension.valueOf(type);
            parser.enableExtension(extension);
            extensionsToParse.add(extension);
        }
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        super.declareOutputFields(declarer);
        declarer.declare(new Fields("url", "content", "metadata"));
    }

    /**
     * Examines the first bytes of the content for a clue of whether this document is a sitemap,
     * based on namespaces. Works for XML and non-compressed documents only.
     */
    private final boolean sniff(byte[] content) {
        byte[] beginning = content;
        if (content.length > maxOffsetGuess && maxOffsetGuess > 0) {
            beginning = Arrays.copyOfRange(content, 0, maxOffsetGuess);
        }
        int position = Bytes.indexOf(beginning, clue);
        if (position != -1) {
            return true;
        }
        return false;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.bolt;

import static com.digitalpebble.stormcrawler.Constants.StatusStreamName;

import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.parse.Outlink;
import com.digitalpebble.stormcrawler.parse.ParseFilter;
import com.digitalpebble.stormcrawler.parse.ParseFilters;
import com.digitalpebble.stormcrawler.parse.ParseResult;
import com.digitalpebble.stormcrawler.persistence.Status;
import com.digitalpebble.stormcrawler.protocol.HttpHeaders;
import com.digitalpebble.stormcrawler.protocol.ProtocolResponse;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import com.google.common.primitives.Bytes;
import com.rometools.rome.feed.synd.SyndContent;
import com.rometools.rome.feed.synd.SyndEntry;
import com.rometools.rome.feed.synd.SyndFeed;
import com.rometools.rome.io.SyndFeedInput;
import java.io.ByteArrayInputStream;
import java.net.URL;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Calendar;
import java.util.Date;
import java.util.List;
import java.util.Map;
import org.apache.commons.lang.StringUtils;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.tuple.Fields;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.tuple.Values;
import org.slf4j.LoggerFactory;
import org.xml.sax.InputSource;

/** Extracts URLs from feeds */
@SuppressWarnings("serial")
public class FeedParserBolt extends StatusEmitterBolt {

    public static final String isFeedKey = "isFeed";

    private static final org.slf4j.Logger LOG = LoggerFactory.getLogger(FeedParserBolt.class);

    private boolean sniffWhenNoMDKey = false;

    private ParseFilter parseFilters;
    private int filterHoursSincePub = -1;

    private String protocolMDprefix;

    @Override
    public void execute(Tuple tuple) {
        Metadata metadata = (Metadata) tuple.getValueByField("metadata");
        byte[] content = tuple.getBinaryByField("content");
        String url = tuple.getStringByField("url");

        LOG.debug("Processing {}", url);

        boolean isfeed = Boolean.valueOf(metadata.getFirstValue(isFeedKey));
        // doesn't have the metadata expected
        if (!isfeed) {
            if (sniffWhenNoMDKey) {
                // uses mime-type
                // won't work when servers return text/xml
                // TODO use Tika instead?
                String ct = metadata.getFirstValue(HttpHeaders.CONTENT_TYPE, protocolMDprefix);
                if (ct != null && ct.contains("rss+xml")) {
                    isfeed = true;
                } else {
                    // try based on the first bytes?
                    byte[] clue = "<rss ".getBytes();
                    byte[] beginning = content;
                    final int maxOffsetGuess = 100;
                    if (content.length > maxOffsetGuess) {
                        beginning = Arrays.copyOfRange(content, 0, maxOffsetGuess);
                    }
                    if (Bytes.indexOf(beginning, clue) != -1) {
                        LOG.info("{} detected as rss feed based on content", url);
                        isfeed = true;
                    }
                }
            }
        }

        // still not a feed file
        if (!isfeed) {
            LOG.debug("Not a feed {}", url);
            // just pass it on
            this.collector.emit(tuple, tuple.getValues());
            this.collector.ack(tuple);
            return;
        } else {
            // can be used later on for custom scheduling
            metadata.setValue(isFeedKey, "true");
        }

        List<Outlink> outlinks;
        try {
            outlinks = parseFeed(url, content, metadata);
        } catch (Exception e) {
            // exception while parsing the feed
            String errorMessage = "Exception while parsing " + url + ": " + e;
            LOG.error(errorMessage, e);
            // send to status stream in case another component wants to update
            // its status
            metadata.setValue(Constants.STATUS_ERROR_SOURCE, "feed parsing");
            metadata.setValue(Constants.STATUS_ERROR_MESSAGE, errorMessage);
            collector.emit(
                    Constants.StatusStreamName, tuple, new Values(url, metadata, Status.ERROR));
            this.collector.ack(tuple);
            return;
        }

        // apply the parse filters if any to the current document
        ParseResult parse = new ParseResult(outlinks);
        parse.set(url, metadata);

        // apply the parse filters if any
        try {
            parseFilters.filter(url, content, null, parse);
        } catch (RuntimeException e) {
            String errorMessage = "Exception while running parse filters on " + url + ": " + e;
            LOG.error(errorMessage, e);
            metadata.setValue(Constants.STATUS_ERROR_SOURCE, "content filtering");
            metadata.setValue(Constants.STATUS_ERROR_MESSAGE, errorMessage);
            collector.emit(StatusStreamName, tuple, new Values(url, metadata, Status.ERROR));
            collector.ack(tuple);
            return;
        }

        // send to status stream
        for (Outlink ol : parse.getOutlinks()) {
            Values v = new Values(ol.getTargetURL(), ol.getMetadata(), Status.DISCOVERED);
            collector.emit(Constants.StatusStreamName, tuple, v);
        }

        LOG.info("Feed parser done {}", url);

        // marking the main URL as successfully fetched
        // regardless of whether we got a parse exception or not
        collector.emit(
                Constants.StatusStreamName, tuple, new Values(url, metadata, Status.FETCHED));
        this.collector.ack(tuple);
    }

    private List<Outlink> parseFeed(String url, byte[] content, Metadata parentMetadata)
            throws Exception {
        List<Outlink> links = new ArrayList<>();

        SyndFeed feed = null;
        try (ByteArrayInputStream is = new ByteArrayInputStream(content)) {
            SyndFeedInput input = new SyndFeedInput();
            feed = input.build(new InputSource(is));
        }

        URL sURL = new URL(url);

        List<SyndEntry> entries = feed.getEntries();
        for (SyndEntry entry : entries) {
            String targetURL = entry.getLink();
            // targetURL can be null?!?
            // e.g. feed does not use links but guid
            if (StringUtils.isBlank(targetURL)) {
                targetURL = entry.getUri();
                if (StringUtils.isBlank(targetURL)) {
                    continue;
                }
            }
            Outlink newLink = filterOutlink(sURL, targetURL, parentMetadata);
            if (newLink == null) continue;

            String title = entry.getTitle();
            if (StringUtils.isNotBlank(title)) {
                newLink.getMetadata().setValue("feed.title", title.trim());
            }

            Date publishedDate = entry.getPublishedDate();
            if (publishedDate != null) {
                // filter based on the published date
                if (filterHoursSincePub != -1) {
                    Calendar rightNow = Calendar.getInstance();
                    rightNow.add(Calendar.HOUR, -filterHoursSincePub);
                    if (publishedDate.before(rightNow.getTime())) {
                        LOG.info(
                                "{} has a published date {} which is more than {} hours old",
                                targetURL,
                                publishedDate.toString(),
                                filterHoursSincePub);
                        continue;
                    }
                }
                newLink.getMetadata().setValue("feed.publishedDate", publishedDate.toString());
            }

            SyndContent description = entry.getDescription();
            if (description != null && StringUtils.isNotBlank(description.getValue())) {
                newLink.getMetadata().setValue("feed.description", description.getValue());
            }

            links.add(newLink);
        }

        return links;
    }

    @Override
    @SuppressWarnings({"rawtypes", "unchecked"})
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collect) {
        super.prepare(stormConf, context, collect);
        sniffWhenNoMDKey = ConfUtils.getBoolean(stormConf, "feed.sniffContent", false);
        filterHoursSincePub = ConfUtils.getInt(stormConf, "feed.filter.hours.since.published", -1);
        parseFilters = ParseFilters.fromConf(stormConf);
        protocolMDprefix =
                ConfUtils.getString(stormConf, ProtocolResponse.PROTOCOL_MD_PREFIX_PARAM, "");
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        super.declareOutputFields(declarer);
        declarer.declare(new Fields("url", "content", "metadata"));
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.bolt;

import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.persistence.Status;
import com.digitalpebble.stormcrawler.protocol.HttpHeaders;
import com.digitalpebble.stormcrawler.protocol.Protocol;
import com.digitalpebble.stormcrawler.protocol.ProtocolFactory;
import com.digitalpebble.stormcrawler.protocol.ProtocolResponse;
import com.digitalpebble.stormcrawler.protocol.RobotRules;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import com.digitalpebble.stormcrawler.util.PerSecondReducer;
import crawlercommons.domains.PaidLevelDomain;
import crawlercommons.robots.BaseRobotRules;
import java.io.File;
import java.net.InetAddress;
import java.net.MalformedURLException;
import java.net.URL;
import java.net.UnknownHostException;
import java.time.Instant;
import java.util.Arrays;
import java.util.Collections;
import java.util.HashMap;
import java.util.Iterator;
import java.util.LinkedHashMap;
import java.util.Locale;
import java.util.Map;
import java.util.Map.Entry;
import java.util.concurrent.BlockingDeque;
import java.util.concurrent.LinkedBlockingDeque;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicLong;
import java.util.regex.Pattern;
import org.apache.commons.lang.StringUtils;
import org.apache.storm.Config;
import org.apache.storm.metric.api.MeanReducer;
import org.apache.storm.metric.api.MultiCountMetric;
import org.apache.storm.metric.api.MultiReducedMetric;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.tuple.Fields;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.tuple.Values;
import org.apache.storm.utils.TupleUtils;
import org.apache.storm.utils.Utils;
import org.slf4j.LoggerFactory;

/**
 * A multithreaded, queue-based fetcher adapted from Apache Nutch. Enforces the politeness and
 * handles the fetching threads itself.
 */
@SuppressWarnings("serial")
public class FetcherBolt extends StatusEmitterBolt {

    private static final org.slf4j.Logger LOG = LoggerFactory.getLogger(FetcherBolt.class);

    private static final String SITEMAP_DISCOVERY_PARAM_KEY = "sitemap.discovery";

    /**
     * Acks URLs which have spent too much time in the queue, should be set to a value equals to the
     * topology timeout
     */
    public static final String QUEUED_TIMEOUT_PARAM_KEY = "fetcher.timeout.queue";

    private final AtomicInteger activeThreads = new AtomicInteger(0);
    private final AtomicInteger spinWaiting = new AtomicInteger(0);

    private FetchItemQueues fetchQueues;

    private MultiCountMetric eventCounter;
    private MultiReducedMetric averagedMetrics;

    private ProtocolFactory protocolFactory;

    private int taskID = -1;

    boolean sitemapsAutoDiscovery = false;

    private MultiReducedMetric perSecMetrics;

    private File debugfiletrigger;

    /** blocks the processing of new URLs if this value is reached * */
    private int maxNumberURLsInQueues = -1;

    private String[] beingFetched;

    @Override
    public Map<String, Object> getComponentConfiguration() {
        Config conf = new Config();
        int tickFrequencyInSeconds = 5;
        conf.put(Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS, tickFrequencyInSeconds);
        return conf;
    }

    /** This class described the item to be fetched. */
    private static class FetchItem {

        String queueID;
        String url;
        Tuple t;
        long creationTime;

        private FetchItem(String url, Tuple t, String queueID) {
            this.url = url;
            this.queueID = queueID;
            this.t = t;
            this.creationTime = System.currentTimeMillis();
        }

        /**
         * Create an item. Queue id will be created based on <code>queueMode</code> argument, either
         * as a protocol + hostname pair, protocol + IP address pair or protocol+domain pair.
         */
        public static FetchItem create(URL u, String url, Tuple t, String queueMode) {

            String queueID;

            String key = null;
            // reuse any key that might have been given
            // be it the hostname, domain or IP
            if (t.contains("key")) {
                key = t.getStringByField("key");
            }
            if (StringUtils.isNotBlank(key)) {
                queueID = key.toLowerCase(Locale.ROOT);
                return new FetchItem(url, t, queueID);
            }

            if (FetchItemQueues.QUEUE_MODE_IP.equalsIgnoreCase(queueMode)) {
                try {
                    final InetAddress addr = InetAddress.getByName(u.getHost());
                    key = addr.getHostAddress();
                } catch (final UnknownHostException e) {
                    LOG.warn("Unable to resolve IP for {}, using hostname as key.", u.getHost());
                    key = u.getHost();
                }
            } else if (FetchItemQueues.QUEUE_MODE_DOMAIN.equalsIgnoreCase(queueMode)) {
                key = PaidLevelDomain.getPLD(u.getHost());
                if (key == null) {
                    LOG.warn("Unknown domain for url: {}, using hostname as key", url);
                    key = u.getHost();
                }
            } else {
                key = u.getHost();
            }

            if (key == null) {
                LOG.warn("Unknown host for url: {}, using URL string as key", url);
                key = u.toExternalForm();
            }

            queueID = key.toLowerCase(Locale.ROOT);
            return new FetchItem(url, t, queueID);
        }
    }

    /**
     * This class handles FetchItems which come from the same host ID (be it a proto/hostname or
     * proto/IP pair). It also keeps track of requests in progress and elapsed time between
     * requests.
     */
    private static class FetchItemQueue {
        final BlockingDeque<FetchItem> queue;

        private final AtomicInteger inProgress = new AtomicInteger();
        private final AtomicLong nextFetchTime = new AtomicLong();

        private final long minCrawlDelay;
        private final int maxThreads;

        long crawlDelay;

        public FetchItemQueue(
                int maxThreads, long crawlDelay, long minCrawlDelay, int maxQueueSize) {
            this.maxThreads = maxThreads;
            this.crawlDelay = crawlDelay;
            this.minCrawlDelay = minCrawlDelay;
            this.queue = new LinkedBlockingDeque<>(maxQueueSize);
            // ready to start
            setNextFetchTime(System.currentTimeMillis(), true);
        }

        public int getQueueSize() {
            return queue.size();
        }

        public int getInProgressSize() {
            return inProgress.get();
        }

        public void finishFetchItem(FetchItem it, boolean asap) {
            if (it != null) {
                inProgress.decrementAndGet();
                setNextFetchTime(System.currentTimeMillis(), asap);
            }
        }

        public boolean addFetchItem(FetchItem it) {
            return queue.offer(it);
        }

        public FetchItem getFetchItem() {
            if (inProgress.get() >= maxThreads) return null;
            if (nextFetchTime.get() > System.currentTimeMillis()) return null;
            FetchItem it = queue.pollFirst();
            if (it != null) {
                inProgress.incrementAndGet();
            }
            return it;
        }

        private void setNextFetchTime(long endTime, boolean asap) {
            if (!asap) nextFetchTime.set(endTime + (maxThreads > 1 ? minCrawlDelay : crawlDelay));
            else nextFetchTime.set(endTime);
        }
    }

    /**
     * Convenience class - a collection of queues that keeps track of the total number of items, and
     * provides items eligible for fetching from any queue.
     */
    private static class FetchItemQueues {
        Map<String, FetchItemQueue> queues =
                Collections.synchronizedMap(new LinkedHashMap<String, FetchItemQueue>());

        AtomicInteger inQueues = new AtomicInteger(0);

        final int defaultMaxThread;
        final long crawlDelay;
        final long minCrawlDelay;

        int maxQueueSize;

        final Config conf;

        public static final String QUEUE_MODE_HOST = "byHost";
        public static final String QUEUE_MODE_DOMAIN = "byDomain";
        public static final String QUEUE_MODE_IP = "byIP";

        String queueMode;

        final Map<Pattern, Integer> customMaxThreads = new HashMap<>();

        public FetchItemQueues(Config conf) {
            this.conf = conf;
            this.defaultMaxThread = ConfUtils.getInt(conf, "fetcher.threads.per.queue", 1);
            queueMode = ConfUtils.getString(conf, "fetcher.queue.mode", QUEUE_MODE_HOST);
            // check that the mode is known
            if (!queueMode.equals(QUEUE_MODE_IP)
                    && !queueMode.equals(QUEUE_MODE_DOMAIN)
                    && !queueMode.equals(QUEUE_MODE_HOST)) {
                LOG.error("Unknown partition mode : {} - forcing to byHost", queueMode);
                queueMode = QUEUE_MODE_HOST;
            }
            LOG.info("Using queue mode : {}", queueMode);

            this.crawlDelay =
                    (long) (ConfUtils.getFloat(conf, "fetcher.server.delay", 1.0f) * 1000);
            this.minCrawlDelay =
                    (long) (ConfUtils.getFloat(conf, "fetcher.server.min.delay", 0.0f) * 1000);
            this.maxQueueSize = ConfUtils.getInt(conf, "fetcher.max.queue.size", -1);
            if (this.maxQueueSize == -1) {
                this.maxQueueSize = Integer.MAX_VALUE;
            }

            // order is not guaranteed
            for (Entry<String, Object> e : conf.entrySet()) {
                String key = e.getKey();
                if (!key.startsWith("fetcher.maxThreads.")) continue;
                Pattern patt = Pattern.compile(key.substring("fetcher.maxThreads.".length()));
                customMaxThreads.put(patt, ((Number) e.getValue()).intValue());
            }
        }

        /** @return true if the URL has been added, false otherwise * */
        public synchronized boolean addFetchItem(URL u, String url, Tuple input) {
            FetchItem it = FetchItem.create(u, url, input, queueMode);
            FetchItemQueue fiq = getFetchItemQueue(it.queueID);
            boolean added = fiq.addFetchItem(it);
            if (added) {
                inQueues.incrementAndGet();
            }

            LOG.debug("{} added to queue {}", url, it.queueID);

            return added;
        }

        public synchronized void finishFetchItem(FetchItem it, boolean asap) {
            FetchItemQueue fiq = queues.get(it.queueID);
            if (fiq == null) {
                LOG.warn("Attempting to finish item from unknown queue: {}", it.queueID);
                return;
            }
            fiq.finishFetchItem(it, asap);
        }

        public synchronized FetchItemQueue getFetchItemQueue(String id) {
            FetchItemQueue fiq = queues.get(id);
            if (fiq == null) {
                int customThreadVal = defaultMaxThread;
                // custom maxThread value?
                for (Entry<Pattern, Integer> p : customMaxThreads.entrySet()) {
                    if (p.getKey().matcher(id).matches()) {
                        customThreadVal = p.getValue().intValue();
                        break;
                    }
                }
                // initialize queue
                fiq = new FetchItemQueue(customThreadVal, crawlDelay, minCrawlDelay, maxQueueSize);
                queues.put(id, fiq);
            }
            return fiq;
        }

        public synchronized FetchItem getFetchItem() {
            if (queues.isEmpty()) {
                return null;
            }

            FetchItemQueue start = null;

            do {
                Iterator<Entry<String, FetchItemQueue>> i = queues.entrySet().iterator();

                if (!i.hasNext()) {
                    return null;
                }

                Map.Entry<String, FetchItemQueue> nextEntry = i.next();

                if (nextEntry == null) {
                    return null;
                }

                FetchItemQueue fiq = nextEntry.getValue();

                // We remove the entry and put it at the end of the map
                i.remove();

                // reap empty queues
                if (fiq.getQueueSize() == 0 && fiq.getInProgressSize() == 0) {
                    continue;
                }

                // Put the entry at the end no matter the result
                queues.put(nextEntry.getKey(), nextEntry.getValue());

                // In case of we are looping
                if (start == null) {
                    start = fiq;
                } else if (fiq == start) {
                    return null;
                }

                FetchItem fit = fiq.getFetchItem();

                if (fit != null) {
                    inQueues.decrementAndGet();
                    return fit;
                }

            } while (!queues.isEmpty());

            return null;
        }
    }

    /** This class picks items from queues and fetches the pages. */
    private class FetcherThread extends Thread {

        // max. delay accepted from robots.txt
        private final long maxCrawlDelay;
        // whether maxCrawlDelay overwrites the longer value in robots.txt
        // (otherwise URLs in this queue are skipped)
        private final boolean maxCrawlDelayForce;
        // whether the default delay is used even if the robots.txt
        // specifies a shorter crawl-delay
        private final boolean crawlDelayForce;
        private int threadNum;

        private long timeoutInQueues = -1;

        // by default remains as is-pre 1.17
        private String protocolMDprefix = "";

        public FetcherThread(Config conf, int num) {
            this.setDaemon(true); // don't hang JVM on exit
            this.setName("FetcherThread #" + num); // use an informative name

            this.maxCrawlDelay = ConfUtils.getInt(conf, "fetcher.max.crawl.delay", 30) * 1000;
            this.maxCrawlDelayForce =
                    ConfUtils.getBoolean(conf, "fetcher.max.crawl.delay.force", false);
            this.crawlDelayForce = ConfUtils.getBoolean(conf, "fetcher.server.delay.force", false);
            this.threadNum = num;
            timeoutInQueues = ConfUtils.getLong(conf, QUEUED_TIMEOUT_PARAM_KEY, timeoutInQueues);
            protocolMDprefix =
                    ConfUtils.getString(
                            conf, ProtocolResponse.PROTOCOL_MD_PREFIX_PARAM, protocolMDprefix);
        }

        @Override
        public void run() {
            while (true) {
                FetchItem fit = fetchQueues.getFetchItem();
                if (fit == null) {
                    LOG.trace("{} spin-waiting ...", getName());
                    // spin-wait.
                    spinWaiting.incrementAndGet();
                    try {
                        Thread.sleep(100);
                    } catch (InterruptedException e) {
                        LOG.error("{} caught interrupted exception", getName());
                        Thread.currentThread().interrupt();
                    }
                    spinWaiting.decrementAndGet();
                    continue;
                }

                activeThreads.incrementAndGet(); // count threads

                beingFetched[threadNum] = fit.url;

                LOG.debug(
                        "[Fetcher #{}] {}  => activeThreads={}, spinWaiting={}, queueID={}",
                        taskID,
                        getName(),
                        activeThreads,
                        spinWaiting,
                        fit.queueID);

                LOG.debug("[Fetcher #{}] {} : Fetching {}", taskID, getName(), fit.url);

                Metadata metadata = null;

                if (fit.t.contains("metadata")) {
                    metadata = (Metadata) fit.t.getValueByField("metadata");
                }
                if (metadata == null) {
                    metadata = Metadata.empty;
                }

                // https://github.com/DigitalPebble/storm-crawler/issues/813
                metadata.remove("fetch.exception");

                boolean asap = false;

                try {
                    URL url = new URL(fit.url);
                    Protocol protocol = protocolFactory.getProtocol(url);

                    if (protocol == null)
                        throw new RuntimeException(
                                "No protocol implementation found for " + fit.url);

                    BaseRobotRules rules = protocol.getRobotRules(fit.url);
                    boolean fromCache = false;
                    if (rules instanceof RobotRules
                            && ((RobotRules) rules).getContentLengthFetched().length == 0) {
                        fromCache = true;
                        eventCounter.scope("robots.fromCache").incrBy(1);
                    } else {
                        eventCounter.scope("robots.fetched").incrBy(1);
                    }

                    // autodiscovery of sitemaps
                    // the sitemaps will be sent down the topology
                    // if the robot file did not come from the cache
                    // to avoid sending them unecessarily

                    // check in the metadata if discovery setting has been
                    // overridden
                    boolean smautodisco = sitemapsAutoDiscovery;
                    String localSitemapDiscoveryVal =
                            metadata.getFirstValue(SITEMAP_DISCOVERY_PARAM_KEY);
                    if ("true".equalsIgnoreCase(localSitemapDiscoveryVal)) {
                        smautodisco = true;
                    } else if ("false".equalsIgnoreCase(localSitemapDiscoveryVal)) {
                        smautodisco = false;
                    }

                    if (!fromCache && smautodisco) {
                        for (String sitemapURL : rules.getSitemaps()) {
                            if (rules.isAllowed(sitemapURL)) {
                                emitOutlink(
                                        fit.t,
                                        url,
                                        sitemapURL,
                                        metadata,
                                        SiteMapParserBolt.isSitemapKey,
                                        "true");
                            }
                        }
                    }

                    // has found sitemaps
                    // https://github.com/DigitalPebble/storm-crawler/issues/710
                    // note: we don't care if the sitemap URLs where actually
                    // kept
                    boolean foundSitemap = (rules.getSitemaps().size() > 0);
                    metadata.setValue(
                            SiteMapParserBolt.foundSitemapKey, Boolean.toString(foundSitemap));

                    if (!rules.isAllowed(fit.url)) {
                        LOG.info("Denied by robots.txt: {}", fit.url);
                        // pass the info about denied by robots
                        metadata.setValue(Constants.STATUS_ERROR_CAUSE, "robots.txt");
                        collector.emit(
                                com.digitalpebble.stormcrawler.Constants.StatusStreamName,
                                fit.t,
                                new Values(fit.url, metadata, Status.ERROR));
                        // no need to wait next time as we won't request from
                        // that site
                        asap = true;
                        continue;
                    }
                    FetchItemQueue fiq = fetchQueues.getFetchItemQueue(fit.queueID);
                    if (rules.getCrawlDelay() > 0 && rules.getCrawlDelay() != fiq.crawlDelay) {
                        if (rules.getCrawlDelay() > maxCrawlDelay && maxCrawlDelay >= 0) {
                            boolean force = false;
                            String msg = "skipping";
                            if (maxCrawlDelayForce) {
                                force = true;
                                msg = "using value of fetcher.max.crawl.delay instead";
                            }
                            LOG.info(
                                    "Crawl-Delay for {} too long ({}), {}",
                                    fit.url,
                                    rules.getCrawlDelay(),
                                    msg);
                            if (force) {
                                fiq.crawlDelay = maxCrawlDelay;
                            } else {
                                // pass the info about crawl delay
                                metadata.setValue(Constants.STATUS_ERROR_CAUSE, "crawl_delay");
                                collector.emit(
                                        com.digitalpebble.stormcrawler.Constants.StatusStreamName,
                                        fit.t,
                                        new Values(fit.url, metadata, Status.ERROR));
                                // no need to wait next time as we won't request
                                // from that site
                                asap = true;
                                continue;
                            }
                        } else if (rules.getCrawlDelay() < fetchQueues.crawlDelay
                                && crawlDelayForce) {
                            fiq.crawlDelay = fetchQueues.crawlDelay;
                            LOG.info(
                                    "Crawl delay for {} too short ({}), set to fetcher.server.delay",
                                    fit.url,
                                    rules.getCrawlDelay());
                        } else {
                            fiq.crawlDelay = rules.getCrawlDelay();
                            LOG.info(
                                    "Crawl delay for queue: {}  is set to {} as per robots.txt. url: {}",
                                    fit.queueID,
                                    fiq.crawlDelay,
                                    fit.url);
                        }
                    }

                    long start = System.currentTimeMillis();
                    long timeInQueues = start - fit.creationTime;

                    // been in the queue far too long and already failed
                    // by the timeout - let's not fetch it
                    if (timeoutInQueues != -1 && timeInQueues > timeoutInQueues * 1000) {
                        LOG.info(
                                "[Fetcher #{}] Waited in queue for too long - {}", taskID, fit.url);
                        // no need to wait next time as we won't request from
                        // that site
                        asap = true;
                        continue;
                    }

                    ProtocolResponse response = protocol.getProtocolOutput(fit.url, metadata);

                    long timeFetching = System.currentTimeMillis() - start;

                    final int byteLength = response.getContent().length;

                    // get any metrics from the protocol metadata
                    // expect Longs
                    response.getMetadata().keySet().stream()
                            .filter(s -> s.startsWith("metrics."))
                            .forEach(
                                    s ->
                                            averagedMetrics
                                                    .scope(s.substring(8))
                                                    .update(
                                                            Long.parseLong(
                                                                    response.getMetadata()
                                                                            .getFirstValue(s))));

                    averagedMetrics.scope("fetch_time").update(timeFetching);
                    averagedMetrics.scope("time_in_queues").update(timeInQueues);
                    averagedMetrics.scope("bytes_fetched").update(byteLength);
                    perSecMetrics.scope("bytes_fetched_perSec").update(byteLength);
                    perSecMetrics.scope("fetched_perSec").update(1);
                    eventCounter.scope("fetched").incrBy(1);
                    eventCounter.scope("bytes_fetched").incrBy(byteLength);

                    LOG.info(
                            "[Fetcher #{}] Fetched {} with status {} in msec {}",
                            taskID,
                            fit.url,
                            response.getStatusCode(),
                            timeFetching);

                    // merges the original MD and the ones returned by the
                    // protocol
                    Metadata mergedMD = new Metadata();
                    mergedMD.putAll(metadata);

                    // add a prefix to avoid confusion, preserve protocol
                    // metadata persisted or transferred from previous fetches
                    mergedMD.putAll(response.getMetadata(), protocolMDprefix);

                    mergedMD.setValue(
                            "fetch.statusCode", Integer.toString(response.getStatusCode()));

                    mergedMD.setValue("fetch.byteLength", Integer.toString(byteLength));

                    mergedMD.setValue("fetch.loadingTime", Long.toString(timeFetching));

                    mergedMD.setValue("fetch.timeInQueues", Long.toString(timeInQueues));

                    // determine the status based on the status code
                    final Status status = Status.fromHTTPCode(response.getStatusCode());

                    eventCounter.scope("status_" + response.getStatusCode()).incrBy(1);

                    final Values tupleToSend = new Values(fit.url, mergedMD, status);

                    // if the status is OK emit on default stream
                    if (status.equals(Status.FETCHED)) {
                        if (response.getStatusCode() == 304) {
                            // mark this URL as fetched so that it gets
                            // rescheduled
                            // but do not try to parse or index
                            collector.emit(Constants.StatusStreamName, fit.t, tupleToSend);
                        } else {
                            // send content for parsing
                            collector.emit(
                                    Utils.DEFAULT_STREAM_ID,
                                    fit.t,
                                    new Values(fit.url, response.getContent(), mergedMD));
                        }
                    } else if (status.equals(Status.REDIRECTION)) {

                        // find the URL it redirects to
                        String redirection =
                                response.getMetadata().getFirstValue(HttpHeaders.LOCATION);

                        // stores the URL it redirects to
                        // used for debugging mainly - do not resolve the target
                        // URL
                        if (StringUtils.isNotBlank(redirection)) {
                            mergedMD.setValue("_redirTo", redirection);
                        }

                        // mark this URL as redirected
                        collector.emit(Constants.StatusStreamName, fit.t, tupleToSend);

                        if (allowRedirs() && StringUtils.isNotBlank(redirection)) {
                            emitOutlink(fit.t, url, redirection, mergedMD);
                        }
                    }
                    // error
                    else {
                        collector.emit(Constants.StatusStreamName, fit.t, tupleToSend);
                    }

                } catch (Exception exece) {
                    String message = exece.getMessage();
                    if (message == null) message = "";

                    // common exceptions for which we log only a short message
                    if (exece.getCause() instanceof java.util.concurrent.TimeoutException
                            || message.contains(" timed out")) {
                        LOG.info("Socket timeout fetching {}", fit.url);
                        message = "Socket timeout fetching";
                    } else if (exece.getCause() instanceof java.net.UnknownHostException
                            || exece instanceof java.net.UnknownHostException) {
                        LOG.info("Unknown host {}", fit.url);
                        message = "Unknown host";
                    } else {
                        message = exece.getClass().getName();
                        if (LOG.isDebugEnabled()) {
                            LOG.debug("Exception while fetching {}", fit.url, exece);
                        } else {
                            LOG.info("Exception while fetching {} -> {}", fit.url, message);
                        }
                    }

                    if (metadata.size() == 0) {
                        metadata = new Metadata();
                    }
                    // add the reason of the failure in the metadata
                    metadata.setValue("fetch.exception", message);

                    // send to status stream
                    collector.emit(
                            Constants.StatusStreamName,
                            fit.t,
                            new Values(fit.url, metadata, Status.FETCH_ERROR));

                    eventCounter.scope("exception").incrBy(1);
                } finally {
                    fetchQueues.finishFetchItem(fit, asap);
                    activeThreads.decrementAndGet(); // count threads
                    // ack it whatever happens
                    collector.ack(fit.t);
                    beingFetched[threadNum] = "";
                }
            }
        }
    }

    private void checkConfiguration(Config stormConf) {

        // ensure that a value has been set for the agent name and that that
        // agent name is the first value in the agents we advertise for robot
        // rules parsing
        String agentName = (String) stormConf.get("http.agent.name");
        if (agentName == null || agentName.trim().length() == 0) {
            String message = "Fetcher: No agents listed in 'http.agent.name'" + " property.";
            LOG.error(message);
            throw new IllegalArgumentException(message);
        }
    }

    @SuppressWarnings({"rawtypes", "unchecked"})
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {

        super.prepare(stormConf, context, collector);

        Config conf = new Config();
        conf.putAll(stormConf);

        checkConfiguration(conf);

        LOG.info("[Fetcher #{}] : starting at {}", taskID, Instant.now());

        int metricsTimeBucketSecs = ConfUtils.getInt(conf, "fetcher.metrics.time.bucket.secs", 10);

        // Register a "MultiCountMetric" to count different events in this bolt
        // Storm will emit the counts every n seconds to a special bolt via a
        // system stream
        // The data can be accessed by registering a "MetricConsumer" in the
        // topology
        this.eventCounter =
                context.registerMetric(
                        "fetcher_counter", new MultiCountMetric(), metricsTimeBucketSecs);

        // create gauges
        context.registerMetric(
                "activethreads",
                () -> {
                    return activeThreads.get();
                },
                metricsTimeBucketSecs);

        context.registerMetric(
                "in_queues",
                () -> {
                    return fetchQueues.inQueues.get();
                },
                metricsTimeBucketSecs);

        context.registerMetric(
                "num_queues",
                () -> {
                    return fetchQueues.queues.size();
                },
                metricsTimeBucketSecs);

        this.averagedMetrics =
                context.registerMetric(
                        "fetcher_average_perdoc",
                        new MultiReducedMetric(new MeanReducer()),
                        metricsTimeBucketSecs);

        this.perSecMetrics =
                context.registerMetric(
                        "fetcher_average_persec",
                        new MultiReducedMetric(new PerSecondReducer()),
                        metricsTimeBucketSecs);

        protocolFactory = ProtocolFactory.getInstance(conf);

        this.fetchQueues = new FetchItemQueues(conf);

        this.taskID = context.getThisTaskId();

        int threadCount = ConfUtils.getInt(conf, "fetcher.threads.number", 10);
        for (int i = 0; i < threadCount; i++) { // spawn threads
            new FetcherThread(conf, i).start();
        }

        // keep track of the URLs in fetching
        beingFetched = new String[threadCount];
        Arrays.fill(beingFetched, "");

        sitemapsAutoDiscovery = ConfUtils.getBoolean(stormConf, SITEMAP_DISCOVERY_PARAM_KEY, false);

        maxNumberURLsInQueues = ConfUtils.getInt(conf, "fetcher.max.urls.in.queues", -1);

        /**
         * If set to a valid path e.g. /tmp/fetcher-dump-{port} on a worker node, the content of the
         * queues will be dumped to the logs for debugging. The port number needs to match the one
         * used by the FetcherBolt instance.
         */
        String debugfiletriggerpattern =
                ConfUtils.getString(conf, "fetcherbolt.queue.debug.filepath");

        if (StringUtils.isNotBlank(debugfiletriggerpattern)) {
            debugfiletrigger =
                    new File(
                            debugfiletriggerpattern.replaceAll(
                                    "\\{port\\}", Integer.toString(context.getThisWorkerPort())));
        }
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        super.declareOutputFields(declarer);
        declarer.declare(new Fields("url", "content", "metadata"));
    }

    @Override
    public void cleanup() {
        protocolFactory.cleanup();
    }

    @Override
    public void execute(Tuple input) {

        if (TupleUtils.isTick(input)) {
            // detect whether there is a file indicating that we should
            // dump the content of the queues to the log
            if (debugfiletrigger != null && debugfiletrigger.exists()) {
                LOG.info("Found trigger file {}", debugfiletrigger);
                logQueuesContent();
                debugfiletrigger.delete();
            }
            return;
        }

        if (this.maxNumberURLsInQueues != -1) {
            while (this.activeThreads.get() + this.fetchQueues.inQueues.get()
                    >= maxNumberURLsInQueues) {
                try {
                    Thread.sleep(500);
                } catch (InterruptedException e) {
                    LOG.error("Interrupted exception caught in execute method");
                    Thread.currentThread().interrupt();
                }
                LOG.debug(
                        "[Fetcher #{}] Threads : {}\tqueues : {}\tin_queues : {}",
                        taskID,
                        this.activeThreads.get(),
                        this.fetchQueues.queues.size(),
                        this.fetchQueues.inQueues.get());
            }
        }

        final String urlString = input.getStringByField("url");
        if (StringUtils.isBlank(urlString)) {
            LOG.info("[Fetcher #{}] Missing value for field url in tuple {}", taskID, input);
            // ignore silently
            collector.ack(input);
            return;
        }

        LOG.debug("Received in Fetcher {}", urlString);

        URL url;

        try {
            url = new URL(urlString);
        } catch (MalformedURLException e) {
            LOG.error("{} is a malformed URL", urlString);

            Metadata metadata = (Metadata) input.getValueByField("metadata");
            if (metadata == null) {
                metadata = new Metadata();
            }
            // Report to status stream and ack
            metadata.setValue(Constants.STATUS_ERROR_CAUSE, "malformed URL");
            collector.emit(
                    com.digitalpebble.stormcrawler.Constants.StatusStreamName,
                    input,
                    new Values(urlString, metadata, Status.ERROR));
            collector.ack(input);
            return;
        }

        boolean added = fetchQueues.addFetchItem(url, urlString, input);
        if (!added) {
            collector.fail(input);
        }
    }

    private void logQueuesContent() {
        StringBuilder sb = new StringBuilder();
        synchronized (fetchQueues.queues) {
            sb.append("\nNum queues : ").append(fetchQueues.queues.size());
            Iterator<Entry<String, FetchItemQueue>> iterator =
                    fetchQueues.queues.entrySet().iterator();
            while (iterator.hasNext()) {
                Entry<String, FetchItemQueue> entry = iterator.next();
                sb.append("\nQueue ID : ").append(entry.getKey());
                FetchItemQueue fiq = entry.getValue();
                sb.append("\t size : ").append(fiq.getQueueSize());
                sb.append("\t in progress : ").append(fiq.getInProgressSize());
                Iterator<FetchItem> urlsIter = fiq.queue.iterator();
                while (urlsIter.hasNext()) {
                    sb.append("\n\t").append(urlsIter.next().url);
                }
            }
            LOG.info("Dumping queue content {}", sb.toString());

            StringBuilder sb2 = new StringBuilder("\n");
            // dump the list of URLs being fetched
            for (int i = 0; i < beingFetched.length; i++) {
                if (beingFetched[i].length() > 0) {
                    sb2.append("\n\tThread #").append(i).append(": ").append(beingFetched[i]);
                }
            }
            LOG.info("URLs being fetched {}", sb2.toString());
        }
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.bolt;

import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.persistence.Status;
import com.digitalpebble.stormcrawler.protocol.HttpHeaders;
import com.digitalpebble.stormcrawler.protocol.Protocol;
import com.digitalpebble.stormcrawler.protocol.ProtocolFactory;
import com.digitalpebble.stormcrawler.protocol.ProtocolResponse;
import com.digitalpebble.stormcrawler.protocol.RobotRules;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import com.digitalpebble.stormcrawler.util.PerSecondReducer;
import com.github.benmanes.caffeine.cache.Cache;
import com.github.benmanes.caffeine.cache.Caffeine;
import crawlercommons.domains.PaidLevelDomain;
import crawlercommons.robots.BaseRobotRules;
import java.net.InetAddress;
import java.net.MalformedURLException;
import java.net.URL;
import java.net.UnknownHostException;
import java.text.SimpleDateFormat;
import java.util.Locale;
import java.util.Map;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicInteger;
import org.apache.commons.lang.StringUtils;
import org.apache.storm.Config;
import org.apache.storm.metric.api.IMetric;
import org.apache.storm.metric.api.MeanReducer;
import org.apache.storm.metric.api.MultiCountMetric;
import org.apache.storm.metric.api.MultiReducedMetric;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.tuple.Fields;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.tuple.Values;
import org.apache.storm.utils.Utils;
import org.slf4j.LoggerFactory;

/**
 * A simple fetcher with no internal queues. This bolt either enforces the delay set by the
 * configuration or robots.txt by either sleeping or resending the tuple to itself on the
 * THROTTLE_STREAM using Direct grouping.
 *
 * <pre>
 * .directGrouping("fetch", "throttle")
 * </pre>
 */
@SuppressWarnings("serial")
public class SimpleFetcherBolt extends StatusEmitterBolt {

    private static final org.slf4j.Logger LOG = LoggerFactory.getLogger(SimpleFetcherBolt.class);

    private static final String SITEMAP_DISCOVERY_PARAM_KEY = "sitemap.discovery";

    public static final String QUEUE_MODE_HOST = "byHost";
    public static final String QUEUE_MODE_DOMAIN = "byDomain";
    public static final String QUEUE_MODE_IP = "byIP";

    public static final String THROTTLE_STREAM = "throttle";

    private Config conf;

    private MultiCountMetric eventCounter;
    private MultiReducedMetric averagedMetrics;
    private MultiReducedMetric perSecMetrics;

    private ProtocolFactory protocolFactory;

    private int taskID = -1;

    boolean sitemapsAutoDiscovery = false;

    // TODO configure the max time
    private Cache<String, Long> throttler =
            Caffeine.newBuilder().expireAfterAccess(30, TimeUnit.SECONDS).build();

    private String queueMode;

    /** default crawl delay in msec, can be overridden by robots directives * */
    private long crawlDelay = 1000;

    /** max value accepted from robots.txt * */
    private long maxCrawlDelay = 30000;

    /**
     * whether to enforce the configured max. delay, or to skip URLs from queues with overlong
     * crawl-delay
     */
    private boolean maxCrawlDelayForce = true;

    /** whether the default delay is used even if the robots.txt specifies a shorter crawl-delay */
    private boolean crawlDelayForce = false;

    private final AtomicInteger activeThreads = new AtomicInteger(0);

    /**
     * Amount of time the bolt will sleep to enfore politeness, if the time needed to wait is above
     * it, the tuple is sent back to the Storm internal queue. Deactivate by default i.e. nothing is
     * sent back to the bolt via the throttle stream.
     */
    private long maxThrottleSleepMSec = Long.MAX_VALUE;

    // by default remains as is-pre 1.17
    private String protocolMDprefix = "";

    private void checkConfiguration() {

        // ensure that a value has been set for the agent name and that that
        // agent name is the first value in the agents we advertise for robot
        // rules parsing
        String agentName = (String) getConf().get("http.agent.name");
        if (agentName == null || agentName.trim().length() == 0) {
            String message = "Fetcher: No agents listed in 'http.agent.name'" + " property.";
            LOG.error(message);
            throw new IllegalArgumentException(message);
        }
    }

    private Config getConf() {
        return this.conf;
    }

    @SuppressWarnings({"rawtypes", "unchecked"})
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
        super.prepare(stormConf, context, collector);
        this.conf = new Config();
        this.conf.putAll(stormConf);

        checkConfiguration();

        this.taskID = context.getThisTaskId();

        SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss", Locale.ENGLISH);
        long start = System.currentTimeMillis();
        LOG.info("[Fetcher #{}] : starting at {}", taskID, sdf.format(start));

        // Register a "MultiCountMetric" to count different events in this bolt
        // Storm will emit the counts every n seconds to a special bolt via a
        // system stream
        // The data can be accessed by registering a "MetricConsumer" in the
        // topology

        int metricsTimeBucketSecs = ConfUtils.getInt(conf, "fetcher.metrics.time.bucket.secs", 10);

        this.eventCounter =
                context.registerMetric(
                        "fetcher_counter", new MultiCountMetric(), metricsTimeBucketSecs);

        this.averagedMetrics =
                context.registerMetric(
                        "fetcher_average",
                        new MultiReducedMetric(new MeanReducer()),
                        metricsTimeBucketSecs);

        this.perSecMetrics =
                context.registerMetric(
                        "fetcher_average_persec",
                        new MultiReducedMetric(new PerSecondReducer()),
                        metricsTimeBucketSecs);

        // create gauges
        context.registerMetric(
                "activethreads",
                new IMetric() {
                    @Override
                    public Object getValueAndReset() {
                        return activeThreads.get();
                    }
                },
                metricsTimeBucketSecs);

        context.registerMetric(
                "throttler_size",
                new IMetric() {
                    @Override
                    public Object getValueAndReset() {
                        return throttler.estimatedSize();
                    }
                },
                metricsTimeBucketSecs);

        protocolFactory = ProtocolFactory.getInstance(conf);

        sitemapsAutoDiscovery = ConfUtils.getBoolean(stormConf, SITEMAP_DISCOVERY_PARAM_KEY, false);

        queueMode = ConfUtils.getString(conf, "fetcher.queue.mode", QUEUE_MODE_HOST);
        // check that the mode is known
        if (!queueMode.equals(QUEUE_MODE_IP)
                && !queueMode.equals(QUEUE_MODE_DOMAIN)
                && !queueMode.equals(QUEUE_MODE_HOST)) {
            LOG.error("Unknown partition mode : {} - forcing to byHost", queueMode);
            queueMode = QUEUE_MODE_HOST;
        }
        LOG.info("Using queue mode : {}", queueMode);

        this.crawlDelay = (long) (ConfUtils.getFloat(conf, "fetcher.server.delay", 1.0f) * 1000);

        this.maxCrawlDelay = (long) ConfUtils.getInt(conf, "fetcher.max.crawl.delay", 30) * 1000;

        this.maxCrawlDelayForce =
                ConfUtils.getBoolean(conf, "fetcher.max.crawl.delay.force", false);
        this.crawlDelayForce = ConfUtils.getBoolean(conf, "fetcher.server.delay.force", false);

        this.maxThrottleSleepMSec = ConfUtils.getLong(conf, "fetcher.max.throttle.sleep", -1);

        this.protocolMDprefix =
                ConfUtils.getString(
                        conf, ProtocolResponse.PROTOCOL_MD_PREFIX_PARAM, protocolMDprefix);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        super.declareOutputFields(declarer);
        declarer.declare(new Fields("url", "content", "metadata"));
        declarer.declareStream(THROTTLE_STREAM, true, new Fields("url", "metadata"));
    }

    @Override
    public void cleanup() {
        protocolFactory.cleanup();
    }

    @Override
    public void execute(Tuple input) {

        String urlString = input.getStringByField("url");
        if (StringUtils.isBlank(urlString)) {
            LOG.info("[Fetcher #{}] Missing value for field url in tuple {}", taskID, input);
            // ignore silently
            collector.ack(input);
            return;
        }

        Metadata metadata = null;

        if (input.contains("metadata")) metadata = (Metadata) input.getValueByField("metadata");
        if (metadata == null) metadata = Metadata.empty;

        // https://github.com/DigitalPebble/storm-crawler/issues/813
        metadata.remove("fetch.exception");

        URL url;

        try {
            url = new URL(urlString);
        } catch (MalformedURLException e) {
            LOG.error("{} is a malformed URL", urlString);
            // Report to status stream and ack
            if (metadata == Metadata.empty) {
                metadata = new Metadata();
            }
            metadata.setValue(Constants.STATUS_ERROR_CAUSE, "malformed URL");
            collector.emit(
                    com.digitalpebble.stormcrawler.Constants.StatusStreamName,
                    input,
                    new Values(urlString, metadata, Status.ERROR));
            collector.ack(input);
            return;
        }

        String key = getPolitenessKey(url);
        long delay = 0;

        try {
            activeThreads.incrementAndGet();

            Protocol protocol = protocolFactory.getProtocol(url);

            BaseRobotRules rules = protocol.getRobotRules(urlString);
            boolean fromCache = false;
            if (rules instanceof RobotRules
                    && ((RobotRules) rules).getContentLengthFetched().length == 0) {
                fromCache = true;
                eventCounter.scope("robots.fromCache").incrBy(1);
            } else {
                eventCounter.scope("robots.fetched").incrBy(1);
            }

            // autodiscovery of sitemaps
            // the sitemaps will be sent down the topology
            // if the robot file did not come from the cache
            // to avoid sending them unecessarily

            // check in the metadata if discovery setting has been
            // overridden
            boolean smautodisco = sitemapsAutoDiscovery;
            String localSitemapDiscoveryVal = metadata.getFirstValue(SITEMAP_DISCOVERY_PARAM_KEY);
            if ("true".equalsIgnoreCase(localSitemapDiscoveryVal)) {
                smautodisco = true;
            } else if ("false".equalsIgnoreCase(localSitemapDiscoveryVal)) {
                smautodisco = false;
            }

            if (!fromCache && smautodisco) {
                for (String sitemapURL : rules.getSitemaps()) {
                    if (rules.isAllowed(sitemapURL)) {
                        emitOutlink(
                                input,
                                url,
                                sitemapURL,
                                metadata,
                                SiteMapParserBolt.isSitemapKey,
                                "true");
                    }
                }
            }

            // has found sitemaps
            // https://github.com/DigitalPebble/storm-crawler/issues/710
            // note: we don't care if the sitemap URLs where actually
            // kept
            boolean foundSitemap = (rules.getSitemaps().size() > 0);
            metadata.setValue(SiteMapParserBolt.foundSitemapKey, Boolean.toString(foundSitemap));

            activeThreads.decrementAndGet();

            if (!rules.isAllowed(urlString)) {
                LOG.info("Denied by robots.txt: {}", urlString);

                metadata.setValue(Constants.STATUS_ERROR_CAUSE, "robots.txt");

                // Report to status stream and ack
                collector.emit(
                        com.digitalpebble.stormcrawler.Constants.StatusStreamName,
                        input,
                        new Values(urlString, metadata, Status.ERROR));
                collector.ack(input);
                return;
            }

            // check when we are allowed to process it
            long timeWaiting = 0;

            Long timeAllowed = throttler.getIfPresent(key);

            if (timeAllowed != null) {
                long now = System.currentTimeMillis();
                long timeToWait = timeAllowed - now;
                if (timeToWait > 0) {
                    // too long -> send it to the back of the internal queue
                    if (maxThrottleSleepMSec != -1 && timeToWait > maxThrottleSleepMSec) {
                        collector.emitDirect(
                                this.taskID,
                                THROTTLE_STREAM,
                                input,
                                new Values(urlString, metadata));
                        collector.ack(input);
                        LOG.debug("[Fetcher #{}] sent back to the queue {}", taskID, urlString);
                        eventCounter.scope("sentBackToQueue").incrBy(1);
                        return;
                    }
                    // not too much of a wait - sleep here
                    timeWaiting = timeToWait;
                    try {
                        Thread.sleep(timeToWait);
                    } catch (InterruptedException e) {
                        LOG.error("[Fetcher #{}] caught InterruptedException caught while waiting");
                        Thread.currentThread().interrupt();
                    }
                }
            }

            delay = this.crawlDelay;

            // get the delay from robots
            // value is negative when not set
            long robotsDelay = rules.getCrawlDelay();
            if (robotsDelay > 0) {
                if (robotsDelay > maxCrawlDelay) {
                    if (maxCrawlDelayForce) {
                        // cap the value to a maximum
                        // as some sites specify ridiculous values
                        LOG.debug("Delay from robots capped at {} for {}", robotsDelay, url);
                        delay = maxCrawlDelay;
                    } else {
                        LOG.debug(
                                "Skipped URL from queue with overlong crawl-delay ({}): {}",
                                robotsDelay,
                                url);
                        metadata.setValue(Constants.STATUS_ERROR_CAUSE, "crawl_delay");
                        collector.emit(
                                com.digitalpebble.stormcrawler.Constants.StatusStreamName,
                                input,
                                new Values(urlString, metadata, Status.ERROR));
                        collector.ack(input);
                        return;
                    }
                } else if (robotsDelay < crawlDelay && crawlDelayForce) {
                    LOG.debug(
                            "Crawl delay for {} too short ({}), set to fetcher.server.delay",
                            url,
                            robotsDelay);
                    delay = crawlDelay;
                } else {
                    delay = robotsDelay;
                }
            }

            LOG.debug("[Fetcher #{}] : Fetching {}", taskID, urlString);

            activeThreads.incrementAndGet();

            long start = System.currentTimeMillis();
            ProtocolResponse response = protocol.getProtocolOutput(urlString, metadata);
            long timeFetching = System.currentTimeMillis() - start;

            final int byteLength = response.getContent().length;

            // get any metrics from the protocol metadata
            response.getMetadata().keySet().stream()
                    .filter(s -> s.startsWith("metrics."))
                    .forEach(
                            s ->
                                    averagedMetrics
                                            .scope(s.substring(8))
                                            .update(
                                                    Long.parseLong(
                                                            response.getMetadata()
                                                                    .getFirstValue(s))));

            averagedMetrics.scope("wait_time").update(timeWaiting);
            averagedMetrics.scope("fetch_time").update(timeFetching);
            averagedMetrics.scope("bytes_fetched").update(byteLength);
            eventCounter.scope("fetched").incrBy(1);
            eventCounter.scope("bytes_fetched").incrBy(byteLength);
            perSecMetrics.scope("bytes_fetched_perSec").update(byteLength);
            perSecMetrics.scope("fetched_perSec").update(1);

            LOG.info(
                    "[Fetcher #{}] Fetched {} with status {} in {} after waiting {}",
                    taskID,
                    urlString,
                    response.getStatusCode(),
                    timeFetching,
                    timeWaiting);

            Metadata mergedMD = new Metadata();
            mergedMD.putAll(metadata);

            // add a prefix to avoid confusion, preserve protocol metadata
            // persisted or transferred from previous fetches
            mergedMD.putAll(response.getMetadata(), protocolMDprefix);

            mergedMD.setValue("fetch.statusCode", Integer.toString(response.getStatusCode()));

            mergedMD.setValue("fetch.loadingTime", Long.toString(timeFetching));

            mergedMD.setValue("fetch.byteLength", Integer.toString(byteLength));

            // determine the status based on the status code
            final Status status = Status.fromHTTPCode(response.getStatusCode());

            eventCounter.scope("status_" + response.getStatusCode()).incrBy(1);

            // used when sending to status stream
            final Values values4status = new Values(urlString, mergedMD, status);

            // if the status is OK emit on default stream
            if (status.equals(Status.FETCHED)) {
                if (response.getStatusCode() == 304) {
                    // mark this URL as fetched so that it gets
                    // rescheduled
                    // but do not try to parse or index
                    collector.emit(
                            com.digitalpebble.stormcrawler.Constants.StatusStreamName,
                            input,
                            values4status);
                } else {
                    collector.emit(
                            Utils.DEFAULT_STREAM_ID,
                            input,
                            new Values(urlString, response.getContent(), mergedMD));
                }
            } else if (status.equals(Status.REDIRECTION)) {

                // find the URL it redirects to
                String redirection = response.getMetadata().getFirstValue(HttpHeaders.LOCATION);

                // stores the URL it redirects to
                // used for debugging mainly - do not resolve the target
                // URL
                if (StringUtils.isNotBlank(redirection)) {
                    mergedMD.setValue("_redirTo", redirection);
                }

                if (allowRedirs() && StringUtils.isNotBlank(redirection)) {
                    emitOutlink(input, url, redirection, mergedMD);
                }
                // Mark URL as redirected
                collector.emit(
                        com.digitalpebble.stormcrawler.Constants.StatusStreamName,
                        input,
                        values4status);
            } else {
                // Error
                collector.emit(
                        com.digitalpebble.stormcrawler.Constants.StatusStreamName,
                        input,
                        values4status);
            }

        } catch (Exception exece) {

            String message = exece.getMessage();
            if (message == null) message = "";

            // common exceptions for which we log only a short message
            if (exece.getCause() instanceof java.util.concurrent.TimeoutException
                    || message.contains(" timed out")) {
                LOG.error("Socket timeout fetching {}", urlString);
                message = "Socket timeout fetching";
            } else if (exece.getCause() instanceof java.net.UnknownHostException
                    || exece instanceof java.net.UnknownHostException) {
                LOG.error("Unknown host {}", urlString);
                message = "Unknown host";
            } else {
                LOG.error("Exception while fetching {}", urlString, exece);
                message = exece.getClass().getName();
            }
            eventCounter.scope("exception").incrBy(1);

            // could be an empty, immutable Metadata
            if (metadata.size() == 0) {
                metadata = new Metadata();
            }

            // add the reason of the failure in the metadata
            metadata.setValue("fetch.exception", message);

            collector.emit(
                    com.digitalpebble.stormcrawler.Constants.StatusStreamName,
                    input,
                    new Values(urlString, metadata, Status.FETCH_ERROR));
        }
        activeThreads.decrementAndGet();

        // update the throttler
        throttler.put(key, System.currentTimeMillis() + delay);

        collector.ack(input);
    }

    private String getPolitenessKey(URL u) {
        String key;
        if (QUEUE_MODE_IP.equalsIgnoreCase(queueMode)) {
            try {
                final InetAddress addr = InetAddress.getByName(u.getHost());
                key = addr.getHostAddress();
            } catch (final UnknownHostException e) {
                // unable to resolve it, so don't fall back to host name
                LOG.warn("Unable to resolve: {}, skipping.", u.getHost());
                return null;
            }
        } else if (QUEUE_MODE_DOMAIN.equalsIgnoreCase(queueMode)) {
            key = PaidLevelDomain.getPLD(u.getHost());
            if (key == null) {
                LOG.warn("Unknown domain for url: {}, using hostname as key", u.toExternalForm());
                key = u.getHost();
            }
        } else {
            key = u.getHost();
            if (key == null) {
                LOG.warn("Unknown host for url: {}, using URL string as key", u.toExternalForm());
                key = u.toExternalForm();
            }
        }
        return key.toLowerCase(Locale.ROOT);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.bolt;

import static com.digitalpebble.stormcrawler.Constants.StatusStreamName;

import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.parse.DocumentFragmentBuilder;
import com.digitalpebble.stormcrawler.parse.JSoupFilter;
import com.digitalpebble.stormcrawler.parse.JSoupFilters;
import com.digitalpebble.stormcrawler.parse.Outlink;
import com.digitalpebble.stormcrawler.parse.ParseData;
import com.digitalpebble.stormcrawler.parse.ParseFilter;
import com.digitalpebble.stormcrawler.parse.ParseFilters;
import com.digitalpebble.stormcrawler.parse.ParseResult;
import com.digitalpebble.stormcrawler.parse.TextExtractor;
import com.digitalpebble.stormcrawler.persistence.Status;
import com.digitalpebble.stormcrawler.protocol.HttpHeaders;
import com.digitalpebble.stormcrawler.protocol.ProtocolResponse;
import com.digitalpebble.stormcrawler.util.CharsetIdentification;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import com.digitalpebble.stormcrawler.util.RefreshTag;
import com.digitalpebble.stormcrawler.util.RobotsTags;
import java.io.ByteArrayInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.net.MalformedURLException;
import java.net.URL;
import java.nio.ByteBuffer;
import java.nio.charset.Charset;
import java.util.HashMap;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.stream.Collectors;
import org.apache.commons.lang.StringUtils;
import org.apache.storm.metric.api.MultiCountMetric;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.tuple.Fields;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.tuple.Values;
import org.apache.tika.config.TikaConfig;
import org.apache.tika.detect.Detector;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.mime.MediaType;
import org.jsoup.nodes.Element;
import org.jsoup.parser.Parser;
import org.jsoup.select.Elements;
import org.slf4j.LoggerFactory;
import org.w3c.dom.DocumentFragment;

/**
 * Parser for HTML documents only which uses ICU4J to detect the charset encoding. Kindly donated to
 * storm-crawler by shopstyle.com.
 */
@SuppressWarnings("serial")
public class JSoupParserBolt extends StatusEmitterBolt {

    /** Metadata key name for tracking the anchors */
    public static final String ANCHORS_KEY_NAME = "anchors";

    private static final org.slf4j.Logger LOG = LoggerFactory.getLogger(JSoupParserBolt.class);

    private MultiCountMetric eventCounter;

    private ParseFilter parseFilters = null;

    private JSoupFilter jsoupFilters = null;

    private Detector detector = TikaConfig.getDefaultConfig().getDetector();

    private boolean detectMimeType = true;

    private boolean trackAnchors = true;

    private boolean emitOutlinks = true;

    private int maxOutlinksPerPage = -1;

    private boolean robots_noFollow_strict = true;

    /**
     * If a Tuple is not HTML whether to send it to the status stream as an error or pass it on the
     * default stream
     */
    private boolean treat_non_html_as_error = true;

    /**
     * Length of content to use for detecting the charset. Set to -1 to use the full content (will
     * make the parser slow), 0 to deactivate the detection altogether, or any other value (at least
     * a few hundred bytes).
     */
    private int maxLengthCharsetDetection = -1;

    private TextExtractor textExtractor;

    private String protocolMDprefix;

    private boolean robotsHeaderSkip;

    private boolean robotsMetaSkip;

    private boolean fastCharsetDetection;

    private boolean ignoreMetaRedirections;

    @SuppressWarnings({"rawtypes", "unchecked"})
    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollector collector) {

        super.prepare(conf, context, collector);

        eventCounter =
                context.registerMetric(this.getClass().getSimpleName(), new MultiCountMetric(), 10);

        parseFilters = ParseFilters.fromConf(conf);

        jsoupFilters = JSoupFilters.fromConf(conf);

        emitOutlinks = ConfUtils.getBoolean(conf, "parser.emitOutlinks", true);

        trackAnchors = ConfUtils.getBoolean(conf, "track.anchors", true);

        robots_noFollow_strict =
                ConfUtils.getBoolean(conf, RobotsTags.ROBOTS_NO_FOLLOW_STRICT, true);

        treat_non_html_as_error = ConfUtils.getBoolean(conf, "jsoup.treat.non.html.as.error", true);

        detectMimeType = ConfUtils.getBoolean(conf, "detect.mimetype", true);

        maxLengthCharsetDetection = ConfUtils.getInt(conf, "detect.charset.maxlength", -1);

        fastCharsetDetection = ConfUtils.getBoolean(conf, "detect.charset.fast", false);

        maxOutlinksPerPage = ConfUtils.getInt(conf, "parser.emitOutlinks.max.per.page", -1);

        protocolMDprefix = ConfUtils.getString(conf, ProtocolResponse.PROTOCOL_MD_PREFIX_PARAM, "");

        robotsHeaderSkip = ConfUtils.getBoolean(conf, "http.robots.headers.skip", false);

        robotsMetaSkip = ConfUtils.getBoolean(conf, "http.robots.meta.skip", false);

        ignoreMetaRedirections =
                ConfUtils.getBoolean(conf, "jsoup.ignore.meta.redirections", false);

        textExtractor = new TextExtractor(conf);
    }

    @Override
    public void execute(Tuple tuple) {

        byte[] content = tuple.getBinaryByField("content");
        String url = tuple.getStringByField("url");
        Metadata metadata = (Metadata) tuple.getValueByField("metadata");

        LOG.info("Parsing : starting {}", url);

        // check that its content type is HTML
        // look at value found in HTTP headers
        boolean CT_OK = false;

        String mimeType = metadata.getFirstValue(HttpHeaders.CONTENT_TYPE, this.protocolMDprefix);

        if (detectMimeType) {
            try {
                mimeType = guessMimeType(url, mimeType, content);
            } catch (Exception e) {
                String errorMessage = "Exception while guessing mimetype on " + url + ": " + e;
                handleException(url, e, metadata, tuple, "mimetype guessing", errorMessage);
                return;
            }
            // store identified type in md
            metadata.setValue("parse.Content-Type", mimeType);
        }

        if (StringUtils.isNotBlank(mimeType)) {
            if (mimeType.toLowerCase().contains("html")) {
                CT_OK = true;
            }
        }
        // go ahead even if no mimetype is available
        else {
            CT_OK = true;
        }

        if (!CT_OK) {
            if (this.treat_non_html_as_error) {
                String errorMessage = "Exception content-type " + mimeType + " for " + url;
                RuntimeException e = new RuntimeException(errorMessage);
                handleException(url, e, metadata, tuple, "content-type checking", errorMessage);
            } else {
                LOG.info("Unsupported mimetype {} - passing on : {}", mimeType, url);
                collector.emit(tuple, new Values(url, content, metadata, ""));
                collector.ack(tuple);
            }
            return;
        }

        long start = System.currentTimeMillis();

        String charset;

        if (fastCharsetDetection) {
            charset =
                    CharsetIdentification.getCharsetFast(
                            metadata, content, maxLengthCharsetDetection);
        } else {
            charset =
                    CharsetIdentification.getCharset(metadata, content, maxLengthCharsetDetection);
        }

        LOG.debug(
                "Charset identified as {} in {} msec",
                charset,
                (System.currentTimeMillis() - start));

        RobotsTags robotsTags = new RobotsTags();

        // get the robots tags from the fetch metadata
        if (!robotsHeaderSkip) {
            robotsTags = new RobotsTags(metadata, this.protocolMDprefix);
        }

        Map<String, List<String>> slinks;
        String text = "";
        final org.jsoup.nodes.Document jsoupDoc;

        try {
            String html = Charset.forName(charset).decode(ByteBuffer.wrap(content)).toString();

            jsoupDoc = Parser.htmlParser().parseInput(html, url);

            if (!robotsMetaSkip) {
                // extracts the robots directives from the meta tags
                Element robotelement = jsoupDoc.selectFirst("meta[name~=(?i)robots][content]");
                if (robotelement != null) {
                    robotsTags.extractMetaTags(robotelement.attr("content"));
                }
            }

            // store a normalised representation in metadata
            // so that the indexer is aware of it
            robotsTags.normaliseToMetadata(metadata);

            // do not extract the links if no follow has been set
            // and we are in strict mode
            if (robotsTags.isNoFollow() && robots_noFollow_strict) {
                slinks = new HashMap<>(0);
            } else {
                Elements links = jsoupDoc.select("a[href]");
                slinks = new HashMap<>(links.size());
                for (Element link : links) {
                    // abs:href tells jsoup to return fully qualified domains
                    // for
                    // relative urls.
                    // e.g.: /foo will resolve to http://shopstyle.com/foo
                    String targetURL = link.attr("abs:href");

                    // nofollow
                    boolean noFollow = "nofollow".equalsIgnoreCase(link.attr("rel"));
                    // remove altogether
                    if (noFollow && robots_noFollow_strict) {
                        continue;
                    }

                    // link not specifically marked as no follow
                    // but whole page is
                    if (!noFollow && robotsTags.isNoFollow()) {
                        noFollow = true;
                    }

                    String anchor = link.text();
                    if (StringUtils.isNotBlank(targetURL)) {
                        // any existing anchors for the same target?
                        List<String> anchors = slinks.get(targetURL);
                        if (anchors == null) {
                            anchors = new LinkedList<>();
                            slinks.put(targetURL, anchors);
                        }
                        // track the anchors only if no follow is false
                        if (!noFollow && StringUtils.isNotBlank(anchor)) {
                            anchors.add(anchor);
                        }
                    }
                }
            }

            Element body = jsoupDoc.body();
            if (body != null) {
                text = textExtractor.text(body);
            }

        } catch (Throwable e) {
            String errorMessage = "Exception while parsing " + url + ": " + e;
            handleException(url, e, metadata, tuple, "content parsing", errorMessage);
            return;
        }

        // store identified charset in md
        metadata.setValue("parse.Content-Encoding", charset);

        // track that is has been successfully handled
        metadata.setValue("parsed.by", this.getClass().getName());

        long duration = System.currentTimeMillis() - start;

        LOG.info("Parsed {} in {} msec", url, duration);

        // redirection?
        if (!ignoreMetaRedirections) {
            try {
                String redirection = null;

                Element redirElement =
                        jsoupDoc.selectFirst("meta[http-equiv~=(?i)refresh][content]");
                if (redirElement != null) {
                    redirection = RefreshTag.extractRefreshURL(redirElement.attr("content"));
                }

                if (StringUtils.isNotBlank(redirection)) {
                    // stores the URL it redirects to
                    // used for debugging mainly - do not resolve the target
                    // URL
                    LOG.info("Found redir in {} to {}", url, redirection);
                    metadata.setValue("_redirTo", redirection);

                    if (allowRedirs() && StringUtils.isNotBlank(redirection)) {
                        emitOutlink(tuple, new URL(url), redirection, metadata);
                    }

                    // Mark URL as redirected
                    collector.emit(
                            com.digitalpebble.stormcrawler.Constants.StatusStreamName,
                            tuple,
                            new Values(url, metadata, Status.REDIRECTION));
                    collector.ack(tuple);
                    eventCounter.scope("tuple_success").incr();
                    return;
                }
            } catch (MalformedURLException e) {
                LOG.error("MalformedURLException on {}", url);
            }
        }

        List<Outlink> outlinks = toOutlinks(url, metadata, slinks);

        ParseResult parse = new ParseResult(outlinks);

        // parse data of the parent URL
        ParseData parseData = parse.get(url);
        parseData.setMetadata(metadata);
        parseData.setText(text);
        parseData.setContent(content);

        // apply the JSoup filters if any
        try {
            jsoupFilters.filter(url, content, jsoupDoc, parse);
        } catch (RuntimeException e) {
            String errorMessage = "Exception while running jsoup filters on " + url + ": " + e;
            handleException(url, e, metadata, tuple, "jsoup filtering", errorMessage);
            return;
        }

        // apply the parse filters if any
        try {
            DocumentFragment fragment = null;
            // lazy building of fragment
            if (parseFilters.needsDOM()) {
                fragment = DocumentFragmentBuilder.fromJsoup(jsoupDoc);
            }
            parseFilters.filter(url, content, fragment, parse);
        } catch (RuntimeException e) {
            String errorMessage = "Exception while running parse filters on " + url + ": " + e;
            handleException(url, e, metadata, tuple, "content filtering", errorMessage);
            return;
        }

        if (emitOutlinks) {
            final List<Outlink> outlinksAfterLimit =
                    (maxOutlinksPerPage == -1)
                            ? parse.getOutlinks()
                            : parse.getOutlinks().stream()
                                    .limit(maxOutlinksPerPage)
                                    .collect(Collectors.toList());
            for (Outlink outlink : outlinksAfterLimit) {
                collector.emit(
                        StatusStreamName,
                        tuple,
                        new Values(
                                outlink.getTargetURL(), outlink.getMetadata(), Status.DISCOVERED));
            }
        }

        // emit each document/subdocument in the ParseResult object
        // there should be at least one ParseData item for the "parent" URL

        for (Map.Entry<String, ParseData> doc : parse) {
            ParseData parseDoc = doc.getValue();
            collector.emit(
                    tuple,
                    new Values(
                            doc.getKey(),
                            parseDoc.getContent(),
                            parseDoc.getMetadata(),
                            parseDoc.getText()));
        }

        LOG.info("Total for {} - {} msec", url, System.currentTimeMillis() - start);

        collector.ack(tuple);
        eventCounter.scope("tuple_success").incr();
    }

    private void handleException(
            String url,
            Throwable e,
            Metadata metadata,
            Tuple tuple,
            String errorSource,
            String errorMessage) {
        LOG.error(errorMessage);
        // send to status stream in case another component wants to update
        // its status
        metadata.setValue(Constants.STATUS_ERROR_SOURCE, errorSource);
        metadata.setValue(Constants.STATUS_ERROR_MESSAGE, errorMessage);
        collector.emit(StatusStreamName, tuple, new Values(url, metadata, Status.ERROR));
        collector.ack(tuple);
        // Increment metric that is context specific
        String s = "error_" + errorSource.replaceAll(" ", "_") + "_";
        eventCounter.scope(s + e.getClass().getSimpleName()).incrBy(1);
        // Increment general metric
        eventCounter.scope("parse exception").incrBy(1);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        super.declareOutputFields(declarer);
        // output of this module is the list of fields to index
        // with at least the URL, text content
        declarer.declare(new Fields("url", "content", "metadata", "text"));
    }

    public String guessMimeType(String URL, String httpCT, byte[] content) {

        org.apache.tika.metadata.Metadata metadata = new org.apache.tika.metadata.Metadata();

        if (StringUtils.isNotBlank(httpCT)) {
            // pass content type from server as a clue
            metadata.set(org.apache.tika.metadata.Metadata.CONTENT_TYPE, httpCT);
        }

        // use full URL as a clue
        metadata.set(TikaCoreProperties.RESOURCE_NAME_KEY, URL);

        metadata.set(
                org.apache.tika.metadata.Metadata.CONTENT_LENGTH, Integer.toString(content.length));

        try (InputStream stream = new ByteArrayInputStream(content)) {
            MediaType mt = detector.detect(stream, metadata);
            return mt.toString();
        } catch (IOException e) {
            throw new IllegalStateException("Unexpected IOException", e);
        }
    }

    private List<Outlink> toOutlinks(
            String url, Metadata metadata, Map<String, List<String>> slinks) {
        Map<String, Outlink> outlinks = new HashMap<>();
        URL sourceUrl;
        try {
            sourceUrl = new URL(url);
        } catch (MalformedURLException e) {
            // we would have known by now as previous components check whether
            // the URL is valid
            LOG.error("MalformedURLException on {}", url);
            eventCounter.scope("error_invalid_source_url").incrBy(1);
            return new LinkedList<Outlink>();
        }

        for (Map.Entry<String, List<String>> linkEntry : slinks.entrySet()) {
            String targetURL = linkEntry.getKey();

            Outlink ol = filterOutlink(sourceUrl, targetURL, metadata);
            if (ol == null) {
                eventCounter.scope("outlink_filtered").incr();
                continue;
            }

            // the same link could already be there post-normalisation
            Outlink old = outlinks.get(ol.getTargetURL());
            if (old != null) {
                ol = old;
            }

            List<String> anchors = linkEntry.getValue();
            if (trackAnchors && anchors.size() > 0) {
                ol.getMetadata().addValues(ANCHORS_KEY_NAME, anchors);
                // sets the first anchor
                ol.setAnchor(anchors.get(0));
            }
            if (old == null) {
                outlinks.put(ol.getTargetURL(), ol);
                eventCounter.scope("outlink_kept").incr();
            }
        }

        return new LinkedList<Outlink>(outlinks.values());
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.bolt;

import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import crawlercommons.domains.PaidLevelDomain;
import java.net.InetAddress;
import java.net.MalformedURLException;
import java.net.URL;
import java.util.Collections;
import java.util.LinkedHashMap;
import java.util.Map;
import org.apache.commons.lang.StringUtils;
import org.apache.storm.metric.api.MultiCountMetric;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.topology.base.BaseRichBolt;
import org.apache.storm.tuple.Fields;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.tuple.Values;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/** Generates a partition key for a given URL based on the hostname, domain or IP address. */
public class URLPartitionerBolt extends BaseRichBolt {

    private static final Logger LOG = LoggerFactory.getLogger(URLPartitionerBolt.class);

    private OutputCollector _collector;

    private MultiCountMetric eventCounter;

    private Map<String, String> cache;

    private String mode = Constants.PARTITION_MODE_HOST;

    @Override
    public void execute(Tuple tuple) {
        String url = tuple.getStringByField("url");
        Metadata metadata = null;

        if (tuple.contains("metadata")) metadata = (Metadata) tuple.getValueByField("metadata");

        // maybe there is a field metadata but it can be null
        // or there was no field at all
        if (metadata == null) metadata = Metadata.empty;

        String partitionKey = null;
        String host = "";

        // IP in metadata?
        if (mode.equalsIgnoreCase(Constants.PARTITION_MODE_IP)) {
            String ip_provided = metadata.getFirstValue("ip");
            if (StringUtils.isNotBlank(ip_provided)) {
                partitionKey = ip_provided;
                eventCounter.scope("provided").incrBy(1);
            }
        }

        if (partitionKey == null) {
            URL u;
            try {
                u = new URL(url);
                host = u.getHost();
            } catch (MalformedURLException e1) {
                eventCounter.scope("Invalid URL").incrBy(1);
                LOG.warn("Invalid URL: {}", url);
                // ack it so that it doesn't get replayed
                _collector.ack(tuple);
                return;
            }
        }

        // partition by hostname
        if (mode.equalsIgnoreCase(Constants.PARTITION_MODE_HOST)) partitionKey = host;

        // partition by domain : needs fixing
        else if (mode.equalsIgnoreCase(Constants.PARTITION_MODE_DOMAIN)) {
            partitionKey = PaidLevelDomain.getPLD(host);
        }

        // partition by IP
        if (mode.equalsIgnoreCase(Constants.PARTITION_MODE_IP) && partitionKey == null) {
            // try to get it from cache first
            partitionKey = cache.get(host);
            if (partitionKey != null) {
                eventCounter.scope("from cache").incrBy(1);
            } else {
                try {
                    long start = System.currentTimeMillis();
                    final InetAddress addr = InetAddress.getByName(host);
                    partitionKey = addr.getHostAddress();
                    long end = System.currentTimeMillis();
                    LOG.debug("Resolved IP {} in {} msec for : {}", partitionKey, end - start, url);

                    // add to cache
                    cache.put(host, partitionKey);

                } catch (final Exception e) {
                    eventCounter.scope("Unable to resolve IP").incrBy(1);
                    LOG.warn("Unable to resolve IP for: {}", host);
                    _collector.ack(tuple);
                    return;
                }
            }
        }

        LOG.debug("Partition Key for: {} > {}", url, partitionKey);

        _collector.emit(tuple, new Values(url, partitionKey, metadata));
        _collector.ack(tuple);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("url", "key", "metadata"));
    }

    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {

        mode =
                ConfUtils.getString(
                        stormConf,
                        Constants.PARTITION_MODEParamName,
                        Constants.PARTITION_MODE_HOST);

        // check that the mode is known
        if (!mode.equals(Constants.PARTITION_MODE_IP)
                && !mode.equals(Constants.PARTITION_MODE_DOMAIN)
                && !mode.equals(Constants.PARTITION_MODE_HOST)) {
            LOG.error("Unknown partition mode : {} - forcing to byHost", mode);
            mode = Constants.PARTITION_MODE_HOST;
        }

        LOG.info("Using partition mode : {}", mode);

        _collector = collector;
        // Register a "MultiCountMetric" to count different events in this bolt
        // Storm will emit the counts every n seconds to a special bolt via a
        // system stream
        // The data can be accessed by registering a "MetricConsumer" in the
        // topology
        this.eventCounter = context.registerMetric("URLPartitioner", new MultiCountMetric(), 10);

        final int MAX_ENTRIES = 500;
        cache =
                new LinkedHashMap(MAX_ENTRIES + 1, .75F, true) {
                    // This method is called just after a new entry has been added
                    @Override
                    public boolean removeEldestEntry(Map.Entry eldest) {
                        return size() > MAX_ENTRIES;
                    }
                };

        // If the cache is to be used by multiple threads,
        // the cache must be wrapped with code to synchronize the methods
        cache = Collections.synchronizedMap(cache);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.util;

import com.digitalpebble.stormcrawler.Metadata;
import java.nio.ByteBuffer;
import java.nio.charset.StandardCharsets;
import java.util.List;
import org.apache.storm.spout.Scheme;
import org.apache.storm.tuple.Fields;
import org.apache.storm.tuple.Values;

/** Converts a byte array into URL + metadata */
public class StringTabScheme implements Scheme {

    @Override
    public List<Object> deserialize(ByteBuffer bytes) {
        String input = new String(bytes.array(), StandardCharsets.UTF_8);

        String[] tokens = input.split("\t");
        if (tokens.length < 1) return new Values();

        String url = tokens[0];
        Metadata metadata = null;

        for (int i = 1; i < tokens.length; i++) {
            String token = tokens[i];
            // split into key & value
            int firstequals = token.indexOf("=");
            String value = null;
            String key = token;
            if (firstequals != -1) {
                key = token.substring(0, firstequals);
                value = token.substring(firstequals + 1);
            }
            if (metadata == null) metadata = new Metadata();
            metadata.addValue(key, value);
        }

        if (metadata == null) metadata = new Metadata();

        return new Values(url, metadata);
    }

    @Override
    public Fields getOutputFields() {
        return new Fields("url", "metadata");
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.util;

import com.digitalpebble.stormcrawler.Metadata;
import javax.xml.xpath.XPath;
import javax.xml.xpath.XPathConstants;
import javax.xml.xpath.XPathExpression;
import javax.xml.xpath.XPathExpressionException;
import javax.xml.xpath.XPathFactory;
import org.w3c.dom.DocumentFragment;
import org.w3c.dom.NamedNodeMap;
import org.w3c.dom.Node;
import org.w3c.dom.NodeList;

/**
 * Normalises the robots instructions provided by the HTML meta tags or the HTTP X-Robots-Tag
 * headers.
 */
public class RobotsTags {

    public static final String ROBOTS_NO_INDEX = "robots.noIndex";

    public static final String ROBOTS_NO_FOLLOW = "robots.noFollow";

    /**
     * Whether to interpret the noFollow directive strictly (remove links) or not (remove anchor and
     * do not track original URL). True by default.
     */
    public static final String ROBOTS_NO_FOLLOW_STRICT = "robots.noFollow.strict";

    public static final String ROBOTS_NO_CACHE = "robots.noCache";

    private boolean noIndex = false;

    private boolean noFollow = false;

    private boolean noCache = false;

    private static final XPathExpression expression;

    static {
        XPath xpath = XPathFactory.newInstance().newXPath();
        try {
            expression = xpath.compile("/HTML/*/META");
        } catch (XPathExpressionException e) {
            throw new RuntimeException(e);
        }
    }

    /** Get the values from the fetch metadata * */
    public RobotsTags(Metadata metadata, String protocolMDprefix) {
        // HTTP headers
        // X-Robots-Tag: noindex
        String[] values = metadata.getValues("X-Robots-Tag", protocolMDprefix);
        if (values == null) return;
        if (values.length == 1) {
            // just in case they put all the values on a single line
            values = values[0].split(" *, *");
        }
        parseValues(values);
    }

    public RobotsTags() {}

    // set the values based on the meta tags
    // HTML tags
    // <meta name="robots" content="noarchive, nofollow"/>
    // called by the parser bolts
    public void extractMetaTags(DocumentFragment doc) throws XPathExpressionException {
        NodeList nodes = (NodeList) expression.evaluate(doc, XPathConstants.NODESET);
        if (nodes == null) return;
        int numNodes = nodes.getLength();
        for (int i = 0; i < numNodes; i++) {
            Node n = (Node) nodes.item(i);
            // iterate on the attributes
            // and check that it has name=robots and content
            // whatever the case is
            boolean isRobots = false;
            String content = null;
            NamedNodeMap attrs = n.getAttributes();
            for (int att = 0; att < attrs.getLength(); att++) {
                Node keyval = attrs.item(att);
                if ("name".equalsIgnoreCase(keyval.getNodeName())
                        && "robots".equalsIgnoreCase(keyval.getNodeValue())) {
                    isRobots = true;
                    continue;
                }
                if ("content".equalsIgnoreCase(keyval.getNodeName())) {
                    content = keyval.getNodeValue();
                    continue;
                }
            }

            if (isRobots && content != null) {
                // got a value - split it
                String[] vals = content.split(" *, *");
                parseValues(vals);
                return;
            }
        }
    }

    /** Extracts meta tags based on the value of the content attribute * */
    public void extractMetaTags(String content) {
        if (content == null) return;
        String[] vals = content.split(" *, *");
        parseValues(vals);
    }

    private void parseValues(String[] values) {
        for (String v : values) {
            v = v.trim();
            if ("noindex".equalsIgnoreCase(v)) {
                noIndex = true;
            } else if ("nofollow".equalsIgnoreCase(v)) {
                noFollow = true;
            } else if ("noarchive".equalsIgnoreCase(v)) {
                noCache = true;
            } else if ("none".equalsIgnoreCase(v)) {
                noIndex = true;
                noFollow = true;
                noCache = true;
            }
        }
    }

    /** Adds a normalised representation of the directives in the metadata * */
    public void normaliseToMetadata(Metadata metadata) {
        metadata.setValue(ROBOTS_NO_INDEX, Boolean.toString(noIndex));
        metadata.setValue(ROBOTS_NO_CACHE, Boolean.toString(noCache));
        metadata.setValue(ROBOTS_NO_FOLLOW, Boolean.toString(noFollow));
    }

    public boolean isNoIndex() {
        return noIndex;
    }

    public boolean isNoFollow() {
        return noFollow;
    }

    public boolean isNoCache() {
        return noCache;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.util;

import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.Metadata;
import crawlercommons.domains.PaidLevelDomain;
import java.net.InetAddress;
import java.net.MalformedURLException;
import java.net.URL;
import java.util.Map;
import org.apache.commons.lang.StringUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Generates a partition key for a given URL based on the hostname, domain or IP address. This can
 * be called by the URLPartitionerBolt or any other component.
 */
public class URLPartitioner {

    private static final Logger LOG = LoggerFactory.getLogger(URLPartitioner.class);

    private String mode = Constants.PARTITION_MODE_HOST;

    /**
     * Returns the host, domain, IP of a URL so that it can be partitioned for politeness, depending
     * on the value of the config <i>partition.url.mode</i>.
     */
    public String getPartition(String url, Metadata metadata) {

        String partitionKey = null;
        String host = "";

        // IP in metadata?
        if (mode.equalsIgnoreCase(Constants.PARTITION_MODE_IP)) {
            String ip_provided = metadata.getFirstValue("ip");
            if (StringUtils.isNotBlank(ip_provided)) {
                partitionKey = ip_provided;
            }
        }

        if (partitionKey == null) {
            URL u;
            try {
                u = new URL(url);
                host = u.getHost();
            } catch (MalformedURLException e1) {
                LOG.warn("Invalid URL: {}", url);
                return null;
            }
        }

        // partition by hostname
        if (mode.equalsIgnoreCase(Constants.PARTITION_MODE_HOST)) partitionKey = host;

        // partition by domain : needs fixing
        else if (mode.equalsIgnoreCase(Constants.PARTITION_MODE_DOMAIN)) {
            partitionKey = PaidLevelDomain.getPLD(host);
        }

        // partition by IP
        if (mode.equalsIgnoreCase(Constants.PARTITION_MODE_IP) && partitionKey == null) {
            try {
                long start = System.currentTimeMillis();
                final InetAddress addr = InetAddress.getByName(host);
                partitionKey = addr.getHostAddress();
                long end = System.currentTimeMillis();
                LOG.debug("Resolved IP {} in {} msec for : {}", partitionKey, end - start, url);
            } catch (final Exception e) {
                LOG.warn("Unable to resolve IP for: {}", host);
                return null;
            }
        }

        LOG.debug("Partition Key for: {} > {}", url, partitionKey);

        return partitionKey;
    }

    public void configure(Map stormConf) {

        mode =
                ConfUtils.getString(
                        stormConf,
                        Constants.PARTITION_MODEParamName,
                        Constants.PARTITION_MODE_HOST);

        // check that the mode is known
        if (!mode.equals(Constants.PARTITION_MODE_IP)
                && !mode.equals(Constants.PARTITION_MODE_DOMAIN)
                && !mode.equals(Constants.PARTITION_MODE_HOST)) {
            LOG.error("Unknown partition mode : {} - forcing to byHost", mode);
            mode = Constants.PARTITION_MODE_HOST;
        }

        LOG.info("Using partition mode : {}", mode);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.util;

import java.util.regex.Matcher;
import java.util.regex.Pattern;
import javax.xml.xpath.XPath;
import javax.xml.xpath.XPathConstants;
import javax.xml.xpath.XPathExpression;
import javax.xml.xpath.XPathExpressionException;
import javax.xml.xpath.XPathFactory;
import org.apache.commons.lang.StringUtils;
import org.w3c.dom.DocumentFragment;

// Utility class used to extract refresh tags from HTML pages
public abstract class RefreshTag {

    private static final XPathExpression expression;
    private static final Matcher matcher =
            Pattern.compile("^.*;\\s*URL=(.+)$", Pattern.CASE_INSENSITIVE).matcher("");

    static {
        XPath xpath = XPathFactory.newInstance().newXPath();
        try {
            expression = xpath.compile("/HTML/*/META[@http-equiv=\"refresh\"]/@content");
        } catch (XPathExpressionException e) {
            throw new RuntimeException(e);
        }
    }

    // Returns a normalised value of the content attribute for the refresh tag
    public static String extractRefreshURL(String value) {
        if (StringUtils.isBlank(value)) return null;

        // 0;URL=http://www.apollocolors.com/site
        try {
            if (matcher.reset(value).matches()) {
                return matcher.group(1);
            }
        } catch (Exception e) {
        }

        return null;
    }

    public static String extractRefreshURL(DocumentFragment doc) {
        String value;
        try {
            value = (String) expression.evaluate(doc, XPathConstants.STRING);
        } catch (XPathExpressionException e) {
            return null;
        }

        return extractRefreshURL(value);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.util;

import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.InputStreamReader;
import java.nio.charset.Charset;
import java.util.Collection;
import java.util.HashMap;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import org.apache.storm.Config;
import org.yaml.snakeyaml.Yaml;

public class ConfUtils {

    private ConfUtils() {}

    public static int getInt(Map<String, Object> conf, String key, int defaultValue) {
        Object ret = conf.get(key);
        if (ret == null) {
            ret = defaultValue;
        }
        return ((Number) ret).intValue();
    }

    public static long getLong(Map<String, Object> conf, String key, long defaultValue) {
        Object ret = conf.get(key);
        if (ret == null) {
            ret = defaultValue;
        }
        return ((Number) ret).longValue();
    }

    public static float getFloat(Map<String, Object> conf, String key, float defaultValue) {
        Object ret = conf.get(key);
        if (ret == null) {
            ret = defaultValue;
        }
        return ((Number) ret).floatValue();
    }

    public static boolean getBoolean(Map<String, Object> conf, String key, boolean defaultValue) {
        Object ret = conf.get(key);
        if (ret == null) {
            ret = defaultValue;
        }
        return ((Boolean) ret).booleanValue();
    }

    public static String getString(Map<String, Object> conf, String key) {
        return (String) conf.get(key);
    }

    public static String getString(Map<String, Object> conf, String key, String defaultValue) {
        Object ret = conf.get(key);
        if (ret == null) {
            ret = defaultValue;
        }
        return (String) ret;
    }

    /**
     * Return one or more Strings regardless of whether they are represented as a single String or a
     * list in the config or an empty List if no value could be found for that key.
     */
    public static List<String> loadListFromConf(String paramKey, Map stormConf) {
        Object obj = stormConf.get(paramKey);
        List<String> list = new LinkedList<>();

        if (obj == null) return list;

        if (obj instanceof Collection) {
            list.addAll((Collection<String>) obj);
        } else { // single value?
            list.add(obj.toString());
        }
        return list;
    }

    public static Config loadConf(String resource, Config conf) throws FileNotFoundException {
        Yaml yaml = new Yaml();
        Map ret =
                (Map)
                        yaml.load(
                                new InputStreamReader(
                                        new FileInputStream(resource), Charset.defaultCharset()));
        if (ret == null) {
            ret = new HashMap();
        }
        // contains a single config element ?
        else {
            ret = extractConfigElement(ret);
        }
        conf.putAll(ret);
        return conf;
    }

    /** If the config consists of a single key 'config', its values are used instead */
    public static Map extractConfigElement(Map conf) {
        if (conf.size() == 1) {
            Object confNode = conf.get("config");
            if (confNode != null && confNode instanceof Map) {
                conf = (Map) confNode;
            }
        }
        return conf;
    }
}

Parse Compilation Unit:
package com.digitalpebble.stormcrawler.util;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.protocol.HttpHeaders;
import com.ibm.icu.text.CharsetDetector;
import com.ibm.icu.text.CharsetMatch;
import java.io.ByteArrayInputStream;
import java.io.IOException;
import java.nio.charset.Charset;
import java.nio.charset.IllegalCharsetNameException;
import java.nio.charset.StandardCharsets;
import java.util.Arrays;
import java.util.Locale;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
import org.apache.commons.io.ByteOrderMark;
import org.apache.commons.io.input.BOMInputStream;
import org.jsoup.nodes.Document;
import org.jsoup.nodes.Element;
import org.jsoup.parser.Parser;
import org.jsoup.select.Elements;

public class CharsetIdentification {

    public static final Charset DEFAULT_CHARSET = StandardCharsets.UTF_8;

    private static final Pattern charsetPattern =
            Pattern.compile("(?i)\\bcharset=\\s*(?:[\"'])?([^\\s,;\"']*)");

    /**
     * Identifies the charset of a document based on the following logic: guess from the
     * ByteOrderMark - else return any charset specified in the http headers if any, otherwise
     * return the one from the html metadata; finally use ICU's charset detector to make an educated
     * guess and if that fails too returns UTF-8. This approach is expected to be faster but maybe
     * less reliable than getCharset.
     *
     * @since 1.18
     */
    public static String getCharsetFast(
            final Metadata metadata, final byte[] content, final int maxLengthCharsetDetection) {

        // let's look at the BOM first
        String charset = getCharsetFromBOM(content);
        if (charset != null) {
            return charset;
        }

        // then look at what we get from HTTP headers
        charset = getCharsetFromHTTP(metadata);
        if (charset != null) {
            return charset;
        }

        charset = getCharsetFromMeta(content, maxLengthCharsetDetection);
        if (charset != null) {
            return charset;
        }

        // let's guess from the text without a hint
        charset = getCharsetFromText(content, null, maxLengthCharsetDetection);
        if (charset != null) {
            return charset;
        }

        // return the default charset
        return DEFAULT_CHARSET.name();
    }

    /**
     * Identifies the charset of a document based on the following logic: guess from the
     * ByteOrderMark - else if the same charset is specified in the http headers and the html
     * metadata then use it - otherwise use ICU's charset detector to make an educated guess and if
     * that fails too returns UTF-8.
     */
    public static String getCharset(
            Metadata metadata, byte[] content, int maxLengthCharsetDetection) {

        // let's look at the BOM first
        String BOMCharset = getCharsetFromBOM(content);
        if (BOMCharset != null) {
            return BOMCharset;
        }

        // then look at what we get from HTTP headers and HTML content
        String httpCharset = getCharsetFromHTTP(metadata);
        String htmlCharset = getCharsetFromMeta(content, maxLengthCharsetDetection);

        // both exist and agree
        if (httpCharset != null
                && htmlCharset != null
                && httpCharset.equalsIgnoreCase(htmlCharset)) {
            return httpCharset;
        }

        // let's guess from the text - using a hint or not
        String hintCharset = null;
        if (httpCharset != null && htmlCharset == null) {
            hintCharset = httpCharset;
        } else if (httpCharset == null && htmlCharset != null) {
            hintCharset = htmlCharset;
        }

        String textCharset = getCharsetFromText(content, hintCharset, maxLengthCharsetDetection);
        if (textCharset != null) {
            return textCharset;
        }

        // return the default charset
        return DEFAULT_CHARSET.name();
    }

    /** Returns the charset declared by the server if any */
    private static String getCharsetFromHTTP(Metadata metadata) {
        return getCharsetFromContentType(metadata.getFirstValue(HttpHeaders.CONTENT_TYPE));
    }

    /** Detects any BOMs and returns the corresponding charset */
    private static String getCharsetFromBOM(final byte[] byteData) {
        try (BOMInputStream bomIn = new BOMInputStream(new ByteArrayInputStream(byteData))) {
            ByteOrderMark bom = bomIn.getBOM();
            if (bom != null) {
                return bom.getCharsetName();
            }
        } catch (IOException e) {
            return null;
        }
        return null;
    }

    /** Use a third party library as last resort to guess the charset from the bytes. */
    private static String getCharsetFromText(
            byte[] content, String declaredCharset, int maxLengthCharsetDetection) {
        String charset = null;
        // filter HTML tags
        CharsetDetector charsetDetector = new CharsetDetector();
        charsetDetector.enableInputFilter(true);
        // give it a hint
        if (declaredCharset != null) charsetDetector.setDeclaredEncoding(declaredCharset);
        // trim the content of the text for the detection
        byte[] subContent = content;
        if (maxLengthCharsetDetection != -1 && content.length > maxLengthCharsetDetection) {
            subContent = Arrays.copyOfRange(content, 0, maxLengthCharsetDetection);
        }
        charsetDetector.setText(subContent);
        try {
            CharsetMatch charsetMatch = charsetDetector.detect();
            charset = validateCharset(charsetMatch.getName());
        } catch (Exception e) {
            charset = null;
        }
        return charset;
    }

    /**
     * Attempt to find a META tag in the HTML that hints at the character set used to write the
     * document.
     */
    private static String getCharsetFromMeta(byte buffer[], int maxlength) {
        // convert to UTF-8 String -- which hopefully will not mess up the
        // characters we're interested in...
        int len = buffer.length;
        if (maxlength > 0 && maxlength < len) {
            len = maxlength;
        }
        String html = new String(buffer, 0, len, DEFAULT_CHARSET);

        // fast search for e.g. <meta charset="utf-8">
        // might not get it 100% but should be frequent enough
        // and faster than parsing
        int start = html.indexOf("<meta charset=\"");
        if (start != -1) {
            int end = html.indexOf('"', start + 15);
            // https://github.com/DigitalPebble/storm-crawler/issues/870
            // try on a slightly larger section of text if it is trimmed
            if (end == -1 && ((maxlength + 10) < buffer.length)) {
                return getCharsetFromMeta(buffer, maxlength + 10);
            }
            if (end == -1) {
                // there is an open tag meta but not closed = we have broken content!
                return null;
            }
            return validateCharset(html.substring(start + 15, end));
        }

        String foundCharset = null;

        try {
            Document doc = Parser.htmlParser().parseInput(html, "dummy");

            // look for <meta http-equiv="Content-Type"
            // content="text/html;charset=gb2312"> or HTML5 <meta
            // charset="gb2312">
            Elements metaElements = doc.select("meta[http-equiv=content-type], meta[charset]");
            for (Element meta : metaElements) {
                if (meta.hasAttr("http-equiv"))
                    foundCharset = getCharsetFromContentType(meta.attr("content"));
                if (foundCharset == null && meta.hasAttr("charset"))
                    foundCharset = meta.attr("charset");
                if (foundCharset != null) return foundCharset;
            }
        } catch (Exception e) {
            foundCharset = null;
        }

        return foundCharset;
    }

    /**
     * Parse out a charset from a content type header. If the charset is not supported, returns null
     * (so the default will kick in.)
     *
     * @param contentType e.g. "text/html; charset=EUC-JP"
     * @return "EUC-JP", or null if not found. Charset is trimmed and uppercased.
     */
    private static String getCharsetFromContentType(String contentType) {
        if (contentType == null) return null;
        Matcher m = charsetPattern.matcher(contentType);
        if (m.find()) {
            String charset = m.group(1).trim();
            charset = charset.replace("charset=", "");
            return validateCharset(charset);
        }
        return null;
    }

    private static String validateCharset(String cs) {
        if (cs == null || cs.length() == 0) return null;
        cs = cs.trim().replaceAll("[\"']", "");
        try {
            if (Charset.isSupported(cs)) return cs;
            cs = cs.toUpperCase(Locale.ENGLISH);
            if (Charset.isSupported(cs)) return cs;
        } catch (IllegalCharsetNameException e) {
        }
        return null;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.util;

import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.Metadata;
import com.google.common.collect.ImmutableList;
import java.io.Serializable;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import org.apache.storm.generated.GlobalStreamId;
import org.apache.storm.grouping.CustomStreamGrouping;
import org.apache.storm.shade.org.apache.commons.lang.StringUtils;
import org.apache.storm.task.WorkerTopologyContext;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

@SuppressWarnings("serial")
/**
 * Directs tuples to a specific bolt instance based on the URLPartitioner, e.g. byIP, byDomain or
 * byHost.
 *
 * <p>Use as follows with Flux :
 *
 * <pre>{@code
 * streams:
 *  - from: "spout"
 *    to: "status"
 *    grouping:
 *      type: CUSTOM
 *      customClass:
 *        className: "com.digitalpebble.stormcrawler.util.URLStreamGrouping"
 *        constructorArgs:
 *          - "byDomain"
 * }</pre>
 */
public class URLStreamGrouping implements CustomStreamGrouping, Serializable {

    private static final Logger LOG = LoggerFactory.getLogger(URLStreamGrouping.class);

    private List<Integer> targetTask;

    private URLPartitioner partitioner;

    private String partitionMode;

    /** Groups URLs based on the hostname * */
    public URLStreamGrouping() {}

    public URLStreamGrouping(String mode) {
        partitionMode = mode;
    }

    @Override
    public void prepare(
            WorkerTopologyContext context, GlobalStreamId stream, List<Integer> targetTasks) {
        this.targetTask = targetTasks;
        partitioner = new URLPartitioner();
        if (StringUtils.isNotBlank(partitionMode)) {
            Map<String, String> conf = new HashMap<>();
            conf.put(Constants.PARTITION_MODEParamName, partitionMode);
            partitioner.configure(conf);
        }
    }

    @Override
    public List<Integer> chooseTasks(int taskId, List<Object> values) {
        // optimisation : single target
        if (targetTask.size() == 1) {
            return targetTask;
        }

        // missing content in tuple
        // should not happen
        // return empty task ids
        if (values.size() < 2) {
            LOG.error("Found tuple with less than 2 values. {}", values);
            return ImmutableList.of();
        }

        // the first value is always the URL
        // and the second the metadata
        String url = (String) values.get(0);
        Metadata metadata = (Metadata) values.get(1);
        String partitionKey = partitioner.getPartition(url, metadata);

        if (StringUtils.isBlank(partitionKey)) {
            LOG.error("No partition key for {}", url);
            return ImmutableList.of();
        }

        // hash on the key
        int partition = Math.abs(partitionKey.hashCode() % targetTask.size());
        return ImmutableList.of(targetTask.get(partition));
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.util;

import org.apache.storm.metric.api.IReducer;

/** Used to return an average value per second * */
public class PerSecondReducer implements IReducer<TimeReducerState> {

    @Override
    public TimeReducerState init() {
        return new TimeReducerState();
    }

    @Override
    public TimeReducerState reduce(TimeReducerState accumulator, Object input) {
        if (input instanceof Double) {
            accumulator.sum += (Double) input;
        } else if (input instanceof Long) {
            accumulator.sum += ((Long) input).doubleValue();
        } else if (input instanceof Integer) {
            accumulator.sum += ((Integer) input).doubleValue();
        } else {
            throw new RuntimeException(
                    "MeanReducer::reduce called with unsupported input type `"
                            + input.getClass()
                            + "`. Supported types are Double, Long, Integer.");
        }
        return accumulator;
    }

    @Override
    public Object extractResult(TimeReducerState accumulator) {
        // time spent
        double msec = System.currentTimeMillis() - accumulator.started;
        if (msec == 0) return 0;
        double permsec = accumulator.sum / msec;
        return new Double(permsec * 1000d);
    }
}

class TimeReducerState {
    public long started = System.currentTimeMillis();
    public double sum = 0.0;
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.util;

import java.net.IDN;
import java.net.MalformedURLException;
import java.net.URI;
import java.net.URL;
import java.util.Locale;
import java.util.regex.Pattern;

/** Utility class for URL analysis */
public class URLUtil {

    private URLUtil() {}

    /**
     * Resolve relative URL-s and fix a few java.net.URL errors in handling of URLs with embedded
     * params and pure query targets.
     *
     * @param base base url
     * @param target target url (may be relative)
     * @return resolved absolute url.
     * @throws MalformedURLException
     */
    public static URL resolveURL(URL base, String target) throws MalformedURLException {
        target = target.trim();

        if (target.startsWith("?")) {
            return fixPureQueryTargets(base, target);
        }

        return new URL(base, target);
    }

    /** Handle the case in RFC3986 section 5.4.1 example 7, and similar. */
    static URL fixPureQueryTargets(URL base, String target) throws MalformedURLException {
        if (!target.startsWith("?")) return new URL(base, target);

        String basePath = base.getPath();
        String baseRightMost = "";
        int baseRightMostIdx = basePath.lastIndexOf("/");
        if (baseRightMostIdx != -1) {
            baseRightMost = basePath.substring(baseRightMostIdx + 1);
        }

        if (target.startsWith("?")) target = baseRightMost + target;

        return new URL(base, target);
    }

    /**
     * Handles cases where the url param information is encoded into the base url as opposed to the
     * target.
     *
     * <p>If the taget contains params (i.e. ';xxxx') information then the target params information
     * is assumed to be correct and any base params information is ignored. If the base contains
     * params information but the tareget does not, then the params information is moved to the
     * target allowing it to be correctly determined by the java.net.URL class.
     *
     * @param base The base URL.
     * @param target The target path from the base URL.
     * @return URL A URL with the params information correctly encoded.
     * @throws MalformedURLException If the url is not a well formed URL.
     */
    private static URL fixEmbeddedParams(URL base, String target) throws MalformedURLException {

        // the target contains params information or the base doesn't then no
        // conversion necessary, return regular URL
        if (target.indexOf(';') >= 0 || base.toString().indexOf(';') == -1) {
            return new URL(base, target);
        }

        // get the base url and it params information
        String baseURL = base.toString();
        int startParams = baseURL.indexOf(';');
        String params = baseURL.substring(startParams);

        // if the target has a query string then put the params information
        // after
        // any path but before the query string, otherwise just append to the
        // path
        int startQS = target.indexOf('?');
        if (startQS >= 0) {
            target = target.substring(0, startQS) + params + target.substring(startQS);
        } else {
            target += params;
        }

        return new URL(base, target);
    }

    private static Pattern IP_PATTERN = Pattern.compile("(\\d{1,3}\\.){3}(\\d{1,3})");

    /** Partitions of the hostname of the url by "." */
    public static String[] getHostSegments(URL url) {
        String host = url.getHost();
        // return whole hostname, if it is an ipv4
        // TODO : handle ipv6
        if (IP_PATTERN.matcher(host).matches()) return new String[] {host};
        return host.split("\\.");
    }

    /**
     * Partitions of the hostname of the url by "."
     *
     * @throws MalformedURLException
     */
    public static String[] getHostSegments(String url) throws MalformedURLException {
        return getHostSegments(new URL(url));
    }

    /**
     * Returns the lowercased hostname for the url or null if the url is not well formed.
     *
     * @param url The url to check.
     * @return String The hostname for the url.
     */
    public static String getHost(String url) {
        try {
            return new URL(url).getHost().toLowerCase(Locale.ROOT);
        } catch (MalformedURLException e) {
            return null;
        }
    }

    /**
     * Returns the page for the url. The page consists of the protocol, host, and path, but does not
     * include the query string. The host is lowercased but the path is not.
     *
     * @param url The url to check.
     * @return String The page for the url.
     */
    public static String getPage(String url) {
        try {
            // get the full url, and replace the query string with and empty
            // string
            url = url.toLowerCase(Locale.ROOT);
            String queryStr = new URL(url).getQuery();
            return (queryStr != null) ? url.replace("?" + queryStr, "") : url;
        } catch (MalformedURLException e) {
            return null;
        }
    }

    public static String toASCII(String url) {
        try {
            URL u = new URL(url);
            URI p =
                    new URI(
                            u.getProtocol(),
                            null,
                            IDN.toASCII(u.getHost()),
                            u.getPort(),
                            u.getPath(),
                            u.getQuery(),
                            u.getRef());

            return p.toString();
        } catch (Exception e) {
            return null;
        }
    }

    public static String toUNICODE(String url) {
        try {
            URL u = new URL(url);
            URI p =
                    new URI(
                            u.getProtocol(),
                            null,
                            IDN.toUnicode(u.getHost()),
                            u.getPort(),
                            u.getPath(),
                            u.getQuery(),
                            u.getRef());

            return p.toString();
        } catch (Exception e) {
            return null;
        }
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.util;

import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.Metadata;
import java.util.HashSet;
import java.util.Map;
import java.util.Set;
import org.apache.commons.lang.StringUtils;

/**
 * Implements the logic of how the metadata should be passed to the outlinks, what should be stored
 * back in the persistence layer etc...
 */
public class MetadataTransfer {

    /**
     * Class to use for transfering metadata to outlinks. Must extend the class MetadataTransfer.
     */
    public static final String metadataTransferClassParamName = "metadata.transfer.class";

    /**
     * Parameter name indicating which metadata to transfer to the outlinks and persist for a given
     * document. Value is either a vector or a single valued String.
     */
    public static final String metadataTransferParamName = "metadata.transfer";

    /**
     * Parameter name indicating which metadata to persist for a given document but <b>not</b>
     * transfer to outlinks. Value is either a vector or a single valued String.
     */
    public static final String metadataPersistParamName = "metadata.persist";

    /**
     * Parameter name indicating whether to track the url path or not. Boolean value, true by
     * default.
     */
    public static final String trackPathParamName = "metadata.track.path";

    /**
     * Parameter name indicating whether to track the depth from seed. Boolean value, true by
     * default.
     */
    public static final String trackDepthParamName = "metadata.track.depth";

    /** Metadata key name for tracking the source URLs */
    public static final String urlPathKeyName = "url.path";

    /** Metadata key name for tracking the depth */
    public static final String depthKeyName = "depth";

    /** Metadata key name for tracking a non-default max depth */
    public static final String maxDepthKeyName = "max.depth";

    private Set<String> mdToTransfer = new HashSet<>();

    private Set<String> mdToPersistOnly = new HashSet<>();

    private boolean trackPath = true;

    private boolean trackDepth = true;

    public static MetadataTransfer getInstance(Map<String, Object> conf) {
        String className = ConfUtils.getString(conf, metadataTransferClassParamName);

        MetadataTransfer transferInstance;

        // no custom class specified
        if (StringUtils.isBlank(className)) {
            transferInstance = new MetadataTransfer();
        } else {
            try {
                Class<?> transferClass = Class.forName(className);
                boolean interfaceOK = MetadataTransfer.class.isAssignableFrom(transferClass);
                if (!interfaceOK) {
                    throw new RuntimeException(
                            "Class " + className + " must extend MetadataTransfer");
                }
                transferInstance = (MetadataTransfer) transferClass.newInstance();
            } catch (Exception e) {
                throw new RuntimeException("Can't instanciate " + className);
            }
        }

        // should not be null
        if (transferInstance != null) transferInstance.configure(conf);

        return transferInstance;
    }

    protected void configure(Map<String, Object> conf) {

        trackPath = ConfUtils.getBoolean(conf, trackPathParamName, true);

        trackDepth = ConfUtils.getBoolean(conf, trackDepthParamName, true);

        // keep the path but don't add anything to it
        if (trackPath) {
            mdToTransfer.add(urlPathKeyName);
        }

        // keep the depth but don't add anything to it
        if (trackDepth) {
            mdToTransfer.add(depthKeyName);
            mdToTransfer.add(maxDepthKeyName);
        }

        mdToTransfer.addAll(ConfUtils.loadListFromConf(metadataTransferParamName, conf));
        mdToPersistOnly.addAll(ConfUtils.loadListFromConf(metadataPersistParamName, conf));
        // always add the fetch error count
        mdToPersistOnly.add(Constants.fetchErrorCountParamName);
    }

    /**
     * Determine which metadata should be transfered to an outlink. Adds additional metadata like
     * the URL path.
     */
    public Metadata getMetaForOutlink(String targetURL, String sourceURL, Metadata parentMD) {
        Metadata md = _filter(parentMD, mdToTransfer);

        // keep the path?
        if (trackPath) {
            md.addValue(urlPathKeyName, sourceURL);
        }

        // track depth
        if (trackDepth) {
            String existingDepth = md.getFirstValue(depthKeyName);
            int depth;
            try {
                depth = Integer.parseInt(existingDepth);
            } catch (Exception e) {
                depth = 0;
            }
            md.setValue(depthKeyName, Integer.toString(++depth));
        }

        return md;
    }

    /**
     * Determine which metadata should be persisted for a given document including those which are
     * not necessarily transferred to the outlinks
     */
    public Metadata filter(Metadata metadata) {
        Metadata filtered_md = _filter(metadata, mdToTransfer);

        // add the features that are only persisted but
        // not transfered like __redirTo_
        filtered_md.putAll(_filter(metadata, mdToPersistOnly));

        return filtered_md;
    }

    private Metadata _filter(Metadata metadata, Set<String> filter) {
        Metadata filtered_md = new Metadata();
        for (String key : filter) {
            String[] vals = metadata.getValues(key);
            if (vals != null) filtered_md.setValues(key, vals);
        }
        return filtered_md;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.util;

import java.util.LinkedList;
import java.util.List;
import org.apache.storm.metric.api.IMetric;

public class CollectionMetric implements IMetric {

    private List<Long> measurements = new LinkedList<>();

    public void addMeasurement(long l) {
        synchronized (measurements) {
            measurements.add(l);
        }
    }

    @Override
    public Object getValueAndReset() {
        synchronized (measurements) {
            LinkedList<Long> copy = new LinkedList<>(measurements);
            measurements.clear();
            return copy;
        }
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.util;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.node.NullNode;
import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import org.slf4j.LoggerFactory;

public interface Configurable {

    static final org.slf4j.Logger LOG = LoggerFactory.getLogger(Configurable.class);

    /**
     * Called when this filter is being initialized
     *
     * @param stormConf The Storm configuration used for the ParserBolt
     * @param filterParams the filter specific configuration. Never null
     */
    public default void configure(Map stormConf, JsonNode filterParams) {}

    /**
     * Used by classes URLFilters and ParseFilters classes to load the configuration of filters from
     * JSON
     */
    @SuppressWarnings("rawtypes")
    public static <T extends Configurable> List<T> configure(
            Map stormConf, JsonNode filtersConf, Class<T> filterClass, String callingClass) {
        // initialises the filters
        List<T> filterLists = new ArrayList<>();

        // get the filters part
        filtersConf = filtersConf.get(callingClass);

        if (filtersConf == null) {
            LOG.info("No field {} in JSON config. Skipping", callingClass);
            return filterLists;
        }

        // conf node contains a list of objects
        Iterator<JsonNode> filterIter = filtersConf.elements();
        while (filterIter.hasNext()) {
            JsonNode afilterConf = filterIter.next();
            String filterName = "<unnamed>";
            JsonNode nameNode = afilterConf.get("name");
            if (nameNode != null) {
                filterName = nameNode.textValue();
            }
            JsonNode classNode = afilterConf.get("class");
            if (classNode == null) {
                LOG.error("Filter {} doesn't specified a 'class' attribute", filterName);
                continue;
            }
            String className = classNode.textValue().trim();
            filterName += '[' + className + ']';
            // check that it is available and implements the interface
            // ParseFilter
            try {
                Class<?> filterImplClass = Class.forName(className);
                boolean subClassOK = filterClass.isAssignableFrom(filterImplClass);
                if (!subClassOK) {
                    LOG.error("Filter {} does not extend {}", filterName, filterClass.getName());
                    continue;
                }
                T filterInstance = (T) filterImplClass.newInstance();

                JsonNode paramNode = afilterConf.get("params");
                if (paramNode != null) {
                    filterInstance.configure(stormConf, paramNode);
                } else {
                    // Pass in a nullNode if missing
                    filterInstance.configure(stormConf, NullNode.getInstance());
                }

                filterLists.add(filterInstance);
                LOG.info("Setup {}", filterName);
            } catch (Exception e) {
                LOG.error("Can't setup {}: {}", filterName, e);
                throw new RuntimeException("Can't setup " + filterName, e);
            }
        }

        return filterLists;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.util;

import java.net.URL;
import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Date;
import java.util.List;
import java.util.Locale;
import org.apache.http.cookie.Cookie;
import org.apache.http.impl.cookie.BasicClientCookie;

/** Helper to extract cookies from cookies string. */
public class CookieConverter {

    private static final SimpleDateFormat DATE_FORMAT =
            new SimpleDateFormat("EEE, dd MMM yyyy HH:mm:ss zzz", Locale.ENGLISH);

    /**
     * Get a list of cookies based on the cookies string taken from response header and the target
     * url.
     *
     * @param cookiesString the value of the http header for "Cookie" in the http response.
     * @param targetURL the url for which we wish to pass the cookies in the request.
     * @return List off cookies to add to the request.
     */
    public static List<Cookie> getCookies(String[] cookiesStrings, URL targetURL) {
        ArrayList<Cookie> list = new ArrayList<Cookie>();

        for (String cs : cookiesStrings) {
            String name = null;
            String value = null;

            String expires = null;
            String domain = null;
            String path = null;

            boolean secure = false;

            String[] tokens = cs.split(";");

            int equals = tokens[0].indexOf("=");
            name = tokens[0].substring(0, equals);
            value = tokens[0].substring(equals + 1);

            for (int i = 1; i < tokens.length; i++) {
                String ti = tokens[i].trim();
                if (ti.equalsIgnoreCase("secure")) secure = true;
                if (ti.toLowerCase().startsWith("path=")) {
                    path = ti.substring(5);
                }
                if (ti.toLowerCase().startsWith("domain=")) {
                    domain = ti.substring(7);
                }
                if (ti.toLowerCase().startsWith("expires=")) {
                    expires = ti.substring(8);
                }
            }

            BasicClientCookie cookie = new BasicClientCookie(name, value);

            // check domain
            if (domain != null) {
                cookie.setDomain(domain);

                if (!checkDomainMatchToUrl(domain, targetURL.getHost())) continue;
            }

            // check path
            if (path != null) {
                cookie.setPath(path);

                if (!path.equals("") && !path.equals("/") && !targetURL.getPath().startsWith(path))
                    continue;
            }

            // check secure
            if (secure) {
                cookie.setSecure(secure);

                if (!targetURL.getProtocol().equalsIgnoreCase("https")) continue;
            }

            // check expiration
            if (expires != null) {
                try {
                    Date expirationDate = DATE_FORMAT.parse(expires);
                    cookie.setExpiryDate(expirationDate);

                    // check that it hasn't expired?
                    if (cookie.isExpired(new Date())) continue;

                    cookie.setExpiryDate(expirationDate);
                } catch (ParseException e) {
                    // ignore exceptions
                }
            }

            // attach additional infos to cookie
            list.add(cookie);
        }

        return list;
    }

    /**
     * Helper method to check if url matches a cookie domain.
     *
     * @param cookieDomain the domain in the cookie
     * @param urlHostName the host name of the url
     * @return does the cookie match the host name
     */
    public static boolean checkDomainMatchToUrl(String cookieDomain, String urlHostName) {
        try {
            if (cookieDomain.startsWith(".")) {
                cookieDomain = cookieDomain.substring(1);
            }
            String[] domainTokens = cookieDomain.split("\\.");
            String[] hostTokens = urlHostName.split("\\.");

            int tokenDif = hostTokens.length - domainTokens.length;
            if (tokenDif < 0) {
                return false;
            }

            for (int i = domainTokens.length - 1; i >= 0; i--) {
                if (!domainTokens[i].equalsIgnoreCase(hostTokens[i + tokenDif])) {
                    return false;
                }
            }
            return true;
        } catch (Exception e) {
            return true;
        }
    }
}

Parse Compilation Unit:
package com.digitalpebble.stormcrawler;

import com.fasterxml.jackson.core.JsonParseException;
import com.fasterxml.jackson.databind.JsonMappingException;
import java.io.IOException;
import java.io.InputStream;

/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Defines a generic behaviour for ParseFilters or URLFilters to load resources from a JSON file.
 */
public interface JSONResource {

    /** @return filename of the JSON resource */
    public String getResourceFile();

    /**
     * Load the resources from an input stream
     *
     * @throws Exception
     */
    public void loadJSONResources(InputStream inputStream)
            throws JsonParseException, JsonMappingException, IOException;

    /**
     * Load the resources from the JSON file in the uber jar
     *
     * @throws Exception
     */
    public default void loadJSONResources() throws Exception {
        try (InputStream inputStream =
                getClass().getClassLoader().getResourceAsStream(getResourceFile())) {
            loadJSONResources(inputStream);
        }
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.parse;

import com.digitalpebble.stormcrawler.util.ConfUtils;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import org.jsoup.internal.StringUtil;
import org.jsoup.nodes.CDataNode;
import org.jsoup.nodes.Element;
import org.jsoup.nodes.Node;
import org.jsoup.nodes.TextNode;
import org.jsoup.select.Elements;
import org.jsoup.select.NodeTraversor;
import org.jsoup.select.NodeVisitor;

/**
 * Filters the text extracted from HTML documents, used by JSoupParserBolt. Configured with optional
 * inclusion patterns based on <a href="https://jsoup.org/cookbook/extracting-data/selector-syntax">
 * JSoup selectors</a>, as well as a list of tags to be excluded.
 *
 * <p>Replaces {@link ContentFilter}.
 *
 * <p>The first matching inclusion pattern is used or the whole document if no expressions are
 * configured or no match has been found.
 *
 * <p>The TextExtraction can be configured as so:
 *
 * <pre>{@code
 * textextractor.include.pattern:
 *  - DIV[id="maincontent"]
 *  - DIV[itemprop="articleBody"]
 *  - ARTICLE
 *
 * textextractor.exclude.tags:
 *  - STYLE
 *  - SCRIPT
 *
 * }</pre>
 *
 * @since 1.13
 */
public class TextExtractor {

    public static final String INCLUDE_PARAM_NAME = "textextractor.include.pattern";
    public static final String EXCLUDE_PARAM_NAME = "textextractor.exclude.tags";
    public static final String NO_TEXT_PARAM_NAME = "textextractor.no.text";

    private List<String> inclusionPatterns;
    private HashSet<String> excludedTags;
    private boolean noText;

    public TextExtractor(Map stormConf) {
        noText = ConfUtils.getBoolean(stormConf, NO_TEXT_PARAM_NAME, false);
        inclusionPatterns = ConfUtils.loadListFromConf(INCLUDE_PARAM_NAME, stormConf);
        excludedTags = new HashSet<String>();
        ConfUtils.loadListFromConf(EXCLUDE_PARAM_NAME, stormConf)
                .forEach((s) -> excludedTags.add(s.toLowerCase()));
    }

    public String text(Element element) {
        // not interested in getting any text?
        if (noText) return "";

        // no patterns at all - return the text from the whole document
        if (inclusionPatterns.size() == 0 && excludedTags.size() == 0) {
            return _text(element);
        }

        Elements matches = new Elements();

        for (String pattern : inclusionPatterns) {
            matches = element.select(pattern);
            if (!matches.isEmpty()) break;
        }

        // if nothing matches or no patterns were defined use the whole doc
        if (matches.isEmpty()) {
            matches.add(element);
        }

        final StringBuilder accum = new StringBuilder();

        for (Element node : matches) {
            accum.append(_text(node)).append("\n");
        }

        return accum.toString().trim();
    }

    private String _text(Node node) {
        final StringBuilder accum = new StringBuilder();
        NodeTraversor.traverse(
                new NodeVisitor() {

                    private Node excluded = null;

                    public void head(Node node, int depth) {
                        if (excluded == null && node instanceof TextNode) {
                            TextNode textNode = (TextNode) node;
                            appendNormalisedText(accum, textNode);
                        } else if (node instanceof Element) {
                            Element element = (Element) node;
                            if (excludedTags.contains(element.tagName())) {
                                excluded = element;
                            }
                            if (accum.length() > 0
                                    && (element.isBlock() || element.tag().getName().equals("br"))
                                    && !lastCharIsWhitespace(accum)) accum.append(' ');
                        }
                    }

                    public void tail(Node node, int depth) {
                        // make sure there is a space between block tags and immediately
                        // following text nodes <div>One</div>Two should be "One Two".
                        if (node instanceof Element) {
                            Element element = (Element) node;
                            if (element == excluded) {
                                excluded = null;
                            }
                            if (element.isBlock()
                                    && (node.nextSibling() instanceof TextNode)
                                    && !lastCharIsWhitespace(accum)) accum.append(' ');
                        }
                    }
                },
                node);
        return accum.toString().trim();
    }

    private static void appendNormalisedText(StringBuilder accum, TextNode textNode) {
        String text = textNode.getWholeText();

        if (preserveWhitespace(textNode.parent()) || textNode instanceof CDataNode)
            accum.append(text);
        else StringUtil.appendNormalisedWhitespace(accum, text, lastCharIsWhitespace(accum));
    }

    static boolean preserveWhitespace(Node node) {
        // looks only at this element and five levels up, to prevent recursion &
        // needless stack searches
        if (node != null && node instanceof Element) {
            Element el = (Element) node;
            int i = 0;
            do {
                if (el.tag().preserveWhitespace()) return true;
                el = el.parent();
                i++;
            } while (i < 6 && el != null);
        }
        return false;
    }

    static boolean lastCharIsWhitespace(StringBuilder sb) {
        return sb.length() != 0 && sb.charAt(sb.length() - 1) == ' ';
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.parse;

import com.digitalpebble.stormcrawler.JSONResource;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import com.digitalpebble.stormcrawler.util.Configurable;
import com.fasterxml.jackson.core.JsonParseException;
import com.fasterxml.jackson.databind.JsonMappingException;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import java.io.IOException;
import java.io.InputStream;
import java.net.URL;
import java.util.List;
import java.util.Map;
import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.CommandLineParser;
import org.apache.commons.cli.DefaultParser;
import org.apache.commons.cli.Options;
import org.apache.commons.cli.ParseException;
import org.apache.commons.io.IOUtils;
import org.apache.commons.lang.StringUtils;
import org.apache.storm.Config;
import org.apache.storm.utils.Utils;
import org.jsoup.Jsoup;
import org.jsoup.nodes.Document;
import org.slf4j.LoggerFactory;

/** Wrapper for the JSoupFilters defined in a JSON configuration */
public class JSoupFilters extends JSoupFilter implements JSONResource {

    public static final JSoupFilters emptyParseFilter = new JSoupFilters();

    private static final org.slf4j.Logger LOG = LoggerFactory.getLogger(JSoupFilters.class);

    private JSoupFilter[] filters;

    private JSoupFilters() {
        filters = new JSoupFilter[0];
    }

    private String configFile;

    private Map stormConf;

    /**
     * Loads and configure the JSoupFilters based on the storm config if there is one otherwise
     * returns an empty JSoupFilter.
     */
    @SuppressWarnings("rawtypes")
    public static JSoupFilters fromConf(Map stormConf) {
        String parseconfigfile = ConfUtils.getString(stormConf, "jsoup.filters.config.file");
        if (StringUtils.isNotBlank(parseconfigfile)) {
            try {
                return new JSoupFilters(stormConf, parseconfigfile);
            } catch (IOException e) {
                String message =
                        "Exception caught while loading the JSoupFilters from " + parseconfigfile;
                LOG.error(message);
                throw new RuntimeException(message, e);
            }
        }

        return JSoupFilters.emptyParseFilter;
    }

    /**
     * loads the filters from a JSON configuration file
     *
     * @throws IOException
     */
    @SuppressWarnings("rawtypes")
    public JSoupFilters(Map stormConf, String configFile) throws IOException {
        this.configFile = configFile;
        this.stormConf = stormConf;
        try {
            loadJSONResources();
        } catch (Exception e) {
            throw new IOException("Unable to build JSON object from file", e);
        }
    }

    @Override
    public void loadJSONResources(InputStream inputStream)
            throws JsonParseException, JsonMappingException, IOException {
        ObjectMapper mapper = new ObjectMapper();
        JsonNode confNode = mapper.readValue(inputStream, JsonNode.class);
        configure(stormConf, confNode);
    }

    @Override
    public String getResourceFile() {
        return this.configFile;
    }

    @SuppressWarnings("rawtypes")
    @Override
    public void configure(Map stormConf, JsonNode filtersConf) {
        List<JSoupFilter> list =
                Configurable.configure(
                        stormConf, filtersConf, JSoupFilter.class, this.getClass().getName());
        filters = list.toArray(new JSoupFilter[list.size()]);
    }

    @Override
    public void filter(String URL, byte[] content, Document doc, ParseResult parse) {
        for (JSoupFilter filter : filters) {
            long start = System.currentTimeMillis();
            filter.filter(URL, content, doc, parse);
            long end = System.currentTimeMillis();
            LOG.debug("JSoupFilter {} took {} msec", filter.getClass().getName(), end - start);
        }
    }

    /** * Used for quick testing + debugging */
    public static void main(String[] args) throws IOException, ParseException {

        Config conf = new Config();

        // loads the default configuration file
        Map defaultSCConfig = Utils.findAndReadConfigFile("crawler-default.yaml", false);
        conf.putAll(ConfUtils.extractConfigElement(defaultSCConfig));

        Options options = new Options();
        options.addOption("c", true, "stormcrawler configuration file");

        CommandLineParser parser = new DefaultParser();
        CommandLine cmd = parser.parse(options, args);

        if (cmd.hasOption("c")) {
            String confFile = cmd.getOptionValue("c");
            ConfUtils.loadConf(confFile, conf);
        }

        JSoupFilters filters = JSoupFilters.fromConf(conf);

        System.out.println(filters.filters.length + " filters found");

        ParseResult parse = new ParseResult();

        String url = cmd.getArgs()[0];

        byte[] content = IOUtils.toByteArray((new URL(url)).openStream());

        Document doc = Jsoup.parse(new String(content), url);

        filters.filter(url, content, doc, parse);

        System.out.println(parse.toString());

        System.exit(0);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.parse;

import com.digitalpebble.stormcrawler.util.Configurable;
import org.w3c.dom.DocumentFragment;

/**
 * Implementations of ParseFilter are responsible for extracting custom data from the crawled
 * content. They are used by parsing bolts such as {@link
 * com.digitalpebble.stormcrawler.bolt.JSoupParserBolt} or {@link
 * com.digitalpebble.stormcrawler.bolt.SiteMapParserBolt}.
 */
public abstract class ParseFilter implements Configurable {

    /**
     * Called when parsing a specific page
     *
     * @param URL the URL of the page being parsed
     * @param content the content being parsed
     * @param doc the DOM tree resulting of the parsing of the content or null if {@link
     *     #needsDOM()} returns <code>false</code>
     * @param parse the metadata to be updated with the resulting of the parsing
     */
    public abstract void filter(
            String URL, byte[] content, DocumentFragment doc, ParseResult parse);

    /**
     * Specifies whether this filter requires a DOM representation of the document
     *
     * @return <code>true</code>if this needs a DOM representation of the document, <code>false
     *     </code> otherwise.
     */
    public boolean needsDOM() {
        return false;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.parse;

import com.digitalpebble.stormcrawler.Metadata;

public class Outlink {

    private String targetURL;
    private String anchor;
    private Metadata metadata;

    public Outlink(String url) {
        targetURL = url;
    }

    public Outlink(String url, String a) {
        targetURL = url;
        anchor = a;
    }

    public String getTargetURL() {
        return targetURL;
    }

    public void setTargetURL(String targetURL) {
        this.targetURL = targetURL;
    }

    public String getAnchor() {
        return anchor;
    }

    public void setAnchor(String anchor) {
        this.anchor = anchor;
    }

    public Metadata getMetadata() {
        return metadata;
    }

    public void setMetadata(Metadata metadata) {
        this.metadata = metadata;
    }

    public String toString() {
        return targetURL;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.parse;

import com.digitalpebble.stormcrawler.util.Configurable;

/**
 * Implementations of ParseFilter are responsible for extracting custom data from the crawled
 * content. They are used exclusively by {@link
 * com.digitalpebble.stormcrawler.bolt.JSoupParserBolt}.
 */
public abstract class JSoupFilter implements Configurable {

    /**
     * Called when parsing a specific page
     *
     * @param URL the URL of the page being parsed
     * @param content the content being parsed
     * @param doc document produced by JSoup's parsingF
     * @param parse the metadata to be updated with the resulting of the parsing
     */
    public abstract void filter(
            String URL, byte[] content, org.jsoup.nodes.Document doc, ParseResult parse);
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.parse;

import org.apache.html.dom.HTMLDocumentImpl;
import org.jsoup.nodes.Attribute;
import org.jsoup.select.NodeTraversor;
import org.jsoup.select.NodeVisitor;
import org.w3c.dom.Comment;
import org.w3c.dom.DocumentFragment;
import org.w3c.dom.Element;
import org.w3c.dom.Text;

/** Adapted from org.jsoup.helper.W3CDom but does not transfer namespaces. * */
public final class DocumentFragmentBuilder {

    /** Restrict instantiation */
    private DocumentFragmentBuilder() {}

    public static DocumentFragment fromJsoup(org.jsoup.nodes.Document jsoupDocument) {
        HTMLDocumentImpl htmlDoc = new HTMLDocumentImpl();
        htmlDoc.setErrorChecking(false);
        DocumentFragment fragment = htmlDoc.createDocumentFragment();
        org.jsoup.nodes.Element rootEl = jsoupDocument.child(0); // skip the
        // #root node
        NodeTraversor.traverse(new W3CBuilder(htmlDoc, fragment), rootEl);
        return fragment;
    }

    /** Implements the conversion by walking the input. */
    protected static class W3CBuilder implements NodeVisitor {
        private final HTMLDocumentImpl doc;
        private final DocumentFragment fragment;

        private Element dest;

        public W3CBuilder(HTMLDocumentImpl doc, DocumentFragment fragment) {
            this.fragment = fragment;
            this.doc = doc;
        }

        public void head(org.jsoup.nodes.Node source, int depth) {
            if (source instanceof org.jsoup.nodes.Element) {
                org.jsoup.nodes.Element sourceEl = (org.jsoup.nodes.Element) source;
                Element el = doc.createElement(sourceEl.tagName());
                copyAttributes(sourceEl, el);
                if (dest == null) { // sets up the root
                    fragment.appendChild(el);
                } else {
                    dest.appendChild(el);
                }
                dest = el; // descend
            } else if (source instanceof org.jsoup.nodes.TextNode) {
                org.jsoup.nodes.TextNode sourceText = (org.jsoup.nodes.TextNode) source;
                Text text = doc.createTextNode(sourceText.getWholeText());
                dest.appendChild(text);
            } else if (source instanceof org.jsoup.nodes.Comment) {
                org.jsoup.nodes.Comment sourceComment = (org.jsoup.nodes.Comment) source;
                Comment comment = doc.createComment(sourceComment.getData());
                dest.appendChild(comment);
            } else if (source instanceof org.jsoup.nodes.DataNode) {
                org.jsoup.nodes.DataNode sourceData = (org.jsoup.nodes.DataNode) source;
                Text node = doc.createTextNode(sourceData.getWholeData());
                dest.appendChild(node);
            } else {
                // unhandled
            }
        }

        public void tail(org.jsoup.nodes.Node source, int depth) {
            if (source instanceof org.jsoup.nodes.Element
                    && dest.getParentNode() instanceof Element) {
                dest = (Element) dest.getParentNode(); // undescend. cromulent.
            }
        }

        private void copyAttributes(org.jsoup.nodes.Node source, Element el) {
            for (Attribute attribute : source.attributes()) {
                // valid xml attribute names are: ^[a-zA-Z_:][-a-zA-Z0-9_:.]
                String key = attribute.getKey().replaceAll("[^-a-zA-Z0-9_:.]", "");
                if (key.matches("[a-zA-Z_:][-a-zA-Z0-9_:.]*"))
                    el.setAttribute(key, attribute.getValue());
            }
        }
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.parse.filter;

import com.digitalpebble.stormcrawler.parse.ParseFilter;
import com.digitalpebble.stormcrawler.parse.ParseResult;
import com.fasterxml.jackson.databind.JsonNode;
import java.io.File;
import java.io.IOException;
import java.io.OutputStream;
import java.util.Map;
import org.apache.commons.io.FileUtils;
import org.apache.xml.serialize.XMLSerializer;
import org.w3c.dom.DocumentFragment;

/** Dumps the DOM representation of a document into a file */
public class DebugParseFilter extends ParseFilter {

    private OutputStream os;

    @Override
    public void filter(String URL, byte[] content, DocumentFragment doc, ParseResult parse) {

        try {
            XMLSerializer serializer = new XMLSerializer(os, null);
            serializer.serialize(doc);
            os.flush();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    @SuppressWarnings("rawtypes")
    @Override
    public void configure(Map stormConf, JsonNode filterParams) {
        try {
            File outFile = File.createTempFile("DOMDump", ".xml");
            os = FileUtils.openOutputStream(outFile);
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    @Override
    public boolean needsDOM() {
        return true;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.parse.filter;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.parse.ParseFilter;
import com.digitalpebble.stormcrawler.parse.ParseResult;
import com.fasterxml.jackson.databind.JsonNode;
import java.util.HashSet;
import java.util.Iterator;
import java.util.Map;
import java.util.Set;
import org.w3c.dom.DocumentFragment;

/**
 * Rewrites single metadata containing comma separated values into multiple values for the same key,
 * useful for instance for keyword tags.
 */
public class CommaSeparatedToMultivaluedMetadata extends ParseFilter {

    private final Set<String> keys = new HashSet<>();

    @Override
    public void configure(Map stormConf, JsonNode filterParams) {
        JsonNode node = filterParams.get("keys");
        if (node == null) {
            return;
        }
        if (node.isArray()) {
            Iterator<JsonNode> iter = node.iterator();
            while (iter.hasNext()) {
                keys.add(iter.next().asText());
            }
        } else {
            keys.add(node.asText());
        }
    }

    @Override
    public void filter(String url, byte[] content, DocumentFragment doc, ParseResult parse) {
        Metadata m = parse.get(url).getMetadata();
        for (String key : keys) {
            String val = m.getFirstValue(key);
            if (val == null) continue;
            m.remove(key);
            String[] tokens = val.split(" *, *");
            for (String t : tokens) {
                m.addValue(key, t);
            }
        }
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.parse.filter;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.parse.ParseData;
import com.digitalpebble.stormcrawler.parse.ParseFilter;
import com.digitalpebble.stormcrawler.parse.ParseResult;
import com.fasterxml.jackson.databind.JsonNode;
import java.io.IOException;
import java.io.StringWriter;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import javax.xml.namespace.QName;
import javax.xml.xpath.XPath;
import javax.xml.xpath.XPathConstants;
import javax.xml.xpath.XPathExpression;
import javax.xml.xpath.XPathExpressionException;
import javax.xml.xpath.XPathFactory;
import org.apache.commons.lang.StringUtils;
import org.apache.xml.serialize.Method;
import org.apache.xml.serialize.OutputFormat;
import org.apache.xml.serialize.XMLSerializer;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.w3c.dom.Document;
import org.w3c.dom.DocumentFragment;
import org.w3c.dom.Element;
import org.w3c.dom.Node;
import org.w3c.dom.NodeList;

/**
 * Simple ParseFilter to illustrate and test the interface. Reads a XPATH pattern from the config
 * file and stores the value as metadata
 */
public class XPathFilter extends ParseFilter {

    private enum EvalFunction {
        NONE,
        STRING,
        SERIALIZE;

        public QName getReturnType() {
            switch (this) {
                case STRING:
                    return XPathConstants.STRING;
                default:
                    return XPathConstants.NODESET;
            }
        }
    }

    private static final Logger LOG = LoggerFactory.getLogger(XPathFilter.class);

    private XPathFactory factory = XPathFactory.newInstance();
    private XPath xpath = factory.newXPath();

    protected final Map<String, List<LabelledExpression>> expressions = new HashMap<>();

    class LabelledExpression {

        String key;
        private EvalFunction evalFunction;
        private XPathExpression expression;

        private LabelledExpression(String key, String expression) throws XPathExpressionException {
            this.key = key;
            if (expression.startsWith("string(")) {
                evalFunction = EvalFunction.STRING;
            } else if (expression.startsWith("serialize(")) {
                expression = expression.substring(10, expression.length() - 1);
                evalFunction = EvalFunction.SERIALIZE;
            } else {
                evalFunction = EvalFunction.NONE;
            }
            this.expression = xpath.compile(expression);
        }

        List<String> evaluate(DocumentFragment doc) throws XPathExpressionException, IOException {
            Object evalResult = expression.evaluate(doc, evalFunction.getReturnType());
            List<String> values = new LinkedList<>();
            switch (evalFunction) {
                case STRING:
                    if (evalResult != null) {
                        String strippedValue = StringUtils.strip((String) evalResult);
                        values.add(strippedValue);
                    }
                    break;
                case SERIALIZE:
                    NodeList nodesToSerialize = (NodeList) evalResult;
                    StringWriter out = new StringWriter();
                    OutputFormat format = new OutputFormat(Method.XHTML, null, false);
                    format.setOmitXMLDeclaration(true);
                    XMLSerializer serializer = new XMLSerializer(out, format);
                    for (int i = 0; i < nodesToSerialize.getLength(); i++) {
                        Node node = nodesToSerialize.item(i);
                        switch (node.getNodeType()) {
                            case Node.ELEMENT_NODE:
                                serializer.serialize((Element) node);
                                break;
                            case Node.DOCUMENT_NODE:
                                serializer.serialize((Document) node);
                                break;
                            case Node.DOCUMENT_FRAGMENT_NODE:
                                serializer.serialize((DocumentFragment) node);
                                break;
                            case Node.TEXT_NODE:
                                String text = node.getTextContent();
                                if (text.length() > 0) {
                                    values.add(text);
                                }
                                // By pass the rest of the code since it is used to
                                // extract
                                // the value out of the serialized which isn't used in
                                // this case
                                continue;
                        }
                        String serializedValue = out.toString();
                        if (serializedValue.length() > 0) {
                            values.add(serializedValue);
                        }
                        out.getBuffer().setLength(0);
                    }
                    break;
                default:
                    NodeList nodes = (NodeList) evalResult;
                    for (int i = 0; i < nodes.getLength(); i++) {
                        Node node = nodes.item(i);
                        values.add(StringUtils.strip(node.getTextContent()));
                    }
            }
            return values;
        }
    }

    @Override
    public void filter(String URL, byte[] content, DocumentFragment doc, ParseResult parse) {

        ParseData parseData = parse.get(URL);
        Metadata metadata = parseData.getMetadata();

        // applies the XPATH expression in the order in which they are produced
        java.util.Iterator<List<LabelledExpression>> iter = expressions.values().iterator();
        while (iter.hasNext()) {
            List<LabelledExpression> leList = iter.next();
            for (LabelledExpression le : leList) {
                try {
                    List<String> values = le.evaluate(doc);
                    if (values != null && !values.isEmpty()) {
                        metadata.addValues(le.key, values);
                        break;
                    }
                } catch (XPathExpressionException e) {
                    LOG.error("Error evaluating {}: {}", le.key, e);
                } catch (IOException e) {
                    LOG.error("Error evaluating {}: {}", le.key, e);
                }
            }
        }
    }

    @SuppressWarnings("rawtypes")
    @Override
    public void configure(Map stormConf, JsonNode filterParams) {
        java.util.Iterator<Entry<String, JsonNode>> iter = filterParams.fields();
        while (iter.hasNext()) {
            Entry<String, JsonNode> entry = iter.next();
            String key = entry.getKey();
            JsonNode node = entry.getValue();
            if (node.isArray()) {
                for (JsonNode expression : node) {
                    addExpression(key, expression);
                }
            } else {
                addExpression(key, entry.getValue());
            }
        }
    }

    private void addExpression(String key, JsonNode expression) {
        String xpathvalue = expression.asText();
        try {
            List<LabelledExpression> lexpressionList = expressions.get(key);
            if (lexpressionList == null) {
                lexpressionList = new ArrayList<>();
                expressions.put(key, lexpressionList);
            }
            LabelledExpression lexpression = new LabelledExpression(key, xpathvalue);
            lexpressionList.add(lexpression);
        } catch (XPathExpressionException e) {
            throw new RuntimeException("Can't compile expression : " + xpathvalue, e);
        }
    }

    @Override
    public boolean needsDOM() {
        return true;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.parse.filter;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.filtering.URLFilters;
import com.digitalpebble.stormcrawler.parse.Outlink;
import com.digitalpebble.stormcrawler.parse.ParseData;
import com.digitalpebble.stormcrawler.parse.ParseResult;
import com.digitalpebble.stormcrawler.util.MetadataTransfer;
import com.digitalpebble.stormcrawler.util.URLUtil;
import com.fasterxml.jackson.databind.JsonNode;
import java.net.MalformedURLException;
import java.net.URL;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.w3c.dom.DocumentFragment;

/**
 * ParseFilter to extract additional links with Xpath can be configured with e.g.
 *
 * <pre>{@code
 * {
 *   "class": "com.digitalpebble.stormcrawler.parse.filter.LinkParseFilter",
 *   "name": "LinkParseFilter",
 *   "params": {
 *     "pattern": "//IMG/@src",
 *     "pattern2": "//VIDEO/SOURCE/@src"
 *   }
 * }
 *
 * }</pre>
 */
public class LinkParseFilter extends XPathFilter {

    private static final Logger LOG = LoggerFactory.getLogger(LinkParseFilter.class);

    private MetadataTransfer metadataTransfer;

    private URLFilters urlFilters;

    @Override
    public void filter(String URL, byte[] content, DocumentFragment doc, ParseResult parse) {

        ParseData parseData = parse.get(URL);
        Metadata metadata = parseData.getMetadata();

        Map<String, Outlink> dedup = new HashMap<String, Outlink>();

        for (Outlink o : parse.getOutlinks()) {
            dedup.put(o.getTargetURL(), o);
        }

        java.net.URL sourceUrl;
        try {
            sourceUrl = new URL(URL);
        } catch (MalformedURLException e1) {
            // we would have known by now as previous components check whether
            // the URL is valid
            LOG.error("MalformedURLException on {}", URL);
            return;
        }

        // applies the XPATH expression in the order in which they are produced
        java.util.Iterator<List<LabelledExpression>> iter = expressions.values().iterator();
        while (iter.hasNext()) {
            List<LabelledExpression> leList = iter.next();
            for (LabelledExpression le : leList) {
                try {
                    List<String> values = le.evaluate(doc);
                    if (values == null || values.isEmpty()) {
                        continue;
                    }
                    for (String target : values) {
                        // resolve URL
                        target = URLUtil.resolveURL(sourceUrl, target).toExternalForm();

                        // apply filtering
                        target = urlFilters.filter(sourceUrl, metadata, target);
                        if (target == null) {
                            continue;
                        }

                        // check whether we already have this link
                        if (dedup.containsKey(target)) {
                            continue;
                        }

                        // create outlink
                        Outlink ol = new Outlink(target);

                        // get the metadata for the outlink from the parent one
                        Metadata metadataOL =
                                metadataTransfer.getMetaForOutlink(target, URL, metadata);

                        ol.setMetadata(metadataOL);
                        dedup.put(ol.getTargetURL(), ol);
                    }
                } catch (Exception e) {
                    LOG.error("Error evaluating {}: {}", le.key, e);
                }
            }
        }

        parse.setOutlinks(new ArrayList(dedup.values()));
    }

    @SuppressWarnings("rawtypes")
    @Override
    public void configure(Map stormConf, JsonNode filterParams) {
        super.configure(stormConf, filterParams);
        this.metadataTransfer = MetadataTransfer.getInstance(stormConf);
        this.urlFilters = URLFilters.fromConf(stormConf);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.parse.filter;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.parse.ParseData;
import com.digitalpebble.stormcrawler.parse.ParseFilter;
import com.digitalpebble.stormcrawler.parse.ParseResult;
import com.fasterxml.jackson.core.JsonPointer;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import javax.xml.xpath.XPath;
import javax.xml.xpath.XPathConstants;
import javax.xml.xpath.XPathExpression;
import javax.xml.xpath.XPathFactory;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.w3c.dom.DocumentFragment;
import org.w3c.dom.Node;

/** Extracts data from JSON-LD representation (https://json-ld.org/) */
public class LDJsonParseFilter extends ParseFilter {

    public static final Logger LOG = LoggerFactory.getLogger(LDJsonParseFilter.class);

    private static XPathFactory factory = XPathFactory.newInstance();
    private static XPath xpath = factory.newXPath();

    private static ObjectMapper mapper = new ObjectMapper();

    private List<LabelledJsonPointer> expressions = new LinkedList<>();

    @Override
    public void filter(String URL, byte[] content, DocumentFragment doc, ParseResult parse) {
        if (doc == null) {
            return;
        }
        try {
            JsonNode json = filterJson(doc);
            if (json == null) {
                return;
            }

            ParseData parseData = parse.get(URL);
            Metadata metadata = parseData.getMetadata();

            // extract patterns and store as metadata
            for (LabelledJsonPointer expression : expressions) {
                JsonNode match = json.at(expression.pointer);
                if (match.isMissingNode()) {
                    continue;
                }
                metadata.addValue(expression.label, match.asText());
            }

        } catch (Exception e) {
            LOG.error("Exception caught when extracting json", e);
        }
    }

    public static JsonNode filterJson(DocumentFragment doc) throws Exception {
        XPathExpression expressionJobPosting =
                xpath.compile("//SCRIPT[@type=\"application/ld+json\"]");
        Node scriptNode = (Node) expressionJobPosting.evaluate(doc, XPathConstants.NODE);
        if (scriptNode == null) {
            return null;
        }
        return mapper.readValue(scriptNode.getTextContent(), JsonNode.class);
    }

    public void configure(@SuppressWarnings("rawtypes") Map stormConf, JsonNode filterParams) {
        java.util.Iterator<Entry<String, JsonNode>> iter = filterParams.fields();
        while (iter.hasNext()) {
            Entry<String, JsonNode> entry = iter.next();
            String key = entry.getKey();
            JsonNode node = entry.getValue();
            LabelledJsonPointer labelP =
                    new LabelledJsonPointer(key, JsonPointer.valueOf(node.asText()));
            expressions.add(labelP);
        }
    }

    class LabelledJsonPointer {

        String label;
        JsonPointer pointer;

        public LabelledJsonPointer(String label, JsonPointer pointer) {
            this.label = label;
            this.pointer = pointer;
        }

        @Override
        public String toString() {
            return label + " => " + pointer.toString();
        }
    }

    @Override
    public boolean needsDOM() {
        return true;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.parse.filter;

import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.parse.ParseFilter;
import com.digitalpebble.stormcrawler.parse.ParseResult;
import com.digitalpebble.stormcrawler.util.URLPartitioner;
import com.fasterxml.jackson.databind.JsonNode;
import java.util.HashMap;
import java.util.Map;
import org.w3c.dom.DocumentFragment;

/** Adds domain (or host) to metadata - can be used later on for indexing * */
public class DomainParseFilter extends ParseFilter {

    private URLPartitioner partitioner;

    private String mdKey = "domain";

    public void configure(@SuppressWarnings("rawtypes") Map stormConf, JsonNode filterParams) {
        JsonNode node = filterParams.get("key");
        if (node != null && node.isTextual()) {
            mdKey = node.asText("domain");
        }

        String partitionMode = Constants.PARTITION_MODE_DOMAIN;

        node = filterParams.get("byHost");
        if (node != null && node.asBoolean()) {
            partitionMode = Constants.PARTITION_MODE_HOST;
        }

        partitioner = new URLPartitioner();
        Map config = new HashMap();
        config.put(Constants.PARTITION_MODEParamName, partitionMode);
        partitioner.configure(config);
    }

    @Override
    public void filter(String URL, byte[] content, DocumentFragment doc, ParseResult parse) {
        Metadata metadata = parse.get(URL).getMetadata();
        String value = partitioner.getPartition(URL, metadata);
        metadata.setValue(mdKey, value);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.parse.filter;

import com.digitalpebble.stormcrawler.JSONResource;
import com.digitalpebble.stormcrawler.parse.ParseFilter;
import com.digitalpebble.stormcrawler.parse.ParseResult;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import com.fasterxml.jackson.core.JsonParseException;
import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.JsonMappingException;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import java.io.IOException;
import java.io.InputStream;
import java.util.HashSet;
import java.util.Map;
import java.util.Set;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.w3c.dom.DocumentFragment;

/**
 * Assigns one or more tags to the metadata of a document based on its URL matching patterns defined
 * in a JSON resource file.
 *
 * <p>The resource file must specifify regular expressions for inclusions but also for exclusions
 * e.g.
 *
 * <pre>
 * {
 *   "collections": [{
 *            "name": "stormcrawler",
 *            "includePatterns": ["http://stormcrawler.net/.+"]
 *        },
 *        {
 *            "name": "crawler",
 *            "includePatterns": [".+crawler.+", ".+nutch.+"],
 *            "excludePatterns": [".+baby.+", ".+spider.+"]
 *        }
 *    ]
 * }
 * </pre>
 *
 * @see <a href=
 *     "https://www.google.com/support/enterprise/static/gsa/docs/admin/74/admin_console_help/crawl_collections.html">collections
 *     in Google Search Appliance</a>
 *     <p>This resources was kindly donated by the Government of Northwestern Territories in Canada
 *     (http://www.gov.nt.ca/).
 */
public class CollectionTagger extends ParseFilter implements JSONResource {

    private static final Logger LOG = LoggerFactory.getLogger(CollectionTagger.class);

    private static final ObjectMapper objectMapper = new ObjectMapper();
    private static final TypeReference<Collections> reference = new TypeReference<Collections>() {};

    private Collections collections = new Collections();

    private String key;
    private String resourceFile;

    public void configure(@SuppressWarnings("rawtypes") Map stormConf, JsonNode filterParams) {

        if (filterParams != null) {
            JsonNode node = filterParams.get("key");
            if (node != null && node.isTextual()) {
                this.key = node.asText("collections");
            }
            node = filterParams.get("file");
            if (node != null && node.isTextual()) {
                this.resourceFile = node.asText("collections.json");
            }
        }

        // config via json failed - trying from global config
        if (this.key == null) {
            this.key = ConfUtils.getString(stormConf, "collections.key", "collections");
        }
        if (this.resourceFile == null) {
            this.resourceFile =
                    ConfUtils.getString(stormConf, "collections.file", "collections.json");
        }

        try {
            loadJSONResources();
        } catch (Exception e) {
            LOG.error("Exception while loading JSON resources from jar", e);
            throw new RuntimeException(e);
        }
    }

    @Override
    public String getResourceFile() {
        return resourceFile;
    }

    @Override
    public void loadJSONResources(InputStream inputStream)
            throws JsonParseException, JsonMappingException, IOException {
        collections = (Collections) objectMapper.readValue(inputStream, reference);
    }

    @Override
    public void filter(String URL, byte[] content, DocumentFragment doc, ParseResult parse) {
        String[] tags = collections.tag(URL);
        if (tags.length > 0) {
            parse.get(URL).getMetadata().setValues(key, tags);
        }
    }
}

class Collections {

    private Set<Collection> collections;

    public void setCollections(Set<Collection> collections) {
        this.collections = collections;
    }

    public String[] tag(String url) {
        Set<String> tags = new HashSet<String>();
        for (Collection collection : collections) {
            if (collection.matches(url)) {
                tags.add(collection.getName());
            }
        }
        return tags.toArray(new String[tags.size()]);
    }
}

class Collection {

    private String name;
    private Set<Pattern> includePatterns;
    private Set<Pattern> excludePatterns;

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    /** @return true if the URL matches a pattern for this collection and no exclusion patterns */
    public boolean matches(String url) {
        boolean matches = false;
        for (Pattern includeP : includePatterns) {
            Matcher m = includeP.matcher(url);
            if (m.matches()) {
                matches = true;
                break;
            }
        }
        // no match
        if (!matches) {
            return false;
        }

        if (excludePatterns == null) {
            return true;
        }

        // check for antipatterns
        for (Pattern excludeP : excludePatterns) {
            Matcher m = excludeP.matcher(url);
            if (m.matches()) {
                return false;
            }
        }

        return true;
    }

    public void setIncludePatterns(Set<Pattern> includePatterns) {
        this.includePatterns = includePatterns;
    }

    public void setExcludePatterns(Set<Pattern> excludePatterns) {
        this.excludePatterns = excludePatterns;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.parse.filter;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.parse.ParseFilter;
import com.digitalpebble.stormcrawler.parse.ParseResult;
import org.apache.storm.shade.org.apache.commons.lang.StringUtils;
import org.w3c.dom.DocumentFragment;

/**
 * Normalises the MimeType value e.g. text/html; charset=UTF-8 => HTML application/pdf => PDF and
 * creates a new entry with a key 'format' in the metadata. Requires the JSoupParserBolt to be used
 * with the configuration _detect.mimetype_ set to true.
 */
public class MimeTypeNormalization extends ParseFilter {

    @Override
    public void filter(String url, byte[] content, DocumentFragment doc, ParseResult parse) {

        Metadata m = parse.get(url).getMetadata();
        String ct = m.getFirstValue("parse.Content-Type");
        if (StringUtils.isBlank(ct)) {
            ct = "unknown";
        } else if (ct.toLowerCase().contains("html")) {
            ct = "html";
        } else if (ct.toLowerCase().contains("pdf")) {
            ct = "pdf";
        } else if (ct.toLowerCase().contains("word")) {
            ct = "word";
        } else if (ct.toLowerCase().contains("excel")) {
            ct = "excel";
        } else if (ct.toLowerCase().contains("powerpoint")) {
            ct = "powerpoint";
        } else if (ct.toLowerCase().startsWith("video/")) {
            ct = "video";
        } else if (ct.toLowerCase().startsWith("image/")) {
            ct = "image";
        } else if (ct.toLowerCase().startsWith("audio/")) {
            ct = "audio";
        } else {
            ct = "other";
        }
        m.setValue("format", ct);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.parse.filter;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.parse.ParseData;
import com.digitalpebble.stormcrawler.parse.ParseFilter;
import com.digitalpebble.stormcrawler.parse.ParseResult;
import com.fasterxml.jackson.databind.JsonNode;
import java.nio.charset.StandardCharsets;
import java.util.Map;
import org.apache.commons.codec.digest.DigestUtils;
import org.apache.storm.shade.org.apache.commons.lang.StringUtils;
import org.w3c.dom.DocumentFragment;

/**
 * Computes a signature for a page, based on the binary content or text. If the content is empty,
 * the URL is used.
 *
 * <p>Configuration properties:
 *
 * <dl>
 *   <dt>useText
 *   <dd>compute signature on plain text, instead of binary content
 *   <dt>keyName
 *   <dd>name of the metadata field to hold the signature (default: &quot;signature&quot;)
 *   <dt>keyNameCopy
 *   <dd>name of the metadata field to hold a temporary copy of the signature used to decide by
 *       signature comparison whether the document has changed. If not defined or empty, the
 *       signature is not copied.
 * </dl>
 */
public class MD5SignatureParseFilter extends ParseFilter {

    private String key_name = "signature";

    private boolean useText = false;

    private String copyKeyName = null;

    @Override
    public void filter(String URL, byte[] content, DocumentFragment doc, ParseResult parse) {
        ParseData parseData = parse.get(URL);
        Metadata metadata = parseData.getMetadata();
        if (copyKeyName != null) {
            String signature = metadata.getFirstValue(key_name);
            if (signature != null) {
                metadata.setValue(copyKeyName, signature);
            }
        }
        byte[] data = null;
        if (useText) {
            String text = parseData.getText();
            if (StringUtils.isNotBlank(text)) {
                data = text.getBytes(StandardCharsets.UTF_8);
            }
        } else {
            data = content;
        }
        if (data == null) {
            data = URL.getBytes(StandardCharsets.UTF_8);
        }
        String hex = DigestUtils.md5Hex(data);
        metadata.setValue(key_name, hex);
    }

    @SuppressWarnings("rawtypes")
    @Override
    public void configure(Map stormConf, JsonNode filterParams) {
        JsonNode node = filterParams.get("useText");
        if (node != null && node.asBoolean()) {
            useText = true;
        }
        node = filterParams.get("keyName");
        if (node != null && node.isTextual()) {
            key_name = node.asText("signature");
        }
        node = filterParams.get("keyNameCopy");
        if (node != null && node.isTextual() && StringUtils.isNotBlank(node.asText(""))) {
            copyKeyName = node.asText("signatureOld");
        }
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.parse;

import com.digitalpebble.stormcrawler.JSONResource;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import com.digitalpebble.stormcrawler.util.Configurable;
import com.fasterxml.jackson.core.JsonParseException;
import com.fasterxml.jackson.databind.JsonMappingException;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import java.io.IOException;
import java.io.InputStream;
import java.net.URL;
import java.util.List;
import java.util.Map;
import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.CommandLineParser;
import org.apache.commons.cli.DefaultParser;
import org.apache.commons.cli.Options;
import org.apache.commons.cli.ParseException;
import org.apache.commons.io.IOUtils;
import org.apache.commons.lang.StringUtils;
import org.apache.storm.Config;
import org.apache.storm.utils.Utils;
import org.jsoup.Jsoup;
import org.jsoup.nodes.Document;
import org.slf4j.LoggerFactory;
import org.w3c.dom.DocumentFragment;

/** Wrapper for the ParseFilters defined in a JSON configuration */
public class ParseFilters extends ParseFilter implements JSONResource {

    public static final ParseFilters emptyParseFilter = new ParseFilters();

    private static final org.slf4j.Logger LOG = LoggerFactory.getLogger(ParseFilters.class);

    private ParseFilter[] filters;

    private ParseFilters() {
        filters = new ParseFilter[0];
    }

    private String configFile;

    private Map stormConf;

    /**
     * Loads and configure the ParseFilters based on the storm config if there is one otherwise
     * returns an emptyParseFilter.
     */
    @SuppressWarnings("rawtypes")
    public static ParseFilters fromConf(Map stormConf) {
        String parseconfigfile = ConfUtils.getString(stormConf, "parsefilters.config.file");
        if (StringUtils.isNotBlank(parseconfigfile)) {
            try {
                return new ParseFilters(stormConf, parseconfigfile);
            } catch (IOException e) {
                String message =
                        "Exception caught while loading the ParseFilters from " + parseconfigfile;
                LOG.error(message);
                throw new RuntimeException(message, e);
            }
        }

        return ParseFilters.emptyParseFilter;
    }

    /**
     * loads the filters from a JSON configuration file
     *
     * @throws IOException
     */
    @SuppressWarnings("rawtypes")
    public ParseFilters(Map stormConf, String configFile) throws IOException {
        this.configFile = configFile;
        this.stormConf = stormConf;
        try {
            loadJSONResources();
        } catch (Exception e) {
            throw new IOException("Unable to build JSON object from file", e);
        }
    }

    @Override
    public void loadJSONResources(InputStream inputStream)
            throws JsonParseException, JsonMappingException, IOException {
        ObjectMapper mapper = new ObjectMapper();
        JsonNode confNode = mapper.readValue(inputStream, JsonNode.class);
        configure(stormConf, confNode);
    }

    @Override
    public String getResourceFile() {
        return this.configFile;
    }

    @SuppressWarnings("rawtypes")
    @Override
    public void configure(Map stormConf, JsonNode filtersConf) {
        List<ParseFilter> list =
                Configurable.configure(
                        stormConf, filtersConf, ParseFilter.class, this.getClass().getName());
        filters = list.toArray(new ParseFilter[list.size()]);
    }

    @Override
    public boolean needsDOM() {
        for (ParseFilter filter : filters) {
            boolean needsDOM = filter.needsDOM();
            if (needsDOM) {
                return true;
            }
        }
        return false;
    }

    @Override
    public void filter(String URL, byte[] content, DocumentFragment doc, ParseResult parse) {

        for (ParseFilter filter : filters) {
            long start = System.currentTimeMillis();
            if (doc == null && filter.needsDOM()) {
                LOG.info(
                        "ParseFilter {} needs DOM but has none to work on - skip : {}",
                        filter.getClass().getName(),
                        URL);
                continue;
            }
            filter.filter(URL, content, doc, parse);
            long end = System.currentTimeMillis();
            LOG.debug("ParseFilter {} took {} msec", filter.getClass().getName(), end - start);
        }
    }

    /**
     * * Used for quick testing + debugging
     *
     * @since 1.17
     * @throws ParseException
     */
    public static void main(String[] args) throws IOException, ParseException {

        Config conf = new Config();

        // loads the default configuration file
        Map defaultSCConfig = Utils.findAndReadConfigFile("crawler-default.yaml", false);
        conf.putAll(ConfUtils.extractConfigElement(defaultSCConfig));

        Options options = new Options();
        options.addOption("c", true, "stormcrawler configuration file");

        CommandLineParser parser = new DefaultParser();
        CommandLine cmd = parser.parse(options, args);

        if (cmd.hasOption("c")) {
            String confFile = cmd.getOptionValue("c");
            ConfUtils.loadConf(confFile, conf);
        }

        ParseFilters filters = ParseFilters.fromConf(conf);

        System.out.println(filters.filters.length + " filters found");

        ParseResult parse = new ParseResult();

        String url = cmd.getArgs()[0];

        byte[] content = IOUtils.toByteArray((new URL(url)).openStream());

        Document doc = Jsoup.parse(new String(content), url);

        filters.filter(url, content, DocumentFragmentBuilder.fromJsoup(doc), parse);

        System.out.println(parse.toString());

        System.exit(0);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.parse;

import com.digitalpebble.stormcrawler.Metadata;

public class ParseData {
    private byte[] content;
    private String text;
    private Metadata metadata;

    public ParseData() {
        this.metadata = new Metadata();
    }

    public ParseData(String text, Metadata metadata) {
        this.text = text;
        this.metadata = metadata;
        this.content = new byte[0];
    }

    public ParseData(Metadata md) {
        this.metadata = md;
    }

    public String getText() {
        return text;
    }

    public Metadata getMetadata() {
        return metadata;
    }

    public void setText(String text) {
        this.text = text;
    }

    public void setMetadata(Metadata metadata) {
        this.metadata = metadata;
    }

    public byte[] getContent() {
        return content;
    }

    public void setContent(byte[] content) {
        this.content = content;
    }

    public void put(String key, String value) {
        metadata.addValue(key, value);
    }

    public String get(String key) {
        return metadata.getFirstValue(key);
    }

    public String[] getValues(String key) {
        return metadata.getValues(key);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.parse;

import com.digitalpebble.stormcrawler.Metadata;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.Iterator;
import java.util.List;
import java.util.Map;

public class ParseResult implements Iterable<Map.Entry<String, ParseData>> {

    private List<Outlink> outlinks;
    private final Map<String, ParseData> parseMap;

    public ParseResult() {
        this(new HashMap<String, ParseData>(), new ArrayList<>());
    }

    public ParseResult(List<Outlink> links) {
        this(new HashMap<String, ParseData>(), links);
    }

    public ParseResult(Map<String, ParseData> map) {
        this(map, new ArrayList<>());
    }

    public ParseResult(Map<String, ParseData> map, List<Outlink> links) {
        if (map == null) {
            throw new NullPointerException();
        }
        parseMap = map;
        outlinks = links;
    }

    public boolean isEmpty() {
        return parseMap.isEmpty();
    }

    public int size() {
        return parseMap.size();
    }

    public List<Outlink> getOutlinks() {
        return outlinks;
    }

    public void setOutlinks(List<Outlink> outlinks) {
        this.outlinks = outlinks;
    }

    /**
     * @return An existent instance of Parse for the given URL or an empty one if none can be found,
     *     useful to avoid unnecessary checks in the parse plugins
     */
    public ParseData get(String URL) {
        ParseData parse = parseMap.get(URL);
        if (parse == null) {
            parse = new ParseData();
            parseMap.put(URL, parse);
            return parse;
        }
        return parse;
    }

    public String[] getValues(String URL, String key) {
        ParseData parseInfo = parseMap.get(URL);
        if (parseInfo == null) {
            return null;
        }
        return parseInfo.getValues(key);
    }

    /** Add the key value to the metadata object for a given URL * */
    public void put(String URL, String key, String value) {
        get(URL).getMetadata().addValue(key, value);
    }

    /** Set the metadata for a given URL * */
    public void set(String URL, Metadata metadata) {
        get(URL).setMetadata(metadata);
    }

    public Map<String, ParseData> getParseMap() {
        return parseMap;
    }

    @Override
    public Iterator<Map.Entry<String, ParseData>> iterator() {
        return parseMap.entrySet().iterator();
    }

    @Override
    public String toString() {
        StringBuilder sb = new StringBuilder();

        sb.append("METADATA\n");

        parseMap.forEach(
                (k, v) ->
                        sb.append(k).append(": ").append(v.getMetadata().toString()).append("\n"));

        sb.append("\nOUTLINKS\n");

        outlinks.forEach(k -> sb.append(k.toString()).append("\n"));

        return sb.toString();
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.spout;

import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.persistence.Status;
import com.digitalpebble.stormcrawler.util.StringTabScheme;
import java.io.BufferedReader;
import java.io.FileInputStream;
import java.io.IOException;
import java.io.InputStreamReader;
import java.nio.ByteBuffer;
import java.nio.charset.StandardCharsets;
import java.nio.file.DirectoryStream;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.Queue;
import org.apache.commons.lang.StringUtils;
import org.apache.storm.spout.Scheme;
import org.apache.storm.spout.SpoutOutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.topology.base.BaseRichSpout;
import org.apache.storm.tuple.Fields;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Reads the lines from a UTF-8 file and use them as a spout. Load the entire content into memory.
 * Uses StringTabScheme to parse the lines into URLs and Metadata, generates tuples on the default
 * stream unless withDiscoveredStatus is set to true.
 */
@SuppressWarnings("serial")
public class FileSpout extends BaseRichSpout {

    public static final int BATCH_SIZE = 10000;
    public static final Logger LOG = LoggerFactory.getLogger(FileSpout.class);

    protected SpoutOutputCollector _collector;

    private Queue<String> _inputFiles;
    private BufferedReader currentBuffer;

    protected Scheme _scheme = new StringTabScheme();

    protected LinkedList<byte[]> buffer = new LinkedList<>();
    protected boolean active;
    private boolean withDiscoveredStatus = false;

    /**
     * @param directory containing the seed files
     * @param filter to apply on the file names
     */
    public FileSpout(String dir, String filter) {
        this(dir, filter, false);
    }

    /** @param files containing the URLs */
    public FileSpout(String... files) {
        this(false, files);
    }

    /**
     * @param withDiscoveredStatus whether the tuples generated should contain a Status field with
     *     DISCOVERED as value and be emitted on the status stream
     * @param directory containing the seed files
     * @param filter to apply on the file names
     * @since 1.13
     */
    public FileSpout(String dir, String filter, boolean withDiscoveredStatus) {
        this.withDiscoveredStatus = withDiscoveredStatus;
        Path pdir = Paths.get(dir);
        _inputFiles = new LinkedList<>();
        LOG.info("Reading directory: {} (filter: {})", pdir, filter);
        try (DirectoryStream<Path> stream = Files.newDirectoryStream(pdir, filter)) {
            for (Path entry : stream) {
                String inputFile = entry.toAbsolutePath().toString();
                _inputFiles.add(inputFile);
                LOG.info("Input : {}", inputFile);
            }
        } catch (IOException ioe) {
            LOG.error("IOException: %s%n", ioe);
        }
    }

    /**
     * @param withDiscoveredStatus whether the tuples generated should contain a Status field with
     *     DISCOVERED as value and be emitted on the status stream
     * @param files containing the URLs
     * @since 1.13
     */
    public FileSpout(boolean withDiscoveredStatus, String... files) {
        this.withDiscoveredStatus = withDiscoveredStatus;
        if (files.length == 0) {
            throw new IllegalArgumentException("Must configure at least one inputFile");
        }
        _inputFiles = new LinkedList<>();
        for (String f : files) {
            _inputFiles.add(f);
        }
    }

    /**
     * Specify a Scheme for parsing the lines into URLs and Metadata. StringTabScheme is used by
     * default. The Scheme must generate a String for the URL and a Metadata object.
     *
     * @since 1.13
     */
    public void setScheme(Scheme scheme) {
        _scheme = scheme;
    }

    protected void populateBuffer() throws IOException {
        if (currentBuffer == null) {
            String file = _inputFiles.poll();
            if (file == null) return;
            Path inputPath = Paths.get(file);
            currentBuffer =
                    new BufferedReader(
                            new InputStreamReader(
                                    new FileInputStream(inputPath.toFile()),
                                    StandardCharsets.UTF_8));
        }

        // no more files to read from
        if (currentBuffer == null) return;

        String line = null;
        int linesRead = 0;
        while (linesRead < BATCH_SIZE && (line = currentBuffer.readLine()) != null) {
            if (StringUtils.isBlank(line)) continue;
            if (line.startsWith("#")) continue;
            buffer.add(line.trim().getBytes(StandardCharsets.UTF_8));
            linesRead++;
        }

        // finished the file?
        if (line == null) {
            currentBuffer.close();
            currentBuffer = null;
        }
    }

    @SuppressWarnings("rawtypes")
    @Override
    public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) {
        _collector = collector;
        try {
            populateBuffer();
        } catch (IOException e) {
            throw new RuntimeException(e);
        }
    }

    @Override
    public void nextTuple() {
        if (!active) return;

        if (buffer.isEmpty()) {
            try {
                populateBuffer();
            } catch (IOException e) {
                throw new RuntimeException(e);
            }
        }

        // still empty?
        if (buffer.isEmpty()) return;

        byte[] head = buffer.removeFirst();
        List<Object> fields = this._scheme.deserialize(ByteBuffer.wrap(head));

        if (withDiscoveredStatus) {
            fields.add(Status.DISCOVERED);
            this._collector.emit(Constants.StatusStreamName, fields, head);
        } else {
            this._collector.emit(fields, head);
        }
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(_scheme.getOutputFields());
        if (withDiscoveredStatus) {
            // add status field to output
            List<String> s = _scheme.getOutputFields().toList();
            s.add("status");
            declarer.declareStream(Constants.StatusStreamName, new Fields(s));
        }
    }

    @Override
    public void close() {}

    @Override
    public void activate() {
        super.activate();
        active = true;
    }

    @Override
    public void deactivate() {
        super.deactivate();
        active = false;
    }

    @Override
    public void ack(Object msgId) {}

    @Override
    public void fail(Object msgId) {
        if (msgId instanceof byte[]) {
            String msg = new String((byte[]) msgId);
            LOG.error("Failed - adding back to the queue: {}", msg);
            buffer.add((byte[]) msgId);
        } else {
            // unknown object type from extending class
            LOG.error(
                    "Failed - unknown message ID type `{}': {}",
                    msgId.getClass().getCanonicalName(),
                    msgId);
            throw new IllegalStateException(
                    "Unknown message ID type: " + msgId.getClass().getCanonicalName());
        }
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.spout;

import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.persistence.Status;
import com.digitalpebble.stormcrawler.util.StringTabScheme;
import java.nio.ByteBuffer;
import java.nio.charset.StandardCharsets;
import java.util.Date;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.PriorityQueue;
import org.apache.storm.spout.SpoutOutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.topology.base.BaseRichSpout;
import org.apache.storm.tuple.Fields;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Stores URLs in memory. Useful for testing and debugging in local mode or with a single worker.
 * Uses StringTabScheme to parse the lines into URLs and Metadata, generates tuples on the default
 * stream unless withDiscoveredStatus is set to true. Can be used with the MemoryStatusUpdater to
 * receive discovered URLs and emulate a recursive crawl.
 */
@SuppressWarnings("serial")
public class MemorySpout extends BaseRichSpout {

    private static final Logger LOG = LoggerFactory.getLogger(MemorySpout.class);

    private SpoutOutputCollector _collector;
    private StringTabScheme scheme = new StringTabScheme();
    private boolean active = true;

    private boolean withDiscoveredStatus = false;

    private static PriorityQueue<ScheduledURL> queue = new PriorityQueue<>();

    private String[] startingURLs;

    public MemorySpout(String... urls) {
        this(false, urls);
    }

    /**
     * Emits tuples with DISCOVERED status, which is useful when injecting seeds directly to a
     * statusupdaterbolt.
     *
     * @param withDiscoveredStatus whether the tuples generated should contain a Status field with
     *     DISCOVERED as value and be emitted on the status stream
     */
    public MemorySpout(boolean withDiscoveredStatus, String... urls) {
        this.withDiscoveredStatus = withDiscoveredStatus;
        startingURLs = urls;
    }

    /**
     * Add a new URL
     *
     * @param nextFetch
     */
    public static void add(String url, Metadata md, Date nextFetch) {
        LOG.debug("Adding {} with md {} and nextFetch {}", url, md, nextFetch);
        ScheduledURL tuple = new ScheduledURL(url, md, nextFetch);
        synchronized (queue) {
            queue.add(tuple);
        }
    }

    @Override
    public void open(
            @SuppressWarnings("rawtypes") Map conf,
            TopologyContext context,
            SpoutOutputCollector collector) {
        _collector = collector;

        // check that there is only one instance of it
        int totalTasks = context.getComponentTasks(context.getThisComponentId()).size();
        if (totalTasks > 1) {
            throw new RuntimeException("Can't have more than one instance of the MemorySpout");
        }

        Date now = new Date();
        for (String u : startingURLs) {
            LOG.debug("About to deserialize {} ", u);
            List<Object> tuple =
                    scheme.deserialize(ByteBuffer.wrap(u.getBytes(StandardCharsets.UTF_8)));
            add((String) tuple.get(0), (Metadata) tuple.get(1), now);
        }
        context.registerMetric("queue_size", () -> queue.size(), 10);
    }

    @Override
    public void nextTuple() {
        if (!active) return;

        synchronized (queue) {
            // removes the URL
            ScheduledURL tuple = queue.poll();
            if (tuple == null) return;

            // check whether it is due for fetching
            if (tuple.nextFetchDate.after(new Date())) {
                LOG.debug("Tuple {} not ready for fetching", tuple.URL);

                // put it back and wait
                queue.add(tuple);
                return;
            }

            List<Object> tobs = new LinkedList<>();
            tobs.add(tuple.URL);
            tobs.add(tuple.m);

            if (withDiscoveredStatus) {
                tobs.add(Status.DISCOVERED);
                _collector.emit(Constants.StatusStreamName, tobs, tuple.URL);
            } else {
                _collector.emit(tobs, tuple.URL);
            }
        }
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(scheme.getOutputFields());
        if (withDiscoveredStatus) {
            // add status field to output
            List<String> s = scheme.getOutputFields().toList();
            s.add("status");
            declarer.declareStream(Constants.StatusStreamName, new Fields(s));
        }
    }

    @Override
    public void activate() {
        super.activate();
        active = true;
    }

    @Override
    public void deactivate() {
        super.deactivate();
        active = false;
    }
}

class ScheduledURL implements Comparable<ScheduledURL> {
    Date nextFetchDate;
    String URL;
    Metadata m;

    ScheduledURL(String URL, Metadata m, Date nextFetchDate) {
        this.nextFetchDate = nextFetchDate;
        this.URL = URL;
        this.m = m;
    }

    @Override
    public String toString() {
        return URL + "\t" + nextFetchDate;
    }

    @Override
    /** Sort by next fetch date then URl * */
    public int compareTo(ScheduledURL o) {
        // compare the URL
        int compString = URL.compareTo(o.URL);
        if (compString == 0) return 0;

        // compare the date
        int comp = nextFetchDate.compareTo(o.nextFetchDate);
        if (comp != 0) return comp;

        return compString;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler;

import com.esotericsoftware.kryo.serializers.DefaultArraySerializers.StringArraySerializer;
import com.esotericsoftware.kryo.serializers.DefaultSerializers.StringSerializer;
import com.esotericsoftware.kryo.serializers.MapSerializer.BindMap;
import java.util.ArrayList;
import java.util.Collection;
import java.util.Collections;
import java.util.ConcurrentModificationException;
import java.util.HashMap;
import java.util.Iterator;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Set;
import org.apache.commons.lang.StringUtils;

/** Wrapper around Map &lt;String,String[]&gt; * */
public class Metadata {

    // customize the behaviour of Kryo via annotations
    @BindMap(
            valueSerializer = StringArraySerializer.class,
            keySerializer = StringSerializer.class,
            valueClass = String[].class,
            keyClass = String.class,
            keysCanBeNull = false)
    private Map<String, String[]> md;

    public static final Metadata empty = new Metadata(Collections.<String, String[]>emptyMap());

    public Metadata() {
        md = new HashMap<>();
    }

    private transient boolean locked = false;

    /** Wraps an existing HashMap into a Metadata object - does not clone the content */
    public Metadata(Map<String, String[]> metadata) {
        if (metadata == null) throw new NullPointerException();
        md = metadata;
    }

    /** Puts all the metadata into the current instance * */
    public void putAll(Metadata m) {
        checkLockException();

        md.putAll(m.md);
    }

    /**
     * Puts all prefixed metadata into the current instance
     *
     * @param m metadata to be added
     * @param prefix string to prefix keys in m before adding them to the current metadata. No
     *     separator is inserted between prefix and original key, so the prefix must include any
     *     separator (eg. a dot)
     */
    public void putAll(Metadata m, String prefix) {
        if (prefix == null || prefix.isEmpty()) {
            putAll(m);
            return;
        }

        Map<String, String[]> ma = m.asMap();
        ma.forEach(
                (k, v) -> {
                    setValues(prefix + k, v);
                });
    }

    /** @return the first value for the key or null if it does not exist * */
    public String getFirstValue(String key) {
        String[] values = md.get(key);
        if (values == null) return null;
        if (values.length == 0) return null;
        return values[0];
    }

    /** @return the first value for the key or null if it does not exist, given a prefix */
    public String getFirstValue(String key, String prefix) {
        if (prefix == null || prefix.length() == 0) return getFirstValue(key);
        return getFirstValue(prefix + key);
    }

    public String[] getValues(String key, String prefix) {
        if (prefix == null || prefix.length() == 0) return getValues(key);
        return getValues(prefix + key);
    }

    public String[] getValues(String key) {
        String[] values = md.get(key);
        if (values == null) return null;
        if (values.length == 0) return null;
        return values;
    }

    public boolean containsKey(String key) {
        return md.containsKey(key);
    }

    public boolean containsKeyWithValue(String key, String value) {
        String[] values = getValues(key);
        if (values == null) return false;
        for (String s : values) {
            if (s.equals(value)) return true;
        }
        return false;
    }

    /** Set the value for a given key. The value can be null. */
    public void setValue(String key, String value) {
        checkLockException();

        md.put(key, new String[] {value});
    }

    public void setValues(String key, String[] values) {
        checkLockException();

        if (values == null || values.length == 0) return;
        md.put(key, values);
    }

    public void addValue(String key, String value) {
        checkLockException();

        if (StringUtils.isBlank(value)) return;

        String[] existingvals = md.get(key);
        if (existingvals == null || existingvals.length == 0) {
            setValue(key, value);
            return;
        }

        int currentLength = existingvals.length;
        String[] newvals = new String[currentLength + 1];
        newvals[currentLength] = value;
        System.arraycopy(existingvals, 0, newvals, 0, currentLength);
        md.put(key, newvals);
    }

    public void addValues(String key, Collection<String> values) {
        checkLockException();

        if (values == null || values.size() == 0) return;
        String[] existingvals = md.get(key);
        if (existingvals == null) {
            md.put(key, values.toArray(new String[values.size()]));
            return;
        }

        ArrayList<String> existing = new ArrayList<>(existingvals.length + values.size());
        for (String v : existingvals) existing.add(v);

        existing.addAll(values);
        md.put(key, existing.toArray(new String[existing.size()]));
    }

    /** @return the previous value(s) associated with <tt>key</tt> */
    public String[] remove(String key) {
        checkLockException();
        return md.remove(key);
    }

    public String toString() {
        return toString("");
    }

    /** Returns a String representation of the metadata with one K/V per line */
    public String toString(String prefix) {
        StringBuilder sb = new StringBuilder();
        if (prefix == null) prefix = "";
        Iterator<Entry<String, String[]>> iter = md.entrySet().iterator();
        while (iter.hasNext()) {
            Entry<String, String[]> entry = iter.next();
            for (String val : entry.getValue()) {
                sb.append(prefix).append(entry.getKey()).append(": ").append(val).append("\n");
            }
        }
        return sb.toString();
    }

    public int size() {
        return md.size();
    }

    public Set<String> keySet() {
        return md.keySet();
    }

    /** Returns the first non empty value found for the keys or null if none found. */
    public static String getFirstValue(Metadata md, String... keys) {
        for (String key : keys) {
            String val = md.getFirstValue(key);
            if (StringUtils.isBlank(val)) continue;
            return val;
        }
        return null;
    }

    /** Returns the underlying Map * */
    public Map<String, String[]> asMap() {
        return md;
    }

    /**
     * Prevents modifications to the metadata object. Useful for debugging modifications of the
     * metadata after they have been serialized. Instead of choking when serializing, a
     * ConcurrentModificationException will be thrown where the metadata are modified. <br>
     * Use like this in any bolt where you see java.lang.RuntimeException:
     * com.esotericsoftware.kryo.KryoException: java.util.ConcurrentModificationException<br>
     * collector.emit(StatusStreamName, tuple, new Values(url, metadata.lock(), Status.FETCHED));
     *
     * @since 1.16
     */
    public Metadata lock() {
        locked = true;
        return this;
    }

    /**
     * Release the lock on a metadata
     *
     * @since 1.16
     */
    public Metadata unlock() {
        locked = false;
        return this;
    }

    /** @since 1.16 */
    private final void checkLockException() {
        if (locked)
            throw new ConcurrentModificationException(
                    "Attempt to modify a metadata after it has been sent to the serializer");
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.persistence.urlbuffer;

import java.util.Iterator;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Queue;
import org.apache.storm.tuple.Values;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Simple implementation of a URLBuffer which rotates on the queues without applying any priority.
 *
 * @since 1.15
 */
public class SimpleURLBuffer extends AbstractURLBuffer {

    static final Logger LOG = LoggerFactory.getLogger(SimpleURLBuffer.class);

    /**
     * Retrieves the next available URL, guarantees that the URLs are always perfectly shuffled
     *
     * @return null if no entries are available
     */
    public synchronized Values next() {

        if (queues.isEmpty()) {
            return null;
        }

        Iterator<Entry<String, Queue<URLMetadata>>> i = queues.entrySet().iterator();

        if (!i.hasNext()) {
            LOG.debug("Empty iterator");
            return null;
        }

        Map.Entry<String, Queue<URLMetadata>> nextEntry = i.next();

        Queue<URLMetadata> queue = nextEntry.getValue();
        String queueName = nextEntry.getKey();

        // remove the entry
        i.remove();

        LOG.debug("Next queue {}", queueName);

        // remove the first element
        URLMetadata item = queue.poll();

        LOG.debug("Item {}", item.url);

        // any left? add to the end of the iterator
        if (!queue.isEmpty()) {
            LOG.debug("adding to the back of the queue {}", queueName);
            queues.put(queueName, queue);
        }
        // notify that the queue is empty
        else {
            if (listener != null) {
                listener.emptyQueue(queueName);
            }
        }

        // remove it from the list of URLs in the queue
        in_buffer.remove(item.url);
        return new Values(item.url, item.metadata);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.persistence.urlbuffer;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.persistence.EmptyQueueListener;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import java.util.Map;
import org.apache.commons.lang.StringUtils;
import org.apache.storm.tuple.Values;

/**
 * Buffers URLs to be processed into separate queues; used by spouts. Guarantees that no URL can be
 * put in the buffer more than once.
 *
 * <p>Configured by setting
 *
 * <p>urlbuffer.class: "com.digitalpebble.stormcrawler.persistence.SimpleURLBuffer"
 *
 * <p>in the configuration
 *
 * @since 1.15
 */
public interface URLBuffer {

    /** Implementation to use for URLBuffer. Must implement the interface URLBuffer. */
    public static final String bufferClassParamName = "urlbuffer.class";

    /**
     * Stores the URL and its Metadata under a given key.
     *
     * <p>Implementations of this method should be synchronised
     *
     * @return false if the URL was already in the buffer, true if it wasn't and was added
     */
    public abstract boolean add(String URL, Metadata m, String key);

    /**
     * Stores the URL and its Metadata using the hostname as key.
     *
     * <p>Implementations of this method should be synchronised
     *
     * @return false if the URL was already in the buffer, true if it wasn't and was added
     */
    public default boolean add(String URL, Metadata m) {
        return add(URL, m, null);
    }

    /** Total number of URLs in the buffer * */
    public abstract int size();

    /** Total number of queues in the buffer * */
    public abstract int numQueues();

    /**
     * Retrieves the next available URL, guarantees that the URLs are always perfectly shuffled
     *
     * <p>Implementations of this method should be synchronised
     */
    public abstract Values next();

    /** Implementations of this method should be synchronised */
    public abstract boolean hasNext();

    public abstract void setEmptyQueueListener(EmptyQueueListener l);

    /**
     * Notify the buffer that a URL has been successfully processed used e.g to compute an ideal
     * delay for a host queue
     */
    public default void acked(String url) {
        // do nothing with the information about URLs being acked
    }

    public default void configure(Map stormConf) {}

    /** Returns a URLBuffer instance based on the configuration * */
    @SuppressWarnings({"rawtypes", "unchecked"})
    public static URLBuffer getInstance(Map stormConf) {
        URLBuffer buffer;

        String className = ConfUtils.getString(stormConf, bufferClassParamName);

        if (StringUtils.isBlank(className)) {
            throw new RuntimeException("Missing value for config  " + bufferClassParamName);
        }

        try {
            Class<?> bufferclass = Class.forName(className);
            boolean interfaceOK = URLBuffer.class.isAssignableFrom(bufferclass);
            if (!interfaceOK) {
                throw new RuntimeException("Class " + className + " must extend URLBuffer");
            }
            buffer = (URLBuffer) bufferclass.newInstance();
            buffer.configure(stormConf);
        } catch (Exception e) {
            throw new RuntimeException("Can't instanciate " + className);
        }

        return buffer;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.persistence.urlbuffer;

import com.digitalpebble.stormcrawler.Metadata;
import java.time.Instant;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Queue;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.atomic.AtomicInteger;
import org.apache.storm.tuple.Values;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Determines the priority of the buffers based on the number of URLs acked in a configurable period
 * of time.
 *
 * @since 1.16
 */
public class PriorityURLBuffer extends SimpleURLBuffer {

    static final Logger LOG = LoggerFactory.getLogger(PriorityURLBuffer.class);

    private Map<String, AtomicInteger> ackCount;

    private Instant lastSorting;

    private long minDelayReRankSec = 10l;

    public PriorityURLBuffer() {
        lastSorting = Instant.now();
        ackCount = new ConcurrentHashMap<>();
    }

    public synchronized Values next() {
        // check whether we need to re-rank the buffers
        if (lastSorting.plusSeconds(minDelayReRankSec).isBefore(Instant.now())) {
            rerank();
        }
        return super.next();
    }

    // sorts the buffers according to the number of acks they got since the
    // previous time
    private void rerank() {
        if (queues.isEmpty()) {
            return;
        }

        List<QueueCount> sorted = new ArrayList<>();

        // populate a sorted set with key - queues
        Iterator<Entry<String, Queue<URLMetadata>>> i = queues.entrySet().iterator();

        while (i.hasNext()) {
            Entry<String, Queue<URLMetadata>> entry = i.next();
            String name = entry.getKey();
            int ackNum = ackCount.getOrDefault(name, new AtomicInteger(0)).get();
            QueueCount qc = new QueueCount(name, ackNum);
            sorted.add(qc);
        }

        Collections.sort(sorted);

        // now create a new Map of queues based on the sorted set
        // lowest score go first

        for (QueueCount q : sorted) {
            Queue<URLMetadata> queue = queues.remove(q.name);
            queues.put(q.name, queue);
        }

        ackCount.clear();
        lastSorting = Instant.now();
    }

    public void acked(String url) {
        // get the queue for this URL
        String key = partitioner.getPartition(url, Metadata.empty);
        if (key == null) {
            key = "_DEFAULT_";
        }
        // increment the counter for it
        ackCount.computeIfAbsent(key, k -> new AtomicInteger(0)).incrementAndGet();
    }

    class QueueCount implements Comparable<QueueCount> {
        String name;
        int count;

        QueueCount(String n, int c) {
            name = n;
            count = c;
        }

        @Override
        public int compareTo(QueueCount otherQueue) {
            int countDiff = otherQueue.count - count;
            if (countDiff == 0) return name.compareTo(otherQueue.name);
            return countDiff;
        }
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.persistence.urlbuffer;

import com.digitalpebble.stormcrawler.util.ConfUtils;
import com.github.benmanes.caffeine.cache.Cache;
import com.github.benmanes.caffeine.cache.Caffeine;
import com.github.benmanes.caffeine.cache.RemovalCause;
import com.github.benmanes.caffeine.cache.RemovalListener;
import com.google.common.collect.EvictingQueue;
import java.time.Instant;
import java.util.Iterator;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Queue;
import java.util.concurrent.TimeUnit;
import org.apache.storm.tuple.Values;
import org.checkerframework.checker.nullness.qual.NonNull;
import org.checkerframework.checker.nullness.qual.Nullable;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/** Checks how long the last N URLs took to work out whether a queue should release a URL. */
public class SchedulingURLBuffer extends AbstractURLBuffer
        implements RemovalListener<String, Object[]> {

    static final Logger LOG = LoggerFactory.getLogger(SchedulingURLBuffer.class);

    public static final String MAXTIMEPARAM = "priority.buffer.max.time.msec";

    private int maxTimeMSec = 30000;

    // TODO make it configurable
    private int historySize = 5;

    // keeps track of the URL having been sent
    private Cache<String, Object[]> unacked;

    private Cache<String, Queue<Long>> timings;

    private Cache<String, Instant> lastReleased;

    public void configure(Map stormConf) {
        super.configure(stormConf);
        maxTimeMSec = ConfUtils.getInt(stormConf, MAXTIMEPARAM, maxTimeMSec);
        unacked =
                Caffeine.newBuilder()
                        .expireAfterWrite(maxTimeMSec, TimeUnit.MILLISECONDS)
                        .removalListener(this)
                        .build();
        timings = Caffeine.newBuilder().expireAfterAccess(10, TimeUnit.MINUTES).build();
        lastReleased = Caffeine.newBuilder().expireAfterAccess(10, TimeUnit.MINUTES).build();
    }

    /**
     * Retrieves the next available URL, guarantees that the URLs are always perfectly shuffled
     *
     * @return null if no entries are available
     */
    public synchronized Values next() {

        do {
            Iterator<Entry<String, Queue<URLMetadata>>> i = queues.entrySet().iterator();

            if (!i.hasNext()) {
                LOG.trace("Empty iterator");
                return null;
            }

            Map.Entry<String, Queue<URLMetadata>> nextEntry = i.next();

            Queue<URLMetadata> queue = nextEntry.getValue();
            String queueName = nextEntry.getKey();

            // remove the entry, gets added back later
            i.remove();

            LOG.trace("Next queue {}", queueName);

            URLMetadata item = null;

            // is this queue ready to be processed?
            if (canRelease(queueName)) {
                // try the first element
                item = queue.poll();
                LOG.trace("Item {}", item.url);
            } else {
                LOG.trace("Queue {} not ready to release yet", queueName);
            }

            // any left? add to the end of the iterator
            if (!queue.isEmpty()) {
                LOG.debug("Adding to the back of the queue {}", queueName);
                queues.put(queueName, queue);
            }
            // notify that the queue is empty
            else {
                if (listener != null) {
                    listener.emptyQueue(queueName);
                }
            }

            if (item != null) {
                lastReleased.put(queueName, Instant.now());
                unacked.put(item.url, new Object[] {Instant.now(), queueName});
                // remove it from the list of URLs in the queue
                in_buffer.remove(item.url);
                return new Values(item.url, item.metadata);
            }
        } while (!queues.isEmpty());
        return null;
    }

    private boolean canRelease(String queueName) {
        // return true if enough time has expired since the previous release
        // given the past performance of the last N urls

        Queue<Long> times = timings.getIfPresent(queueName);
        if (times == null) return true;

        // not enough history yet? just say yes
        if (times.size() < historySize) return true;

        // get the average duration over the recent history
        long totalMsec = 0l;
        for (Long t : times) {
            totalMsec += t;
        }
        long average = totalMsec / historySize;

        LOG.trace("Average for {}: {} msec", queueName, average);

        Instant lastRelease = lastReleased.getIfPresent(queueName);
        if (lastRelease == null) {
            // removed? bit unlikely but nevermind
            return true;
        }

        // check that enough time has elapsed
        // since the previous release from this queue
        return lastRelease.plusMillis(average).isBefore(Instant.now());
    }

    public void acked(String url) {
        // get notified that the URL has been acked
        // use that to compute how long it took
        Object[] cached = (Object[]) unacked.getIfPresent(url);
        // has already been discarded - its timing set to max
        if (cached == null) {
            return;
        }

        Instant t = (Instant) cached[0];
        String key = (String) cached[1];

        long tookmsec = Instant.now().toEpochMilli() - t.toEpochMilli();

        LOG.trace("Adding new timing for {}: {} msec - {}", key, tookmsec, url);

        // add the timing for the queue
        addTiming(tookmsec, key);
    }

    void addTiming(long t, String queueName) {
        Queue<Long> times = timings.get(queueName, k -> EvictingQueue.create(historySize));
        times.add(t);
    }

    @Override
    public void onRemoval(
            @Nullable String key, Object @Nullable [] value, @NonNull RemovalCause cause) {
        addTiming(maxTimeMSec, key);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.persistence.urlbuffer;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.persistence.EmptyQueueListener;
import com.digitalpebble.stormcrawler.util.URLPartitioner;
import java.util.Collections;
import java.util.HashSet;
import java.util.LinkedHashMap;
import java.util.LinkedList;
import java.util.Map;
import java.util.Queue;
import java.util.Set;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Abstract class for URLBuffer interface, meant to simplify the code of the implementations and
 * provide some default methods
 *
 * @since 1.15
 */
public abstract class AbstractURLBuffer implements URLBuffer {

    private static final Logger LOG = LoggerFactory.getLogger(AbstractURLBuffer.class);

    protected Set<String> in_buffer = new HashSet<>();
    protected EmptyQueueListener listener = null;

    protected final URLPartitioner partitioner = new URLPartitioner();

    protected final Map<String, Queue<URLMetadata>> queues =
            Collections.synchronizedMap(new LinkedHashMap<>());

    public void configure(Map stormConf) {
        partitioner.configure(stormConf);
    }

    /** Total number of queues in the buffer * */
    public synchronized int numQueues() {
        return queues.size();
    }

    /**
     * Stores the URL and its Metadata under a given key.
     *
     * @return false if the URL was already in the buffer, true if it wasn't and was added
     */
    public synchronized boolean add(String URL, Metadata m, String key) {

        LOG.debug("Adding {}", URL);

        if (in_buffer.contains(URL)) {
            LOG.debug("already in buffer {}", URL);
            return false;
        }

        // determine which queue to use
        // configure with other than hostname
        if (key == null) {
            key = partitioner.getPartition(URL, m);
            if (key == null) {
                key = "_DEFAULT_";
            }
        }

        // create the queue if it does not exist
        // and add the url
        queues.computeIfAbsent(key, k -> new LinkedList<URLMetadata>())
                .add(new URLMetadata(URL, m));
        return in_buffer.add(URL);
    }

    /**
     * Stores the URL and its Metadata using the hostname as key.
     *
     * @return false if the URL was already in the buffer, true if it wasn't and was added
     */
    public synchronized boolean add(String URL, Metadata m) {
        return add(URL, m, null);
    }

    /** Total number of URLs in the buffer * */
    public synchronized int size() {
        return in_buffer.size();
    }

    public void setEmptyQueueListener(EmptyQueueListener l) {
        listener = l;
    }

    @Override
    public synchronized boolean hasNext() {
        return !queues.isEmpty();
    }

    protected class URLMetadata {
        String url;
        Metadata metadata;

        URLMetadata(String u, Metadata m) {
            url = u;
            metadata = m;
        }
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.persistence;

public enum Status {
    DISCOVERED,
    FETCHED,
    FETCH_ERROR,
    REDIRECTION,
    ERROR;

    /** Maps the HTTP Code to FETCHED, FETCH_ERROR or REDIRECTION */
    public static Status fromHTTPCode(int code) {
        if (code == 200) return Status.FETCHED;
        else if (code == 304) return Status.FETCHED;
        // REDIRS?
        if (code >= 300 && code < 400) return Status.REDIRECTION;
        // error otherwise
        return Status.FETCH_ERROR;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.persistence;

import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.parse.filter.MD5SignatureParseFilter;
import com.digitalpebble.stormcrawler.protocol.HttpHeaders;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import java.time.Duration;
import java.util.Calendar;
import java.util.Date;
import java.util.Locale;
import java.util.Map;
import java.util.Optional;
import org.slf4j.LoggerFactory;

/**
 * Adaptive fetch scheduler, checks by signature comparison whether a re-fetched page has changed:
 *
 * <ul>
 *   <li>if yes, shrink the fetch interval up to a minimum fetch interval
 *   <li>if not, increase the fetch interval up to a maximum
 * </ul>
 *
 * <p>The rate how the fetch interval is incremented or decremented is configurable.
 *
 * <p>Note, that this scheduler requires the following metadata:
 *
 * <dl>
 *   <dt>signature
 *   <dd>page signature, filled by {@link MD5SignatureParseFilter}
 *   <dt>signatureOld
 *   <dd>(temporary) copy of the previous signature, optionally copied by {@link
 *       MD5SignatureParseFilter}
 *   <dt>fetch.statusCode
 *   <dt>
 *   <dd>HTTP response status code, required to handle &quot;HTTP 304 Not Modified&quot; responses
 * </dl>
 *
 * and writes the following metadata fields:
 *
 * <dl>
 *   <dt>fetchInterval
 *   <dd>current fetch interval
 *   <dt>signatureChangeDate
 *   <dd>date when the signature has changed (ISO-8601 date time format)
 *   <dt>last-modified
 *   <dd>last-modified time used to send If-Modified-Since HTTP requests, only written if <code>
 *       scheduler.adaptive.setLastModified</code> is true. Same date string as set in
 *       &quot;signatureChangeDate&quot;. Note that it is assumed that the metadata field
 *       `last-modified` is written only by the scheduler, in detail, the property
 *       `protocol.md.prefix` should not be empty to avoid that `last-modified` is filled with an
 *       incorrect or ill-formed date from the HTTP header.
 *       <h2>Configuration</h2>
 *       <p>The following lines show how to configure the adaptive scheduler in the configuration
 *       file (crawler-conf.yaml):
 *       <pre>
 * scheduler.class: "com.digitalpebble.stormcrawler.persistence.AdaptiveScheduler"
 * # set last-modified time ({@link HttpHeaders.LAST_MODIFIED}) used in HTTP If-Modified-Since request header field
 * scheduler.adaptive.setLastModified: true
 * # min. interval in minutes (default: 1h)
 * scheduler.adaptive.fetchInterval.min: 60
 * # max. interval in minutes (default: 2 weeks)
 * scheduler.adaptive.fetchInterval.max: 20160
 * # increment and decrement rates (0.0 < rate <= 1.0)
 * scheduler.adaptive.fetchInterval.rate.incr: .5
 * scheduler.adaptive.fetchInterval.rate.decr: .5
 *
 * # required persisted metadata (in addition to other persisted metadata):
 * metadata.persist:
 *  - ...
 *  - signature
 *  - fetch.statusCode
 *  - fetchInterval
 *  - last-modified
 * # - signatureOld
 * # - signatureChangeDate
 * # Note: "signatureOld" and "signatureChangeDate" are optional, the adaptive
 * # scheduler will also work if both are temporarily passed and not persisted.
 * </pre>
 *       <p>To generate the signature and keep a copy of the last signature, the parse filters
 *       should be configured accordingly:
 *       <pre>
 * "com.digitalpebble.stormcrawler.parse.ParseFilters": [
 *   ...,
 *   {
 *     "class": "com.digitalpebble.stormcrawler.parse.filter.MD5SignatureParseFilter",
 *     "name": "MD5Digest",
 *     "params": {
 *       "useText": "false",
 *       "keyName": "signature",
 *       "keyNameCopy": "signatureOld"
 *     }
 *   }
 * </pre>
 *       The order is mandatory: first copy the old signature, than generate the current one.
 */
public class AdaptiveScheduler extends DefaultScheduler {

    /**
     * Configuration property (boolean) whether or not to set the &quot;last-modified&quot; metadata
     * field when a page change was detected by signature comparison.
     */
    public static final String SET_LAST_MODIFIED = "scheduler.adaptive.setLastModified";

    /** Configuration property (int) to set the minimum fetch interval in minutes. */
    public static final String INTERVAL_MIN = "scheduler.adaptive.fetchInterval.min";

    /** Configuration property (int) to set the maximum fetch interval in minutes. */
    public static final String INTERVAL_MAX = "scheduler.adaptive.fetchInterval.max";

    /**
     * Configuration property (float) to set the increment rate. If a page hasn't changed when
     * refetched, the fetch interval is multiplied by (1.0 + incr_rate) until the max. fetch
     * interval is reached.
     */
    public static final String INTERVAL_INC_RATE = "scheduler.adaptive.fetchInterval.rate.incr";

    /**
     * Configuration property (float) to set the decrement rate. If a page has changed when
     * refetched, the fetch interval is multiplied by (1.0 - decr_rate). If the fetch interval comes
     * closer to the minimum interval, the decrementing is slowed down.
     */
    public static final String INTERVAL_DEC_RATE = "scheduler.adaptive.fetchInterval.rate.decr";

    /**
     * Name of the signature key in metadata, must be defined as &quot;keyName&quot; in the
     * configuration of {@link com.digitalpebble.stormcrawler.parse.filter.MD5SignatureParseFilter}
     * . This key must be listed in &quot;metadata.persist&quot;.
     */
    public static final String SIGNATURE_KEY = "signature";

    /**
     * Name of key to hold previous signature: a copy, not overwritten by {@link
     * MD5SignatureParseFilter}, is added by {@link
     * com.digitalpebble.stormcrawler.parse.filter.SignatureCopyParseFilter} . This key is a
     * temporary copy, not necessarily persisted in metadata.
     */
    public static final String SIGNATURE_OLD_KEY = "signatureOld";

    /**
     * Key to store the current fetch interval value, must be listed in
     * &quot;metadata.persist&quot;.
     */
    public static final String FETCH_INTERVAL_KEY = "fetchInterval";

    /**
     * Key to store the date when the signature has been changed, must be listed in
     * &quot;metadata.persist&quot;.
     */
    public static final String SIGNATURE_MODIFIED_KEY = "signatureChangeDate";

    private static final org.slf4j.Logger LOG = LoggerFactory.getLogger(AdaptiveScheduler.class);

    protected int defaultfetchInterval;
    protected int minFetchInterval = 60;
    protected int maxFetchInterval = 60 * 24 * 14;
    protected float fetchIntervalDecRate = .5f;
    protected float fetchIntervalIncRate = .5f;

    protected boolean setLastModified = false;
    protected boolean overwriteLastModified = false;

    @Override
    @SuppressWarnings({"rawtypes", "unchecked"})
    public void init(Map stormConf) {
        defaultfetchInterval =
                ConfUtils.getInt(stormConf, Constants.defaultFetchIntervalParamName, 1440);
        setLastModified = ConfUtils.getBoolean(stormConf, SET_LAST_MODIFIED, false);
        minFetchInterval = ConfUtils.getInt(stormConf, INTERVAL_MIN, minFetchInterval);
        maxFetchInterval = ConfUtils.getInt(stormConf, INTERVAL_MAX, maxFetchInterval);
        fetchIntervalDecRate =
                ConfUtils.getFloat(stormConf, INTERVAL_DEC_RATE, fetchIntervalDecRate);
        fetchIntervalIncRate =
                ConfUtils.getFloat(stormConf, INTERVAL_INC_RATE, fetchIntervalIncRate);
        super.init(stormConf);
    }

    @Override
    public Optional<Date> schedule(Status status, Metadata metadata) {
        LOG.debug("Scheduling status: {}, metadata: {}", status, metadata);

        String signature = metadata.getFirstValue(SIGNATURE_KEY);
        String oldSignature = metadata.getFirstValue(SIGNATURE_OLD_KEY);

        if (status != Status.FETCHED) {

            // reset all metadata
            metadata.remove(SIGNATURE_MODIFIED_KEY);
            metadata.remove(FETCH_INTERVAL_KEY);
            metadata.remove(SIGNATURE_KEY);
            metadata.remove(SIGNATURE_OLD_KEY);

            if (status == Status.ERROR) {
                /*
                 * remove last-modified for permanent errors so that no
                 * if-modified-since request is sent: the content is needed
                 * again to be parsed and index
                 */
                metadata.remove(HttpHeaders.LAST_MODIFIED);
            }

            // fall-back to DefaultScheduler
            return super.schedule(status, metadata);
        }

        Calendar now = Calendar.getInstance(Locale.ROOT);

        String signatureModified = metadata.getFirstValue(SIGNATURE_MODIFIED_KEY);

        boolean changed = false;

        final String modifiedTimeString = now.toInstant().toString();

        if (metadata.getFirstValue("fetch.statusCode").equals("304")) {
            // HTTP 304 Not Modified
            // - no new signature calculated because no content fetched
            // - do not compare persisted signatures
            // - leave last-modified time unchanged
        } else if (signature == null || oldSignature == null) {
            // no decision possible by signature comparison if
            // - document not parsed (intentionally or not) or
            // - signature not generated or
            // - old signature not copied
            // fall-back to DefaultScheduler
            LOG.debug("No signature for FETCHED page: {}", metadata);
            if (setLastModified && signature != null && oldSignature == null) {
                // set last-modified time for first fetch
                metadata.setValue(HttpHeaders.LAST_MODIFIED, modifiedTimeString);
            }
            Optional<Date> nextFetch = super.schedule(status, metadata);
            if (nextFetch.isPresent()) {
                long fetchIntervalMinutes =
                        Duration.between(now.toInstant(), nextFetch.get().toInstant()).toMinutes();
                metadata.setValue(FETCH_INTERVAL_KEY, Long.toString(fetchIntervalMinutes));
            }
            return nextFetch;
        } else if (signature.equals(oldSignature)) {
            // unchanged
        } else {
            // change detected by signature comparison
            changed = true;
            signatureModified = modifiedTimeString;
            if (setLastModified) {
                metadata.setValue(HttpHeaders.LAST_MODIFIED, modifiedTimeString);
            }
        }

        String fetchInterval = metadata.getFirstValue(FETCH_INTERVAL_KEY);
        int interval = defaultfetchInterval;
        if (fetchInterval != null) {
            interval = Integer.parseInt(fetchInterval);
        } else {
            // initialize from DefaultScheduler
            Optional<Integer> customInterval = super.checkCustomInterval(metadata, status);
            if (customInterval.isPresent()) {
                interval = customInterval.get();
            } else {
                interval = defaultfetchInterval;
            }
            fetchInterval = Integer.toString(interval);
        }

        if (changed) {
            // shrink fetch interval (slow down decrementing if already close to
            // the minimum interval)
            interval =
                    (int)
                            ((1.0f - fetchIntervalDecRate) * interval
                                    + fetchIntervalDecRate * minFetchInterval);
            LOG.debug(
                    "Signature has changed, fetchInterval decreased from {} to {}",
                    fetchInterval,
                    interval);

        } else {
            // no change or not modified, increase fetch interval
            interval = (int) (interval * (1.0f + fetchIntervalIncRate));
            if (interval > maxFetchInterval) {
                interval = maxFetchInterval;
            }
            LOG.debug("Unchanged, fetchInterval increased from {} to {}", fetchInterval, interval);
            // remove old signature (do not keep same signature twice)
            metadata.remove(SIGNATURE_OLD_KEY);
            if (signatureModified == null) {
                signatureModified = modifiedTimeString;
            }
        }

        metadata.setValue(FETCH_INTERVAL_KEY, Integer.toString(interval));
        metadata.setValue(SIGNATURE_MODIFIED_KEY, signatureModified);

        now.add(Calendar.MINUTE, interval);

        return Optional.of(now.getTime());
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.persistence;

import com.digitalpebble.stormcrawler.Metadata;
import java.util.Date;
import java.util.Optional;
import org.apache.storm.tuple.Tuple;

/**
 * Dummy status updater which dumps the content of the incoming tuples to the standard output.
 * Useful for debugging and as an illustration of what AbstractStatusUpdaterBolt provides.
 */
@SuppressWarnings("serial")
public class StdOutStatusUpdater extends AbstractStatusUpdaterBolt {

    @Override
    public void store(
            String url, Status status, Metadata metadata, Optional<Date> nextFetch, Tuple t)
            throws Exception {
        String nextFetchS = "NEVER";
        if (nextFetch.isPresent()) {
            nextFetchS = nextFetch.get().toString();
        }
        System.out.println(url + "\t" + status + "\t" + nextFetchS);
        System.out.println(metadata.toString("\t"));
        super.ack(t, url);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.persistence;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.spout.MemorySpout;
import java.util.Date;
import java.util.Optional;
import org.apache.storm.tuple.Tuple;

/**
 * Use in combination with the MemorySpout for testing in local mode. There is no guarantee that
 * this will work in distributed mode as it expects the MemorySpout to be in the same execution
 * thread.
 */
@SuppressWarnings("serial")
public class MemoryStatusUpdater extends AbstractStatusUpdaterBolt {

    @Override
    public void store(
            String url, Status status, Metadata metadata, Optional<Date> nextFetch, Tuple t)
            throws Exception {
        // no next fetch date present means never refetch
        if (nextFetch.isPresent()) {
            MemorySpout.add(url, metadata, nextFetch.get());
        }
        super.ack(t, url);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.persistence;

import com.digitalpebble.stormcrawler.persistence.urlbuffer.URLBuffer;
import com.digitalpebble.stormcrawler.util.CollectionMetric;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import com.github.benmanes.caffeine.cache.Cache;
import com.github.benmanes.caffeine.cache.Caffeine;
import java.time.Instant;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Optional;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import org.apache.storm.metric.api.MultiCountMetric;
import org.apache.storm.spout.SpoutOutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.topology.base.BaseRichSpout;
import org.apache.storm.tuple.Fields;
import org.apache.storm.utils.Utils;

/**
 * Common features of spouts which query a backend to generate tuples. Tracks the URLs being
 * processes, with an optional delay before they are removed from the cache. Throttles the rate a
 * which queries are emitted and provides a buffer to store the URLs waiting to be sent.
 *
 * @since 1.11
 */
public abstract class AbstractQueryingSpout extends BaseRichSpout {

    /**
     * Time in seconds for which acked or failed URLs will be considered for fetching again, default
     * 30 secs.
     */
    protected static final String StatusTTLPurgatory = "spout.ttl.purgatory";
    /**
     * Min time to allow between 2 successive queries to the backend. Value in msecs, default 2000.
     */
    protected static final String StatusMinDelayParamName = "spout.min.delay.queries";

    protected long minDelayBetweenQueries = 2000;

    /**
     * Max time to allow between 2 successive queries to the backend. Value in msecs, default 20000.
     */
    protected static final String StatusMaxDelayParamName = "spout.max.delay.queries";

    protected long maxDelayBetweenQueries = 20000;

    /**
     * Delay in seconds after which the nextFetchDate filter is set to the current time, default
     * 120. Is used to prevent the search to be limited to a handful of sources.
     */
    protected static final String resetFetchDateParamName = "spout.reset.fetchdate.after";

    protected int resetFetchDateAfterNSecs = 120;

    protected Instant lastTimeResetToNOW;

    private long timeLastQuerySent = 0;
    private long timeLastQueryReceived = 0;

    private long timestampEmptyBuffer = -1;

    protected MultiCountMetric eventCounter;

    protected URLBuffer buffer;

    protected SpoutOutputCollector _collector;

    /** Required for implementations doing asynchronous calls * */
    protected AtomicBoolean isInQuery = new AtomicBoolean(false);

    protected CollectionMetric queryTimes;

    @Override
    public void open(Map stormConf, TopologyContext context, SpoutOutputCollector collector) {

        int ttlPurgatory = ConfUtils.getInt(stormConf, StatusTTLPurgatory, 30);

        minDelayBetweenQueries = ConfUtils.getLong(stormConf, StatusMinDelayParamName, 2000);

        maxDelayBetweenQueries =
                ConfUtils.getLong(stormConf, StatusMaxDelayParamName, maxDelayBetweenQueries);

        beingProcessed = new InProcessMap<>(ttlPurgatory, TimeUnit.SECONDS);

        eventCounter = context.registerMetric("counters", new MultiCountMetric(), 10);

        buffer = URLBuffer.getInstance(stormConf);

        context.registerMetric("buffer_size", () -> buffer.size(), 10);
        context.registerMetric("numQueues", () -> buffer.numQueues(), 10);

        context.registerMetric("beingProcessed", () -> beingProcessed.size(), 10);
        context.registerMetric("inPurgatory", () -> beingProcessed.inCache(), 10);

        queryTimes = new CollectionMetric();
        context.registerMetric("spout_query_time_msec", queryTimes, 10);

        resetFetchDateAfterNSecs =
                ConfUtils.getInt(stormConf, resetFetchDateParamName, resetFetchDateAfterNSecs);

        _collector = collector;
    }

    /**
     * Method where specific implementations query the storage. Implementations should call
     * markQueryReceivedNow when the documents have been received.
     */
    protected abstract void populateBuffer();

    /**
     * Map to keep in-process URLs, with the URL as key and optional value depending on the spout
     * implementation. The entries are kept in a cache for a configurable amount of time to avoid
     * that some items are fetched a second time if new items are queried shortly after they have
     * been acked.
     */
    protected InProcessMap<String, Object> beingProcessed;

    private boolean active;

    /** Map which holds elements some additional time after the removal. */
    public class InProcessMap<K, V> extends HashMap<K, V> {

        private Cache<K, Optional<V>> deletionCache;

        public InProcessMap(long maxDuration, TimeUnit timeUnit) {
            deletionCache = Caffeine.newBuilder().expireAfterWrite(maxDuration, timeUnit).build();
        }

        @Override
        public boolean containsKey(Object key) {
            boolean incache = super.containsKey(key);
            if (!incache) {
                incache = (deletionCache.getIfPresent(key) != null);
            }
            return incache;
        }

        @Override
        public V remove(Object key) {
            deletionCache.put((K) key, Optional.empty());
            return super.remove(key);
        }

        public long inCache() {
            return deletionCache.estimatedSize();
        }
    }

    @Override
    public void nextTuple() {
        if (!active) return;

        // force the refresh of the buffer even if the buffer is not empty
        if (!isInQuery.get() && triggerQueries()) {
            populateBuffer();
            timeLastQuerySent = System.currentTimeMillis();
        }

        if (buffer.hasNext()) {
            // track how long the buffer had been empty for
            if (timestampEmptyBuffer != -1) {
                eventCounter
                        .scope("empty.buffer")
                        .incrBy(System.currentTimeMillis() - timestampEmptyBuffer);
                timestampEmptyBuffer = -1;
            }
            List<Object> fields = buffer.next();
            String url = fields.get(0).toString();
            this._collector.emit(fields, url);
            beingProcessed.put(url, null);
            eventCounter.scope("emitted").incrBy(1);
            return;
        } else if (timestampEmptyBuffer == -1) {
            timestampEmptyBuffer = System.currentTimeMillis();
        }

        if (isInQuery.get() || throttleQueries() > 0) {
            // sleep for a bit but not too much in order to give ack/fail a
            // chance
            Utils.sleep(10);
            return;
        }

        // re-populate the buffer
        populateBuffer();

        timeLastQuerySent = System.currentTimeMillis();
    }

    /**
     * Returns the amount of time to wait if the backend was queried too recently and needs
     * throttling or -1 if the backend can be queried straight away.
     */
    private long throttleQueries() {
        if (timeLastQuerySent != 0) {
            // check that we allowed some time between queries
            long difference = System.currentTimeMillis() - timeLastQuerySent;
            if (difference < minDelayBetweenQueries) {
                return minDelayBetweenQueries - difference;
            }
        }
        return -1;
    }

    /**
     * Indicates whether enough time has elapsed since receiving the results of the previous query
     * so that a new one can be sent even if the buffer is not empty. Applies to asynchronous
     * clients only.
     */
    private boolean triggerQueries() {
        if (timeLastQueryReceived != 0 && maxDelayBetweenQueries > 0) {
            // check that we allowed some time between queries
            long difference = System.currentTimeMillis() - timeLastQueryReceived;
            if (difference > maxDelayBetweenQueries) {
                return true;
            }
        }
        return false;
    }

    protected long getTimeLastQuerySent() {
        return timeLastQuerySent;
    }

    /** sets the marker that we are in a query to false and timeLastQueryReceived to now */
    protected void markQueryReceivedNow() {
        isInQuery.set(false);
        timeLastQueryReceived = System.currentTimeMillis();
    }

    @Override
    public void activate() {
        active = true;
    }

    @Override
    public void deactivate() {
        active = false;
    }

    @Override
    public void ack(Object msgId) {
        beingProcessed.remove(msgId);
        eventCounter.scope("acked").incrBy(1);
        buffer.acked(msgId.toString());
    }

    @Override
    public void fail(Object msgId) {
        beingProcessed.remove(msgId);
        eventCounter.scope("failed").incrBy(1);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("url", "metadata"));
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.persistence;

import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import com.digitalpebble.stormcrawler.util.MetadataTransfer;
import com.github.benmanes.caffeine.cache.Cache;
import com.github.benmanes.caffeine.cache.Caffeine;
import java.time.Instant;
import java.time.format.DateTimeFormatter;
import java.util.Calendar;
import java.util.Date;
import java.util.HashMap;
import java.util.Map;
import java.util.Optional;
import org.apache.commons.lang.time.DateUtils;
import org.apache.storm.metric.api.IMetric;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.topology.base.BaseRichBolt;
import org.apache.storm.tuple.Fields;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.tuple.Values;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Abstract bolt used to store the status of URLs. Uses the DefaultScheduler and MetadataTransfer.
 */
@SuppressWarnings("serial")
public abstract class AbstractStatusUpdaterBolt extends BaseRichBolt {

    private static final Logger LOG = LoggerFactory.getLogger(AbstractStatusUpdaterBolt.class);

    /**
     * Parameter name to indicate whether the internal cache should be used for discovered URLs. The
     * value of the parameter is a boolean - true by default.
     */
    public static String useCacheParamName = "status.updater.use.cache";

    /** Number of successive FETCH_ERROR before status changes to ERROR * */
    public static String maxFetchErrorsParamName = "max.fetch.errors";

    /**
     * Parameter name to configure the cache @see http://docs.guava-libraries.googlecode
     * .com/git/javadoc/com/google/common/cache/CacheBuilderSpec.html Default value is
     * "maximumSize=10000,expireAfterAccess=1h"
     */
    public static String cacheConfigParamName = "status.updater.cache.spec";

    /**
     * Used for rounding nextFetchDates. Values are hour, minute or second, the latter is the
     * default value.
     */
    public static String roundDateParamName = "status.updater.unit.round.date";

    /**
     * Key used to pass a preset Date to use as nextFetchDate. The value must represent a valid
     * instant in UTC and be parsable using {@link DateTimeFormatter#ISO_INSTANT}. This also
     * indicates that the storage can be done directly on the metadata as-is.
     */
    public static final String AS_IS_NEXTFETCHDATE_METADATA =
            "status.store.as.is.with.nextfetchdate";

    protected OutputCollector _collector;

    private Scheduler scheduler;
    private MetadataTransfer mdTransfer;

    private Cache<Object, Object> cache;
    private boolean useCache = true;

    private int maxFetchErrors = 3;

    private long cacheHits = 0;
    private long cacheMisses = 0;

    private int roundDateUnit = Calendar.SECOND;

    @SuppressWarnings({"unchecked", "rawtypes"})
    @Override
    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
        _collector = collector;

        scheduler = Scheduler.getInstance(stormConf);

        mdTransfer = MetadataTransfer.getInstance(stormConf);

        useCache = ConfUtils.getBoolean(stormConf, useCacheParamName, true);

        if (useCache) {
            String spec = ConfUtils.getString(stormConf, cacheConfigParamName);
            cache = Caffeine.from(spec).build();

            context.registerMetric(
                    "cache",
                    new IMetric() {
                        @Override
                        public Object getValueAndReset() {
                            Map<String, Long> statsMap = new HashMap<>();
                            statsMap.put("hits", cacheHits);
                            statsMap.put("misses", cacheMisses);
                            statsMap.put("size", cache.estimatedSize());
                            cacheHits = 0;
                            cacheMisses = 0;
                            return statsMap;
                        }
                    },
                    30);
        }

        maxFetchErrors = ConfUtils.getInt(stormConf, maxFetchErrorsParamName, 3);

        String tmpdateround = ConfUtils.getString(stormConf, roundDateParamName, "SECOND");
        if (tmpdateround.equalsIgnoreCase("MINUTE")) {
            roundDateUnit = Calendar.MINUTE;
        } else if (tmpdateround.equalsIgnoreCase("HOUR")) {
            roundDateUnit = Calendar.HOUR;
        }
    }

    @Override
    public void execute(Tuple tuple) {

        String url = tuple.getStringByField("url");
        Status status = (Status) tuple.getValueByField("status");

        boolean potentiallyNew = status.equals(Status.DISCOVERED);

        // if the URL is a freshly discovered one
        // check whether it is already known in the cache
        // if so we've already seen it and don't need to
        // store it again
        if (potentiallyNew && useCache) {
            if (cache.getIfPresent(url) != null) {
                // no need to add it to the queue
                LOG.debug("URL {} already in cache", url);
                cacheHits++;
                _collector.ack(tuple);
                return;
            } else {
                LOG.debug("URL {} not in cache", url);
                cacheMisses++;
            }
        }

        Metadata metadata = (Metadata) tuple.getValueByField("metadata");

        // store directly with the date specified in the metadata without
        // changing the status or scheduling.
        String dateInMetadata = metadata.getFirstValue(AS_IS_NEXTFETCHDATE_METADATA);
        if (dateInMetadata != null) {
            Date nextFetch = Date.from(Instant.parse(dateInMetadata));
            try {
                store(url, status, mdTransfer.filter(metadata), Optional.of(nextFetch), tuple);
                return;
            } catch (Exception e) {
                LOG.error("Exception caught when storing", e);
                _collector.fail(tuple);
                return;
            }
        }

        // store last processed or discovery date in UTC
        final String nowAsString = Instant.now().toString();
        if (status.equals(Status.DISCOVERED)) {
            metadata.setValue("discoveryDate", nowAsString);
        } else {
            metadata.setValue("lastProcessedDate", nowAsString);
        }

        // too many fetch errors?
        if (status.equals(Status.FETCH_ERROR)) {
            String errorCount = metadata.getFirstValue(Constants.fetchErrorCountParamName);
            int count = 0;
            try {
                count = Integer.parseInt(errorCount);
            } catch (NumberFormatException e) {
            }
            count++;
            if (count >= maxFetchErrors) {
                status = Status.ERROR;
                metadata.setValue(Constants.STATUS_ERROR_CAUSE, "maxFetchErrors");
            } else {
                metadata.setValue(Constants.fetchErrorCountParamName, Integer.toString(count));
            }
        }

        // delete any existing error count metadata
        // e.g. status changed
        if (!status.equals(Status.FETCH_ERROR)) {
            metadata.remove(Constants.fetchErrorCountParamName);
        }
        // https://github.com/DigitalPebble/storm-crawler/issues/415
        // remove error related key values in case of success
        if (status.equals(Status.FETCHED) || status.equals(Status.REDIRECTION)) {
            metadata.remove(Constants.STATUS_ERROR_CAUSE);
            metadata.remove(Constants.STATUS_ERROR_MESSAGE);
            metadata.remove(Constants.STATUS_ERROR_SOURCE);
        }
        // gone? notify any deleters. Doesn't need to be anchored
        else if (status == Status.ERROR) {
            _collector.emit(Constants.DELETION_STREAM_NAME, new Values(url, metadata));
        }

        // determine the value of the next fetch based on the status
        Optional<Date> nextFetch = scheduler.schedule(status, metadata);

        // filter metadata just before storing it, so that non-persisted
        // metadata is available to fetch schedulers
        metadata = mdTransfer.filter(metadata);

        // round next fetch date - unless it is never
        if (nextFetch.isPresent()) {
            nextFetch = Optional.of(DateUtils.round(nextFetch.get(), this.roundDateUnit));
        }

        // extensions of this class will handle the storage
        // on a per document basis
        try {
            store(url, status, metadata, nextFetch, tuple);
        } catch (Exception e) {
            LOG.error("Exception caught when storing", e);
            _collector.fail(tuple);
            return;
        }
    }

    /** Must be called by extending classes to store and collect in one go */
    protected final void ack(Tuple t, String url) {
        // keep the URL in the cache
        if (useCache) {
            cache.put(url, "");
        }

        _collector.ack(t);
    }

    protected abstract void store(
            String url, Status status, Metadata metadata, Optional<Date> nextFetch, Tuple t)
            throws Exception;

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declareStream(Constants.DELETION_STREAM_NAME, new Fields("url", "metadata"));
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.persistence;

import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import java.util.Calendar;
import java.util.Date;
import java.util.HashMap;
import java.util.Iterator;
import java.util.Map;
import java.util.Optional;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

/** Schedules a nextFetchDate based on the configuration */
public class DefaultScheduler extends Scheduler {

    /**
     * Key used to pass a custom delay via metadata. Used by the sitemaps to stagger the scheduling
     * of URLs.
     */
    public static final String DELAY_METADATA = "scheduler.delay.mins";

    // fetch intervals in minutes
    private int defaultfetchInterval;
    private int fetchErrorFetchInterval;
    private int errorFetchInterval;

    private CustomInterval[] customIntervals;

    /*
     * (non-Javadoc)
     *
     * @see
     * com.digitalpebble.stormcrawler.persistence.Scheduler#init(java.util.Map)
     */
    @SuppressWarnings("rawtypes")
    @Override
    public void init(Map stormConf) {
        defaultfetchInterval =
                ConfUtils.getInt(stormConf, Constants.defaultFetchIntervalParamName, 1440);
        fetchErrorFetchInterval =
                ConfUtils.getInt(stormConf, Constants.fetchErrorFetchIntervalParamName, 120);
        errorFetchInterval =
                ConfUtils.getInt(stormConf, Constants.errorFetchIntervalParamName, 44640);

        // loads any custom key values
        // must be of form fetchInterval(.STATUS)?.keyname=value
        // e.g. fetchInterval.isFeed=true
        // e.g. fetchInterval.FETCH_ERROR.isFeed=true
        Map<String, CustomInterval> intervals = new HashMap<>();
        Pattern pattern = Pattern.compile("^fetchInterval(\\..+?)?\\.(.+)=(.+)");
        Iterator<String> keyIter = stormConf.keySet().iterator();
        while (keyIter.hasNext()) {
            String key = keyIter.next();
            Matcher m = pattern.matcher(key);
            if (!m.matches()) {
                continue;
            }
            Status status = null;
            // was a status specified?
            if (m.group(1) != null) {
                status = Status.valueOf(m.group(1).substring(1));
            }
            String mdname = m.group(2);
            String mdvalue = m.group(3);
            int customInterval = ConfUtils.getInt(stormConf, key, Integer.MIN_VALUE);
            if (customInterval != Integer.MIN_VALUE) {
                CustomInterval interval = intervals.get(mdname + mdvalue);
                if (interval == null) {
                    interval = new CustomInterval(mdname, mdvalue, status, customInterval);
                } else {
                    interval.setDurationForStatus(status, customInterval);
                }
                // specify particular interval for this status
                intervals.put(mdname + mdvalue, interval);
            }
        }
        customIntervals = intervals.values().toArray(new CustomInterval[intervals.size()]);
    }

    /*
     * (non-Javadoc)
     *
     * @see com.digitalpebble.stormcrawler.persistence.Scheduler#schedule(com.
     * digitalpebble. stormcrawler.persistence .Status,
     * com.digitalpebble.stormcrawler.Metadata)
     */
    @Override
    public Optional<Date> schedule(Status status, Metadata metadata) {

        int minutesIncrement = 0;

        Optional<Integer> customInterval = Optional.empty();

        // try with a value set in the metadata
        String customInMetadata = metadata.getFirstValue(DELAY_METADATA);
        if (customInMetadata != null) {
            customInterval = Optional.of(Integer.parseInt(customInMetadata));
        }
        // try with the rules from the configuration
        if (!customInterval.isPresent()) {
            customInterval = checkCustomInterval(metadata, status);
        }

        if (customInterval.isPresent()) {
            minutesIncrement = customInterval.get();
        } else {
            switch (status) {
                case FETCHED:
                    minutesIncrement = defaultfetchInterval;
                    break;
                case FETCH_ERROR:
                    minutesIncrement = fetchErrorFetchInterval;
                    break;
                case ERROR:
                    minutesIncrement = errorFetchInterval;
                    break;
                case REDIRECTION:
                    minutesIncrement = defaultfetchInterval;
                    break;
                default:
                    // leave it to now e.g. DISCOVERED
            }
        }

        // a value of -1 means never fetch
        // we return null
        if (minutesIncrement == -1) {
            return Optional.empty();
        }

        Calendar cal = Calendar.getInstance();
        cal.add(Calendar.MINUTE, minutesIncrement);

        return Optional.of(cal.getTime());
    }

    /** Returns the first matching custom interval */
    protected final Optional<Integer> checkCustomInterval(Metadata metadata, Status s) {
        if (customIntervals == null) return Optional.empty();

        for (CustomInterval customInterval : customIntervals) {
            String[] values = metadata.getValues(customInterval.key);
            if (values == null) {
                continue;
            }
            for (String v : values) {
                if (v.equals(customInterval.value)) {
                    return customInterval.getDurationForStatus(s);
                }
            }
        }

        return Optional.empty();
    }

    private class CustomInterval {
        private String key;
        private String value;
        private Map<Status, Integer> durationPerStatus;
        private Integer defaultDuration = null;

        private CustomInterval(String key, String value, Status status, int minutes) {
            this.key = key;
            this.value = value;
            this.durationPerStatus = new HashMap<>();
            setDurationForStatus(status, minutes);
        }

        private void setDurationForStatus(Status s, int minutes) {
            if (s == null) {
                defaultDuration = minutes;
            } else {
                this.durationPerStatus.put(s, minutes);
            }
        }

        private Optional<Integer> getDurationForStatus(Status s) {
            // do we have a specific value for this status?
            Integer customD = durationPerStatus.get(s);
            if (customD != null) {
                return Optional.of(customD);
            }
            // is there a default one set?
            if (defaultDuration != null) {
                return Optional.of(defaultDuration);
            }
            // no default value or custom one for that status
            return Optional.empty();
        }
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.persistence;

/** Used by URLBuffer to inform the spouts when a queue has no more URLs in it */
public interface EmptyQueueListener {
    abstract void emptyQueue(String queueName);
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.persistence;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import java.util.Date;
import java.util.Map;
import java.util.Optional;
import org.apache.commons.lang.StringUtils;

public abstract class Scheduler {

    /** Class to use for Scheduler. Must extend the class Scheduler. */
    public static final String schedulerClassParamName = "scheduler.class";

    @SuppressWarnings("rawtypes")
    /**
     * Configuration of the scheduler based on the config. Should be called by
     * Scheduler.getInstance() *
     */
    protected abstract void init(Map stormConf);

    /**
     * Returns an optional Date indicating when the document should be refetched next, based on its
     * status. It is empty if the URL should never be refetched.
     */
    public abstract Optional<Date> schedule(Status status, Metadata metadata);

    /** Returns a Scheduler instance based on the configuration * */
    @SuppressWarnings({"rawtypes", "unchecked"})
    public static Scheduler getInstance(Map stormConf) {
        Scheduler scheduler;

        String className = ConfUtils.getString(stormConf, schedulerClassParamName);

        if (StringUtils.isBlank(className)) {
            throw new RuntimeException("Missing value for config  " + schedulerClassParamName);
        }

        try {
            Class<?> schedulerc = Class.forName(className);
            boolean interfaceOK = Scheduler.class.isAssignableFrom(schedulerc);
            if (!interfaceOK) {
                throw new RuntimeException("Class " + className + " must extend Scheduler");
            }
            scheduler = (Scheduler) schedulerc.newInstance();
        } catch (Exception e) {
            throw new RuntimeException("Can't instanciate " + className);
        }

        scheduler.init(stormConf);
        return scheduler;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler;

import com.digitalpebble.stormcrawler.persistence.Status;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import java.io.FileNotFoundException;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import org.apache.commons.lang.StringUtils;
import org.apache.storm.Config;
import org.apache.storm.StormSubmitter;
import org.apache.storm.topology.TopologyBuilder;
import org.apache.storm.utils.Utils;

public abstract class ConfigurableTopology {

    protected Config conf = new Config();

    public static void start(ConfigurableTopology topology, String args[]) {
        // loads the default configuration file
        Map defaultSCConfig = Utils.findAndReadConfigFile("crawler-default.yaml", false);
        topology.conf.putAll(ConfUtils.extractConfigElement(defaultSCConfig));

        String[] remainingArgs = topology.parse(args);
        topology.run(remainingArgs);
    }

    protected Config getConf() {
        return conf;
    }

    protected abstract int run(String args[]);

    /** Submits the topology with the name taken from the configuration * */
    protected int submit(Config conf, TopologyBuilder builder) {
        String name = ConfUtils.getString(conf, Config.TOPOLOGY_NAME);
        if (StringUtils.isBlank(name))
            throw new RuntimeException("No value found for " + Config.TOPOLOGY_NAME);
        return submit(name, conf, builder);
    }

    /** Submits the topology under a specific name * */
    protected int submit(String name, Config conf, TopologyBuilder builder) {

        // register for serialization with Kryo
        Config.registerSerialization(conf, Metadata.class);
        Config.registerSerialization(conf, Status.class);

        try {
            StormSubmitter.submitTopology(name, conf, builder.createTopology());
        } catch (Exception e) {
            e.printStackTrace();
            return -1;
        }
        return 0;
    }

    private String[] parse(String args[]) {

        List<String> newArgs = new ArrayList<>();
        Collections.addAll(newArgs, args);

        Iterator<String> iter = newArgs.iterator();
        while (iter.hasNext()) {
            String param = iter.next();
            if (param.equals("-conf")) {
                if (!iter.hasNext()) {
                    throw new RuntimeException("Conf file not specified");
                }
                iter.remove();
                String resource = iter.next();
                try {
                    ConfUtils.loadConf(resource, conf);
                } catch (FileNotFoundException e) {
                    throw new RuntimeException("File not found : " + resource);
                }
                iter.remove();
            }
        }

        return newArgs.toArray(new String[newArgs.size()]);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.indexing;

import static com.digitalpebble.stormcrawler.Constants.StatusStreamName;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.persistence.Status;
import java.util.Iterator;
import java.util.Map;
import org.apache.commons.lang.StringUtils;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.tuple.Values;

/**
 * Indexer which generates fields for indexing and sends them to the standard output. Useful for
 * debugging and as an illustration of what AbstractIndexerBolt provides.
 */
@SuppressWarnings("serial")
public class StdOutIndexer extends AbstractIndexerBolt {
    OutputCollector _collector;

    @SuppressWarnings("rawtypes")
    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
        super.prepare(conf, context, collector);
        _collector = collector;
    }

    @Override
    public void execute(Tuple tuple) {
        String url = tuple.getStringByField("url");

        // Distinguish the value used for indexing
        // from the one used for the status
        String normalisedurl = valueForURL(tuple);

        Metadata metadata = (Metadata) tuple.getValueByField("metadata");

        // should this document be kept?
        boolean keep = filterDocument(metadata);
        if (!keep) {
            // treat it as successfully processed even if
            // we do not index it
            _collector.emit(StatusStreamName, tuple, new Values(url, metadata, Status.FETCHED));
            _collector.ack(tuple);
            return;
        }

        // display text of the document?
        if (StringUtils.isNotBlank(fieldNameForText())) {
            String text = tuple.getStringByField("text");
            System.out.println(fieldNameForText() + "\t" + trimValue(text));
        }

        if (StringUtils.isNotBlank(fieldNameForURL())) {
            System.out.println(fieldNameForURL() + "\t" + trimValue(normalisedurl));
        }

        // which metadata to display?
        Map<String, String[]> keyVals = filterMetadata(metadata);

        Iterator<String> iterator = keyVals.keySet().iterator();
        while (iterator.hasNext()) {
            String fieldName = iterator.next();
            String[] values = keyVals.get(fieldName);
            for (String value : values) {
                System.out.println(fieldName + "\t" + trimValue(value));
            }
        }

        _collector.emit(StatusStreamName, tuple, new Values(url, metadata, Status.FETCHED));
        _collector.ack(tuple);
    }

    private String trimValue(String value) {
        if (value.length() > 100) return value.length() + " chars";
        return value;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.indexing;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.persistence.Status;
import java.util.Map;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.tuple.Values;

/**
 * * Any tuple that went through all the previous bolts is sent to the status stream with a Status
 * of FETCHED. This allows the bolt in charge of storing the status to rely exclusively on the
 * status stream, as done with the real indexers.
 */
@SuppressWarnings("serial")
public class DummyIndexer extends AbstractIndexerBolt {
    OutputCollector _collector;

    @SuppressWarnings("rawtypes")
    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
        super.prepare(conf, context, collector);
        _collector = collector;
    }

    @Override
    public void execute(Tuple tuple) {
        String url = tuple.getStringByField("url");
        Metadata metadata = (Metadata) tuple.getValueByField("metadata");

        _collector.emit(
                com.digitalpebble.stormcrawler.Constants.StatusStreamName,
                tuple,
                new Values(url, metadata, Status.FETCHED));
        _collector.ack(tuple);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.indexing;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import com.digitalpebble.stormcrawler.util.RobotsTags;
import com.digitalpebble.stormcrawler.util.URLUtil;
import crawlercommons.domains.PaidLevelDomain;
import java.net.MalformedURLException;
import java.net.URL;
import java.util.HashMap;
import java.util.Iterator;
import java.util.Map;
import java.util.Map.Entry;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
import org.apache.commons.lang.ArrayUtils;
import org.apache.commons.lang.StringUtils;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.topology.base.BaseRichBolt;
import org.apache.storm.tuple.Fields;
import org.apache.storm.tuple.Tuple;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/** Abstract class to simplify writing IndexerBolts * */
@SuppressWarnings("serial")
public abstract class AbstractIndexerBolt extends BaseRichBolt {

    private final Logger LOG = LoggerFactory.getLogger(getClass());

    /**
     * Mapping between metadata keys and field names for indexing Can be a list of values separated
     * by a = or a single string
     */
    public static final String metadata2fieldParamName = "indexer.md.mapping";

    /**
     * list of metadata key + values to be used as a filter. A document will be indexed only if it
     * has such a md. Can be null in which case we don't filter at all.
     */
    public static final String metadataFilterParamName = "indexer.md.filter";

    /** Field name to use for storing the text of a document * */
    public static final String textFieldParamName = "indexer.text.fieldname";

    /** Trim length of text to index. Defaults to -1 to keep it intact * */
    public static final String textLengthParamName = "indexer.text.maxlength";

    /** Field name to use for storing the url of a document * */
    public static final String urlFieldParamName = "indexer.url.fieldname";

    /** Field name to use for reading the canonical property of the metadata */
    public static final String canonicalMetadataParamName = "indexer.canonical.name";

    private String[] filterKeyValue = null;

    private Map<String, String> metadata2field = new HashMap<>();

    private String fieldNameForText = null;

    private int maxLengthText = -1;

    private String fieldNameForURL = null;

    private String canonicalMetadataName = null;

    @SuppressWarnings({"rawtypes", "unchecked"})
    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollector collector) {

        String mdF = ConfUtils.getString(conf, metadataFilterParamName);
        if (StringUtils.isNotBlank(mdF)) {
            // split it in key value
            int equals = mdF.indexOf('=');
            if (equals != -1) {
                String key = mdF.substring(0, equals);
                String value = mdF.substring(equals + 1);
                filterKeyValue = new String[] {key.trim(), value.trim()};
            } else {
                LOG.error("Can't split into key value : {}", mdF);
            }
        }

        fieldNameForText = ConfUtils.getString(conf, textFieldParamName);

        maxLengthText = ConfUtils.getInt(conf, textLengthParamName, -1);

        fieldNameForURL = ConfUtils.getString(conf, urlFieldParamName);

        canonicalMetadataName = ConfUtils.getString(conf, canonicalMetadataParamName);

        for (String mapping : ConfUtils.loadListFromConf(metadata2fieldParamName, conf)) {
            int equals = mapping.indexOf('=');
            if (equals != -1) {
                String key = mapping.substring(0, equals).trim();
                String value = mapping.substring(equals + 1).trim();
                metadata2field.put(key, value);
                LOG.info("Mapping key {} to field {}", key, value);
            } else {
                mapping = mapping.trim();
                metadata2field.put(mapping, mapping);
                LOG.info("Mapping key {} to field {}", mapping, mapping);
            }
        }
    }

    /**
     * Determine whether a document should be indexed based on the presence of a given key/value or
     * the RobotsTags.ROBOTS_NO_INDEX directive.
     *
     * @return true if the document should be kept.
     */
    protected boolean filterDocument(Metadata meta) {
        String noindexVal = meta.getFirstValue(RobotsTags.ROBOTS_NO_INDEX);
        if ("true".equalsIgnoreCase(noindexVal)) return false;

        if (filterKeyValue == null) return true;
        String[] values = meta.getValues(filterKeyValue[0]);
        // key not found
        if (values == null) return false;
        return ArrayUtils.contains(values, filterKeyValue[1]);
    }

    /** Returns a mapping field name / values for the metadata to index * */
    protected Map<String, String[]> filterMetadata(Metadata meta) {

        Pattern indexValuePattern = Pattern.compile("\\[(\\d+)\\]");

        Map<String, String[]> fieldVals = new HashMap<>();
        Iterator<Entry<String, String>> iter = metadata2field.entrySet().iterator();
        while (iter.hasNext()) {
            Entry<String, String> entry = iter.next();
            // check whether we want a specific value or all of them?
            int index = -1;
            String key = entry.getKey();
            Matcher match = indexValuePattern.matcher(key);
            if (match.find()) {
                index = Integer.parseInt(match.group(1));
                key = key.substring(0, match.start());
            }
            String[] values = meta.getValues(key);
            // not found
            if (values == null || values.length == 0) continue;
            // want a value index that it outside the range given
            if (index >= values.length) continue;
            // store all values available
            if (index == -1) fieldVals.put(entry.getValue(), values);
            // or only the one we want
            else fieldVals.put(entry.getValue(), new String[] {values[index]});
        }

        return fieldVals;
    }

    /**
     * Returns the value to be used as the URL for indexing purposes, if present the canonical value
     * is used instead
     */
    protected String valueForURL(Tuple tuple) {

        String url = tuple.getStringByField("url");
        Metadata metadata = (Metadata) tuple.getValueByField("metadata");

        // functionality deactivated
        if (StringUtils.isBlank(canonicalMetadataParamName)) {
            return url;
        }

        String canonicalValue = metadata.getFirstValue(canonicalMetadataName);

        // no value found?
        if (StringUtils.isBlank(canonicalValue)) {
            return url;
        }

        try {
            URL sURL = new URL(url);
            URL canonical = URLUtil.resolveURL(sURL, canonicalValue);

            String sDomain = PaidLevelDomain.getPLD(sURL.getHost());
            String canonicalDomain = PaidLevelDomain.getPLD(canonical.getHost());

            // check that the domain is the same
            if (sDomain.equalsIgnoreCase(canonicalDomain)) {
                return canonical.toExternalForm();
            } else {
                LOG.info("Canonical URL references a different domain, ignoring in {} ", url);
            }
        } catch (MalformedURLException e) {
            LOG.error("Malformed canonical URL {} was found in {} ", canonicalValue, url);
        }

        return url;
    }

    /** Returns the field name to use for the text or null if the text must not be indexed */
    protected String fieldNameForText() {
        return fieldNameForText;
    }

    /**
     * Returns a trimmed string or the original one if it is below the threshold set in the
     * configuration.
     */
    protected String trimText(String text) {
        if (maxLengthText == -1) return text;
        if (text == null) return text;
        if (text.length() <= maxLengthText) return text;
        return text.substring(0, maxLengthText);
    }

    /** Returns the field name to use for the URL or null if the URL must not be indexed */
    protected String fieldNameForURL() {
        return fieldNameForURL;
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declareStream(
                com.digitalpebble.stormcrawler.Constants.StatusStreamName,
                new Fields("url", "metadata", "status"));
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler;

public class Constants {

    public static final String PARTITION_MODEParamName = "partition.url.mode";

    public static final String PARTITION_MODE_HOST = "byHost";
    public static final String PARTITION_MODE_DOMAIN = "byDomain";
    public static final String PARTITION_MODE_IP = "byIP";

    public static final String STATUS_ERROR_MESSAGE = "error.message";
    public static final String STATUS_ERROR_SOURCE = "error.source";
    public static final String STATUS_ERROR_CAUSE = "error.cause";

    public static final String StatusStreamName = "status";

    public static final String DELETION_STREAM_NAME = "deletion";

    public static final String AllowRedirParamName = "redirections.allowed";

    // when to retry a URL with a fetch error
    public static final String fetchErrorFetchIntervalParamName = "fetchInterval.fetch.error";

    // when to retry a URL with an error, i.e. something very wrong with it
    // set a very large value so that it does not get refetched soon
    public static final String errorFetchIntervalParamName = "fetchInterval.error";

    // when to retry a successful URL by default
    public static final String defaultFetchIntervalParamName = "fetchInterval.default";

    public static final String fetchErrorCountParamName = "fetch.error.count";

    /** Maximum array size, safe value on any JVM */
    public static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;

    private Constants() {}
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.jsoup;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.parse.JSoupFilter;
import com.digitalpebble.stormcrawler.parse.ParseData;
import com.digitalpebble.stormcrawler.parse.ParseResult;
import com.fasterxml.jackson.core.JsonPointer;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import org.jsoup.nodes.Document;
import org.jsoup.nodes.Element;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Extracts data from JSON-LD representation (https://json-ld.org/). Illustrates how to use the
 * JSoupFilters
 */
public class LDJsonParseFilter extends JSoupFilter {

    public static final Logger LOG = LoggerFactory.getLogger(LDJsonParseFilter.class);

    private static ObjectMapper mapper = new ObjectMapper();

    private List<LabelledJsonPointer> expressions = new LinkedList<>();

    public static JsonNode filterJson(Document doc) throws Exception {

        Element el = doc.selectFirst("script[type=application/ld+json]");
        if (el == null) {
            return null;
        }
        return mapper.readValue(el.data(), JsonNode.class);
    }

    public void configure(@SuppressWarnings("rawtypes") Map stormConf, JsonNode filterParams) {
        java.util.Iterator<Entry<String, JsonNode>> iter = filterParams.fields();
        while (iter.hasNext()) {
            Entry<String, JsonNode> entry = iter.next();
            String key = entry.getKey();
            JsonNode node = entry.getValue();
            LabelledJsonPointer labelP =
                    new LabelledJsonPointer(key, JsonPointer.valueOf(node.asText()));
            expressions.add(labelP);
        }
    }

    class LabelledJsonPointer {

        String label;
        JsonPointer pointer;

        public LabelledJsonPointer(String label, JsonPointer pointer) {
            this.label = label;
            this.pointer = pointer;
        }

        @Override
        public String toString() {
            return label + " => " + pointer.toString();
        }
    }

    @Override
    public void filter(String URL, byte[] content, Document doc, ParseResult parse) {
        if (doc == null) {
            return;
        }
        try {
            JsonNode json = filterJson(doc);
            if (json == null) {
                return;
            }

            ParseData parseData = parse.get(URL);
            Metadata metadata = parseData.getMetadata();

            // extract patterns and store as metadata
            for (LabelledJsonPointer expression : expressions) {
                JsonNode match = json.at(expression.pointer);
                if (match.isMissingNode()) {
                    continue;
                }
                metadata.addValue(expression.label, match.asText());
            }

        } catch (Exception e) {
            LOG.error("Exception caught when extracting json", e);
        }
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.filtering;

import com.digitalpebble.stormcrawler.JSONResource;
import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import com.digitalpebble.stormcrawler.util.Configurable;
import com.fasterxml.jackson.core.JsonParseException;
import com.fasterxml.jackson.databind.JsonMappingException;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import java.io.IOException;
import java.io.InputStream;
import java.net.URL;
import java.util.List;
import java.util.Map;
import org.apache.commons.lang.StringUtils;
import org.slf4j.LoggerFactory;

/** Wrapper for the URLFilters defined in a JSON configuration */
public class URLFilters implements URLFilter, JSONResource {

    public static final URLFilters emptyURLFilters = new URLFilters();

    private static final org.slf4j.Logger LOG = LoggerFactory.getLogger(URLFilters.class);

    private URLFilter[] filters;

    private URLFilters() {
        filters = new URLFilters[0];
    }

    private String configFile = "urlfilters.config.file";

    private Map stormConf;

    /**
     * Loads and configure the URLFilters based on the storm config if there is one otherwise
     * returns an empty URLFilter.
     */
    public static URLFilters fromConf(Map stormConf) {

        String configFile = ConfUtils.getString(stormConf, "urlfilters.config.file");
        if (StringUtils.isNotBlank(configFile)) {
            try {
                return new URLFilters(stormConf, configFile);
            } catch (IOException e) {
                String message = "Exception caught while loading the URLFilters from " + configFile;
                LOG.error(message);
                throw new RuntimeException(message, e);
            }
        }

        return URLFilters.emptyURLFilters;
    }

    /**
     * Loads the filters from a JSON configuration file
     *
     * @throws IOException
     */
    public URLFilters(Map stormConf, String configFile) throws IOException {
        this.configFile = configFile;
        this.stormConf = stormConf;
        try {
            loadJSONResources();
        } catch (Exception e) {
            throw new IOException("Unable to build JSON object from file", e);
        }
    }

    @Override
    public void loadJSONResources(InputStream inputStream)
            throws JsonParseException, JsonMappingException, IOException {
        ObjectMapper mapper = new ObjectMapper();
        JsonNode confNode = mapper.readValue(inputStream, JsonNode.class);
        configure(stormConf, confNode);
    }

    @Override
    public String filter(URL sourceUrl, Metadata sourceMetadata, String urlToFilter) {
        String normalizedURL = urlToFilter;
        try {
            for (URLFilter filter : filters) {
                long start = System.currentTimeMillis();
                normalizedURL = filter.filter(sourceUrl, sourceMetadata, normalizedURL);
                long end = System.currentTimeMillis();
                LOG.debug("URLFilter {} took {} msec", filter.getClass().getName(), end - start);
                if (normalizedURL == null) break;
            }
        } catch (Exception e) {
            LOG.error("URL filtering threw exception", e);
        }
        return normalizedURL;
    }

    @Override
    public String getResourceFile() {
        return this.configFile;
    }

    @SuppressWarnings("rawtypes")
    @Override
    public void configure(Map stormConf, JsonNode filtersConf) {
        List<URLFilter> list =
                Configurable.configure(
                        stormConf, filtersConf, URLFilter.class, this.getClass().getName());
        filters = list.toArray(new URLFilter[list.size()]);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.filtering.host;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.filtering.URLFilter;
import com.fasterxml.jackson.databind.JsonNode;
import crawlercommons.domains.PaidLevelDomain;
import java.net.MalformedURLException;
import java.net.URL;
import java.util.Map;

/**
 * Filters URL based on the hostname.
 *
 * <p>This filter has 2 modes:
 *
 * <ul>
 *   <li>if <code>ignoreOutsideHost</code> is <code>true</code>, all URLs with a host different from
 *       the host of the source URL are filtered out
 *   <li>if <code>ignoreOutsideDomain</code> is <code>true</code>, all URLs with a domain different
 *       from the source's domain are filtered out
 * </ul>
 */
public class HostURLFilter implements URLFilter {

    private boolean ignoreOutsideHost;
    private boolean ignoreOutsideDomain;

    private URL previousSourceUrl;
    private String previousSourceHost;
    private String previousSourceDomain;

    @Override
    public void configure(Map stormConf, JsonNode filterParams) {
        JsonNode filterByHostNode = filterParams.get("ignoreOutsideHost");
        if (filterByHostNode == null) {
            ignoreOutsideHost = false;
        } else {
            ignoreOutsideHost = filterByHostNode.asBoolean(false);
        }

        // ignoreOutsideDomain is not necessary if we require the host to be
        // always the same
        if (!ignoreOutsideHost) {
            JsonNode filterByDomainNode = filterParams.get("ignoreOutsideDomain");
            if (filterByHostNode == null) {
                ignoreOutsideDomain = false;
            } else {
                ignoreOutsideDomain = filterByDomainNode.asBoolean(false);
            }
        } else {
            ignoreOutsideDomain = false;
        }
    }

    @Override
    public String filter(URL sourceUrl, Metadata sourceMetadata, String urlToFilter) {
        if (sourceUrl == null || (!ignoreOutsideHost && !ignoreOutsideDomain)) {
            return urlToFilter;
        }

        URL tURL;
        try {
            tURL = new URL(urlToFilter);
        } catch (MalformedURLException e1) {
            return null;
        }

        String fromHost;
        String fromDomain = null;
        // Using identity comparison because URL.equals performs poorly
        if (sourceUrl == previousSourceUrl) {
            fromHost = previousSourceHost;
            if (ignoreOutsideDomain) {
                fromDomain = previousSourceDomain;
            }
        } else {
            fromHost = sourceUrl.getHost();
            if (ignoreOutsideDomain) {
                fromDomain = PaidLevelDomain.getPLD(fromHost);
            }
            previousSourceHost = fromHost;
            previousSourceDomain = fromDomain;
            previousSourceUrl = sourceUrl;
        }

        // resolve the hosts
        String toHost = tURL.getHost();

        if (ignoreOutsideHost) {
            if (toHost == null || !toHost.equalsIgnoreCase(fromHost)) {
                return null;
            }
        }

        if (ignoreOutsideDomain) {
            String toDomain = PaidLevelDomain.getPLD(toHost);
            if (toDomain == null || !toDomain.equals(fromDomain)) {
                return null;
            }
        }

        return urlToFilter;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.filtering.depth;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.filtering.URLFilter;
import com.digitalpebble.stormcrawler.util.MetadataTransfer;
import com.fasterxml.jackson.databind.JsonNode;
import java.net.URL;
import java.util.Map;
import org.apache.commons.lang.StringUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Filter out URLs whose depth is greater than maxDepth. If its value is set to 0 then no outlinks
 * are followed at all.
 */
public class MaxDepthFilter implements URLFilter {

    private static final Logger LOG = LoggerFactory.getLogger(MaxDepthFilter.class);

    private int maxDepth;

    @Override
    public void configure(Map stormConf, JsonNode paramNode) {
        JsonNode node = paramNode.get("maxDepth");
        if (node != null && node.isInt()) {
            maxDepth = node.intValue();
        } else {
            maxDepth = -1;
            LOG.warn("maxDepth parameter not found");
        }
        LOG.info("maxDepth set to {}", maxDepth);
    }

    @Override
    public String filter(URL pageUrl, Metadata sourceMetadata, String url) {
        int depth = getDepth(sourceMetadata, MetadataTransfer.depthKeyName);
        // is there a custom value set for this particular URL?
        int customMax = getDepth(sourceMetadata, MetadataTransfer.maxDepthKeyName);
        if (customMax >= 0) {
            return filter(depth, customMax, url);
        }
        // rely on the default max otherwise
        else if (maxDepth >= 0) {
            return filter(depth, maxDepth, url);
        }
        return url;
    }

    private String filter(final int depth, final int max, final String url) {
        // deactivate the outlink no matter what the depth is
        if (max == 0) {
            return null;
        }
        if (depth >= max) {
            LOG.debug("filtered out {} - depth {} >= {}", url, depth, maxDepth);
            return null;
        }
        return url;
    }

    private int getDepth(Metadata sourceMetadata, String key) {
        if (sourceMetadata == null) {
            return -1;
        }
        String depth = sourceMetadata.getFirstValue(key);
        if (StringUtils.isNumeric(depth)) {
            return Integer.parseInt(depth);
        } else {
            return -1;
        }
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.filtering.regex;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.filtering.URLFilter;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.node.ArrayNode;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.io.Reader;
import java.net.URL;
import java.nio.charset.StandardCharsets;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
import java.util.regex.PatternSyntaxException;
import javax.xml.parsers.DocumentBuilderFactory;
import org.apache.commons.lang.StringUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.w3c.dom.Document;
import org.w3c.dom.Element;
import org.w3c.dom.Node;
import org.w3c.dom.NodeList;
import org.w3c.dom.Text;
import org.xml.sax.InputSource;

/**
 * The RegexURLNormalizer is a URL filter that normalizes URLs by matching a regular expression and
 * inserting a replacement string.
 *
 * <p>Adapted from Apache Nutch 1.9.
 */
public class RegexURLNormalizer implements URLFilter {

    private static final Logger LOG = LoggerFactory.getLogger(RegexURLNormalizer.class);

    /** Class which holds a compiled pattern and its corresponding substitution string. */
    private static class Rule {
        public Pattern pattern;

        public String substitution;
    }

    private List<Rule> rules;

    private static final List<Rule> EMPTY_RULES = Collections.emptyList();

    @Override
    public void configure(Map stormConf, JsonNode paramNode) {
        JsonNode node = paramNode.get("urlNormalizers");
        if (node != null && node.isArray()) {
            rules = readRules((ArrayNode) node);
        } else {
            JsonNode filenameNode = paramNode.get("regexNormalizerFile");
            String rulesFileName;
            if (filenameNode != null) {
                rulesFileName = filenameNode.textValue();
            } else {
                rulesFileName = "default-regex-normalizers.xml";
            }
            rules = readRules(rulesFileName);
        }
    }

    /**
     * This function does the replacements by iterating through all the regex patterns. It accepts a
     * string url as input and returns the altered string. If the normalized url is an empty string,
     * the function will return null.
     */
    @Override
    public String filter(URL sourceUrl, Metadata sourceMetadata, String urlString) {

        Iterator<Rule> i = rules.iterator();
        while (i.hasNext()) {
            Rule r = i.next();

            Matcher matcher = r.pattern.matcher(urlString);

            urlString = matcher.replaceAll(r.substitution);
        }

        if (urlString.equals("")) {
            urlString = null;
        }

        return urlString;
    }

    /** Populates a List of Rules off of JsonNode. */
    private List<Rule> readRules(ArrayNode rulesList) {
        List<Rule> rules = new ArrayList<>();
        for (JsonNode regexNode : rulesList) {
            if (regexNode == null || regexNode.isNull()) {
                LOG.warn("bad config: 'regex' element is null");
                continue;
            }
            JsonNode patternNode = regexNode.get("pattern");
            JsonNode substitutionNode = regexNode.get("substitution");

            String substitutionValue = "";
            if (substitutionNode != null) {
                substitutionValue = substitutionNode.asText();
            }
            if (patternNode != null && StringUtils.isNotBlank(patternNode.asText())) {
                Rule rule = createRule(patternNode.asText(), substitutionValue);
                if (rule != null) {
                    rules.add(rule);
                }
            }
        }
        if (rules.size() == 0) {
            rules = EMPTY_RULES;
        }
        return rules;
    }

    /** Reads the configuration file and populates a List of Rules. */
    private List<Rule> readRules(String rulesFile) {
        try {
            InputStream regexStream = getClass().getClassLoader().getResourceAsStream(rulesFile);
            Reader reader = new InputStreamReader(regexStream, StandardCharsets.UTF_8);
            return readConfiguration(reader);
        } catch (Exception e) {
            LOG.error("Error loading rules from file: {}", e);
            return EMPTY_RULES;
        }
    }

    private List<Rule> readConfiguration(Reader reader) {
        List<Rule> rules = new ArrayList<>();
        try {

            // borrowed heavily from code in Configuration.java
            Document doc =
                    DocumentBuilderFactory.newInstance()
                            .newDocumentBuilder()
                            .parse(new InputSource(reader));
            Element root = doc.getDocumentElement();
            if ((!"regex-normalize".equals(root.getTagName())) && (LOG.isErrorEnabled())) {
                LOG.error("bad conf file: top-level element not <regex-normalize>");
            }
            NodeList regexes = root.getChildNodes();
            for (int i = 0; i < regexes.getLength(); i++) {
                Node regexNode = regexes.item(i);
                if (!(regexNode instanceof Element)) {
                    continue;
                }
                Element regex = (Element) regexNode;
                if ((!"regex".equals(regex.getTagName())) && (LOG.isWarnEnabled())) {
                    LOG.warn("bad conf file: element not <regex>");
                }
                NodeList fields = regex.getChildNodes();
                String patternValue = null;
                String subValue = null;
                for (int j = 0; j < fields.getLength(); j++) {
                    Node fieldNode = fields.item(j);
                    if (!(fieldNode instanceof Element)) {
                        continue;
                    }
                    Element field = (Element) fieldNode;
                    if ("pattern".equals(field.getTagName()) && field.hasChildNodes()) {
                        patternValue = ((Text) field.getFirstChild()).getData();
                    }
                    if ("substitution".equals(field.getTagName()) && field.hasChildNodes()) {
                        subValue = ((Text) field.getFirstChild()).getData();
                    }
                    if (!field.hasChildNodes()) {
                        subValue = "";
                    }
                }
                if (patternValue != null && subValue != null) {
                    Rule rule = createRule(patternValue, subValue);
                    rules.add(rule);
                }
            }
        } catch (Exception e) {
            LOG.error("error parsing conf file", e);
            return EMPTY_RULES;
        }
        if (rules.size() == 0) {
            return EMPTY_RULES;
        }
        return rules;
    }

    private Rule createRule(String patternValue, String subValue) {
        Rule rule = new Rule();
        try {
            rule.pattern = Pattern.compile(patternValue);
        } catch (PatternSyntaxException e) {
            LOG.error(
                    "skipped rule: {} -> {} : invalid regular expression pattern" + patternValue,
                    subValue,
                    e);
            return null;
        }
        rule.substitution = subValue;
        return rule;
    }

    /**
     * Utility method to test rules against an input. the first arg is the absolute path of the
     * rules file, the second is the URL to be normalised
     */
    public static void main(String args[]) throws FileNotFoundException {
        RegexURLNormalizer normalizer = new RegexURLNormalizer();
        normalizer.rules = normalizer.readConfiguration(new FileReader(args[0]));

        String output = normalizer.filter(null, null, args[1]);

        System.out.println(args[1] + "\n->\n" + output);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.filtering.regex;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.filtering.URLFilter;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.node.ArrayNode;
import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.io.Reader;
import java.net.URL;
import java.nio.charset.StandardCharsets;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/** An abstract class for implementing Regex URL filtering. Adapted from Apache Nutch 1.9 */
public abstract class RegexURLFilterBase implements URLFilter {

    private static final Logger LOG = LoggerFactory.getLogger(RegexURLFilterBase.class);

    /** A list of applicable rules */
    private List<RegexRule> rules;

    @Override
    public void configure(Map stormConf, JsonNode paramNode) {
        JsonNode node = paramNode.get("urlFilters");
        if (node != null && node.isArray()) {
            rules = readRules((ArrayNode) node);
        } else {
            JsonNode filenameNode = paramNode.get("regexFilterFile");
            String rulesFileName;
            if (filenameNode != null) {
                rulesFileName = filenameNode.textValue();
            } else {
                rulesFileName = "default-regex-filters.txt";
            }
            rules = readRules(rulesFileName);
        }
    }

    /** Populates a List of Rules off of JsonNode. */
    private List<RegexRule> readRules(ArrayNode rulesList) {
        List<RegexRule> rules = new ArrayList<>();
        for (JsonNode urlFilterNode : rulesList) {
            try {
                RegexRule rule = createRule(urlFilterNode.asText());
                if (rule != null) {
                    rules.add(rule);
                }
            } catch (IOException e) {
                LOG.error("There was an error reading regex filter {}", urlFilterNode.asText(), e);
            }
        }
        return rules;
    }

    private List<RegexRule> readRules(String rulesFile) {
        List<RegexRule> rules = new ArrayList<>();

        try {
            InputStream regexStream = getClass().getClassLoader().getResourceAsStream(rulesFile);
            Reader reader = new InputStreamReader(regexStream, StandardCharsets.UTF_8);
            BufferedReader in = new BufferedReader(reader);
            String line;

            while ((line = in.readLine()) != null) {
                if (line.length() == 0) {
                    continue;
                }
                RegexRule rule = createRule(line);
                if (rule != null) {
                    rules.add(rule);
                }
            }
        } catch (IOException e) {
            LOG.error("There was an error reading the default-regex-filters file");
            e.printStackTrace();
        }
        return rules;
    }

    private RegexRule createRule(String line) throws IOException {
        char first = line.charAt(0);
        boolean sign;
        switch (first) {
            case '+':
                sign = true;
                break;
            case '-':
                sign = false;
                break;
            case ' ':
            case '\n':
            case '#': // skip blank & comment lines
                return null;
            default:
                throw new IOException("Invalid first character: " + line);
        }

        String regex = line.substring(1);
        LOG.trace("Adding rule [{}]", regex);
        RegexRule rule = createRule(sign, regex);
        return rule;
    }

    /**
     * Creates a new {@link RegexRule}.
     *
     * @param sign of the regular expression. A <code>true</code> value means that any URL matching
     *     this rule must be included, whereas a <code>false</code> value means that any URL
     *     matching this rule must be excluded.
     * @param regex is the regular expression associated to this rule.
     */
    protected abstract RegexRule createRule(boolean sign, String regex);

    /*
     * -------------------------- * <implementation:URLFilter> *
     * --------------------------
     */

    @Override
    public String filter(URL pageUrl, Metadata sourceMetadata, String url) {
        for (RegexRule rule : rules) {
            if (rule.match(url)) {
                return rule.accept() ? url : null;
            }
        }
        return null;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.filtering.regex;

/** A generic regular expression rule. Borrowed from Apache Nutch 1.9. */
public abstract class RegexRule {

    private final boolean sign;

    /**
     * Constructs a new regular expression rule.
     *
     * @param sign specifies if this rule must filter-in or filter-out. A <code>true</code> value
     *     means that any url matching this rule must be accepted, a <code>false</code> value means
     *     that any url matching this rule must be rejected.
     * @param regex is the regular expression used for matching (see {@link #match(String)} method).
     */
    protected RegexRule(boolean sign, String regex) {
        this.sign = sign;
    }

    /**
     * Return if this rule is used for filtering-in or out.
     *
     * @return <code>true</code> if any url matching this rule must be accepted, otherwise <code>
     *     false</code>.
     */
    protected boolean accept() {
        return sign;
    }

    /**
     * Checks if a url matches this rule.
     *
     * @param url is the url to check.
     * @return <code>true</code> if the specified url matches this rule, otherwise <code>false
     *     </code>.
     */
    protected abstract boolean match(String url);
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.filtering.regex;

import java.util.regex.Pattern;

/**
 * Filters URLs based on a file of regular expressions using the {@link java.util.regex Java Regex
 * implementation}.
 *
 * <p>Adapted from Apache Nutch 1.9
 */
public class RegexURLFilter extends RegexURLFilterBase {

    public RegexURLFilter() {
        super();
    }

    // Inherited Javadoc
    @Override
    protected RegexRule createRule(boolean sign, String regex) {
        return new Rule(sign, regex);
    }

    private class Rule extends RegexRule {

        private Pattern pattern;

        Rule(boolean sign, String regex) {
            super(sign, regex);
            pattern = Pattern.compile(regex);
        }

        @Override
        protected boolean match(String url) {
            return pattern.matcher(url).find();
        }
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.filtering.regex;

import com.digitalpebble.stormcrawler.JSONResource;
import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.filtering.URLFilter;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import com.fasterxml.jackson.core.JsonParseException;
import com.fasterxml.jackson.databind.JsonMappingException;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import java.io.IOException;
import java.io.InputStream;
import java.net.MalformedURLException;
import java.net.URL;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.Iterator;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.regex.Pattern;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * URL filter based on regex patterns and organised by [host | domain | metadata | global]. For a
 * given URL, the scopes are tried in the order given above and the URL is kept or removed based on
 * the first matching rule. The default policy is to accept a URL if no matches are found.
 *
 * <p>The resource file is in JSON and at the following format.
 *
 * <pre>
 * [{
 *         "scope": "GLOBAL",
 *         "patterns": [
 *             "DenyPathQuery \\.jpg"
 *         ]
 *     },
 *     {
 *         "scope": "domain:stormcrawler.net",
 *         "patterns": [
 *             "AllowPath /digitalpebble/",
 *             "DenyPath .+"
 *         ]
 *     },
 *     {
 *         "scope": "metadata:key=value",
 *         "patterns": [
 *             "DenyPath .+"
 *         ]
 *     }
 * ]
 * </pre>
 *
 * Partly inspired by https://github.com/commoncrawl/nutch/blob/cc-fast-url-filter
 * /src/plugin/urlfilter -fast/src/java/org/apache/nutch/urlfilter/fast/FastURLFilter.java
 */
public class FastURLFilter implements URLFilter, JSONResource {

    public static final Logger LOG = LoggerFactory.getLogger(FastURLFilter.class);

    private String resourceFile;

    private Rules rules = new Rules();

    private static final ObjectMapper objectMapper = new ObjectMapper();

    public void configure(@SuppressWarnings("rawtypes") Map stormConf, JsonNode filterParams) {

        if (filterParams != null) {
            JsonNode node = filterParams.get("file");
            if (node != null && node.isTextual()) {
                this.resourceFile = node.asText("fast.urlfilter.json");
            }
        }

        // config via json failed - trying from global config
        if (this.resourceFile == null) {
            this.resourceFile =
                    ConfUtils.getString(stormConf, "fast.urlfilter.file", "fast.urlfilter.json");
        }

        try {
            loadJSONResources();
        } catch (Exception e) {
            LOG.error("Exception while loading JSON resources from jar", e);
            throw new RuntimeException(e);
        }
    }

    @Override
    public String getResourceFile() {
        return resourceFile;
    }

    @Override
    public void loadJSONResources(InputStream inputStream)
            throws JsonParseException, JsonMappingException, IOException {

        JsonNode rootNode = objectMapper.readTree(inputStream);
        Rules rules = new Rules();
        Iterator<JsonNode> iter = rootNode.elements();
        while (iter.hasNext()) {
            JsonNode current = iter.next();
            Scope scope = new Scope();
            String scopeval = current.get("scope").asText();
            scopeval = scopeval.trim();
            int offset = 0;
            Scope.Type type;
            String value = null;
            // separate the type from the pattern
            if (scopeval.equals("GLOBAL")) {
                type = Scope.Type.GLOBAL;
            } else if (scopeval.startsWith("domain:")) {
                type = Scope.Type.DOMAIN;
                offset = "domain:".length();
                value = scopeval.substring(offset);
            } else if (scopeval.startsWith("host:")) {
                type = Scope.Type.HOSTNAME;
                offset = "host:".length();
                value = scopeval.substring(offset);
            } else if (scopeval.startsWith("metadata:")) {
                type = Scope.Type.METADATA;
                offset = "metadata:".length();
                value = scopeval.substring(offset);
            } else throw new RuntimeException("Invalid scope: " + scopeval);

            JsonNode patternsNode = current.get("patterns");
            if (patternsNode == null)
                throw new RuntimeException("Missing patterns for scope" + scopeval);

            List<Rule> rlist = new LinkedList<>();

            Iterator<JsonNode> iterPatterns = patternsNode.elements();
            while (iterPatterns.hasNext()) {
                JsonNode patternNode = iterPatterns.next();
                rlist.add(new Rule(patternNode.asText()));
            }

            scope.setRules(rlist);

            rules.addScope(scope, type, value);
        }

        this.rules = rules;
    }

    @Override
    public String filter(URL sourceUrl, Metadata sourceMetadata, String urlToFilter) {
        try {
            if (rules.filter(urlToFilter, sourceMetadata)) return null;
        } catch (MalformedURLException e) {
            return null;
        }
        return urlToFilter;
    }
}

class Rules {

    private Scope globalRules;
    private Map<String, Scope> domainRules = new HashMap<>();
    private Map<String, Scope> hostNameRules = new HashMap<>();
    private List<MDScope> metadataRules = new ArrayList<>();

    public void addScope(Scope s, Scope.Type t, String value) {
        if (t.equals(Scope.Type.GLOBAL)) {
            globalRules = s;
        } else if (t.equals(Scope.Type.DOMAIN)) {
            domainRules.put(value, s);
        } else if (t.equals(Scope.Type.HOSTNAME)) {
            hostNameRules.put(value, s);
        } else if (t.equals(Scope.Type.METADATA)) {
            metadataRules.add(new MDScope(value, s.getRules()));
        }
    }

    /**
     * Try the rules from the hostname, domain name, metadata and global scopes in this order.
     * Returns true if the URL should be removed, false otherwise. The value returns the value of
     * the first matching rule, be it positive or negative.
     *
     * @throws MalformedURLException
     */
    public boolean filter(String url, Metadata metadata) throws MalformedURLException {
        URL u = new URL(url);

        // first try the full hostname
        String hostname = u.getHost();
        if (checkScope(hostNameRules.get(hostname), u)) {
            return true;
        }

        // then on the various components of the domain
        String[] domainParts = hostname.split("\\.");
        String domain = null;
        for (int i = domainParts.length - 1; i >= 0; i--) {
            domain = domainParts[i] + (domain == null ? "" : "." + domain);
            if (checkScope(domainRules.get(domain), u)) {
                return true;
            }
        }

        // check on parent's URL metadata
        for (MDScope scope : metadataRules) {
            String[] vals = metadata.getValues(scope.getKey());
            if (vals == null) {
                continue;
            }
            for (String v : vals) {
                if (v.equalsIgnoreCase(scope.getValue())) {
                    FastURLFilter.LOG.debug(
                            "Filtering {} matching metadata {}:{}",
                            url,
                            scope.getKey(),
                            scope.getValue());
                    if (checkScope(scope, u)) {
                        return true;
                    }
                }
            }
        }

        if (checkScope(globalRules, u)) {
            return true;
        }

        return false;
    }

    private boolean checkScope(Scope s, URL u) {
        if (s == null) return false;
        for (Rule r : s.getRules()) {
            String haystack = u.getPath();
            // whether to include the query as well?
            if (r.getType().toString().endsWith("QUERY")) {
                if (u.getQuery() != null) {
                    haystack += "?" + u.getQuery();
                }
            }
            if (r.getPattern().matcher(haystack).find()) {
                // matches! returns true for DENY, false for ALLOW
                return r.getType().toString().startsWith("DENY");
            }
        }
        return false;
    }
}

class Scope {

    public enum Type {
        DOMAIN,
        GLOBAL,
        HOSTNAME,
        METADATA
    };

    protected Rule[] rules;

    public void setRules(List<Rule> rlist) {
        this.rules = rlist.toArray(new Rule[rlist.size()]);
    }

    public Rule[] getRules() {
        return rules;
    }
}

class MDScope extends Scope {

    private String key;
    private String value;

    MDScope(String constraint, Rule[] rules) {
        this.rules = rules;
        int eq = constraint.indexOf("=");
        if (eq != -1) {
            key = constraint.substring(0, eq);
            value = constraint.substring(eq + 1);
        } else {
            key = constraint;
        }
    }

    public String getKey() {
        return key;
    }

    public String getValue() {
        return value;
    }
}

class Rule {

    public enum Type {
        DENYPATH,
        DENYPATHQUERY,
        ALLOWPATH,
        ALLOWPATHQUERY
    };

    private Type type;
    private Pattern pattern;

    public Rule(String line) {
        int offset = 0;
        String lcline = line.toLowerCase();
        // separate the type from the pattern
        for (Type t : Type.values()) {
            String start = t.toString().toLowerCase() + " ";
            if (lcline.startsWith(start)) {
                type = t;
                offset = start.length();
                break;
            }
        }
        // no match?
        if (type == null) return;

        String patternString = line.substring(offset).trim();
        pattern = Pattern.compile(patternString);
    }

    public Type getType() {
        return type;
    }

    public Pattern getPattern() {
        return pattern;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.filtering;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.util.Configurable;
import java.net.URL;

/**
 * Unlike Nutch, URLFilters can normalise the URLs as well as filtering them. URLFilter instances
 * should be used via URLFilters
 */
public interface URLFilter extends Configurable {

    /**
     * Returns null if the URL is to be removed or a normalised representation which can correspond
     * to the input URL
     *
     * @param sourceUrl the URL of the page where the URL was found. Can be null.
     * @param sourceMetadata the metadata collected for the page
     * @param urlToFilter the URL to be filtered
     */
    public String filter(URL sourceUrl, Metadata sourceMetadata, String urlToFilter);
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.filtering.metadata;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.filtering.URLFilter;
import com.fasterxml.jackson.databind.JsonNode;
import java.net.URL;
import java.util.Iterator;
import java.util.LinkedList;
import java.util.Map;
import java.util.Map.Entry;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/** Filter out URLs based on metadata in the source document */
public class MetadataFilter implements URLFilter {

    private static final Logger LOG = LoggerFactory.getLogger(MetadataFilter.class);

    private final LinkedList<String[]> mdFilters = new LinkedList<>();

    @Override
    public void configure(Map stormConf, JsonNode paramNode) {
        java.util.Iterator<Entry<String, JsonNode>> iter = paramNode.fields();
        while (iter.hasNext()) {
            Entry<String, JsonNode> entry = iter.next();
            String key = entry.getKey();
            String value = entry.getValue().asText();
            mdFilters.add(new String[] {key, value});
        }
    }

    @Override
    public String filter(URL pageUrl, Metadata sourceMetadata, String urlToFilter) {
        if (sourceMetadata == null) {
            return urlToFilter;
        }
        // check whether any of the metadata can be found in the source
        Iterator<String[]> iter = mdFilters.iterator();
        while (iter.hasNext()) {
            String[] kv = iter.next();
            String[] vals = sourceMetadata.getValues(kv[0]);
            if (vals == null) {
                continue;
            }
            for (String v : vals) {
                if (v.equalsIgnoreCase(kv[1])) {
                    LOG.debug("Filtering {} matching metadata {}:{}", urlToFilter, kv[0], kv[1]);
                    return null;
                }
            }
        }
        return urlToFilter;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.filtering.robots;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.filtering.URLFilter;
import com.digitalpebble.stormcrawler.protocol.HttpRobotRulesParser;
import com.digitalpebble.stormcrawler.protocol.ProtocolFactory;
import com.fasterxml.jackson.databind.JsonNode;
import crawlercommons.robots.BaseRobotRules;
import java.net.MalformedURLException;
import java.net.URL;
import java.util.Map;
import org.apache.storm.Config;

/**
 * URLFilter which discards URLs based on the robots.txt directives. This is meant to be used on
 * small, limited crawls where the number of hosts is finite. Using this on a larger or open crawl
 * would have a negative impact on performance as the filter would try to retrieve the robots.txt
 * files for any host found, unless fromCacheOnly is set to true, in which case the performance will
 * be preserved at the cost of coverage. <br>
 * The filter is configured like so"
 *
 * <pre>
 *  {
 *    "class": "com.digitalpebble.stormcrawler.filtering.robots.RobotsFilter",
 *    "name": "RobotsFilter",
 *    "params": {
 *      "fromCacheOnly": true
 *    }
 *  }
 * </pre>
 */
public class RobotsFilter implements URLFilter {

    private com.digitalpebble.stormcrawler.protocol.HttpRobotRulesParser robots;
    private ProtocolFactory factory;
    private boolean fromCacheOnly = true;

    @Override
    public String filter(URL sourceUrl, Metadata sourceMetadata, String urlToFilter) {
        URL target;
        try {
            target = new URL(urlToFilter);
        } catch (MalformedURLException e) {
            return null;
        }

        BaseRobotRules rules = null;

        if (fromCacheOnly) {
            rules = robots.getRobotRulesSetFromCache(target);
        } else {
            rules = robots.getRobotRulesSet(factory.getProtocol(target), target);
        }

        if (!rules.isAllowed(urlToFilter)) {
            return null;
        }
        return urlToFilter;
    }

    @Override
    public void configure(Map stormConf, JsonNode filterParams) {
        Config conf = new Config();
        conf.putAll(stormConf);
        factory = ProtocolFactory.getInstance(conf);
        robots = new HttpRobotRulesParser(conf);

        JsonNode node = filterParams.get("fromCacheOnly");
        if (node != null && node.isBoolean()) {
            fromCacheOnly = node.booleanValue();
        }
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.filtering.sitemap;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.bolt.SiteMapParserBolt;
import com.digitalpebble.stormcrawler.filtering.URLFilter;
import java.net.URL;

/**
 * URLFilter which discards URLs discovered in a page which is not a sitemap when sitemaps have been
 * found for that site. This allows to restrict the crawl to pages found in the sitemaps but won't
 * affect sites which do not have sitemaps.
 *
 * <pre>
 *  {
 *    "class": "com.digitalpebble.stormcrawler.filtering.sitemap.SitemapFilter",
 *    "name": "SitemapFilter"
 *  }
 * </pre>
 *
 * Will be replaced by <a href=
 * "https://github.com/DigitalPebble/storm-crawler/issues/711">MetadataFilter to filter based on
 * multiple key values</a>
 *
 * @since 1.14
 */
public class SitemapFilter implements URLFilter {

    @Override
    public String filter(URL sourceUrl, Metadata sourceMetadata, String urlToFilter) {
        if (sourceMetadata != null) {
            if ("false"
                            .equalsIgnoreCase(
                                    sourceMetadata.getFirstValue(SiteMapParserBolt.isSitemapKey))
                    && "true"
                            .equalsIgnoreCase(
                                    sourceMetadata.getFirstValue(
                                            SiteMapParserBolt.foundSitemapKey))) return null;
        }
        return urlToFilter;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.filtering.basic;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.filtering.URLFilter;
import com.fasterxml.jackson.databind.JsonNode;
import java.net.URL;
import java.util.Map;

/** Filters links to self * */
public class SelfURLFilter implements URLFilter {

    @Override
    public String filter(URL sourceUrl, Metadata sourceMetadata, String urlToFilter) {

        if (sourceUrl == null) {
            return urlToFilter;
        }

        if (sourceUrl.toExternalForm().equalsIgnoreCase(urlToFilter)) return null;

        return urlToFilter;
    }

    @Override
    public void configure(Map stormConf, JsonNode paramNode) {}
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.filtering.basic;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.filtering.URLFilter;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.node.ArrayNode;
import java.net.IDN;
import java.net.MalformedURLException;
import java.net.URI;
import java.net.URL;
import java.nio.charset.Charset;
import java.nio.charset.StandardCharsets;
import java.util.Collections;
import java.util.Comparator;
import java.util.Iterator;
import java.util.List;
import java.util.Locale;
import java.util.Map;
import java.util.Set;
import java.util.TreeSet;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
import org.apache.commons.lang.StringUtils;
import org.apache.http.NameValuePair;
import org.apache.http.client.utils.URLEncodedUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class BasicURLNormalizer implements URLFilter {

    private static final Logger LOG = LoggerFactory.getLogger(BasicURLNormalizer.class);
    /** Nutch 1098 - finds URL encoded parts of the URL */
    private static final Pattern unescapeRulePattern = Pattern.compile("%([0-9A-Fa-f]{2})");

    /** https://github.com/DigitalPebble/storm-crawler/issues/401 * */
    private static final Pattern illegalEscapePattern = Pattern.compile("%u([0-9A-Fa-f]{4})");

    // charset used for encoding URLs before escaping
    private static final Charset utf8 = Charset.forName("UTF-8");

    /** look-up table for characters which should not be escaped in URL paths */
    private static final boolean[] unescapedCharacters = new boolean[128];

    private static final Pattern thirtytwobithash = Pattern.compile("[a-fA-F\\d]{32}");

    static {
        for (int c = 0; c < 128; c++) {
            /*
             * https://tools.ietf.org/html/rfc3986#section-2.2 For consistency,
             * percent-encoded octets in the ranges of ALPHA (%41-%5A and
             * %61-%7A), DIGIT (%30-%39), hyphen (%2D), period (%2E), underscore
             * (%5F), or tilde (%7E) should not be created by URI producers and,
             * when found in a URI, should be decoded to their corresponding
             * unreserved characters by URI normalizers.
             */
            if ((0x41 <= c && c <= 0x5A)
                    || (0x61 <= c && c <= 0x7A)
                    || (0x30 <= c && c <= 0x39)
                    || c == 0x2D
                    || c == 0x2E
                    || c == 0x5F
                    || c == 0x7E) {
                unescapedCharacters[c] = true;
            } else {
                unescapedCharacters[c] = false;
            }
        }
    }

    boolean removeAnchorPart = true;
    boolean unmangleQueryString = true;
    boolean checkValidURI = true;
    boolean removeHashes = false;
    private boolean hostIDNtoASCII = false;
    final Set<String> queryElementsToRemove = new TreeSet<>();

    @Override
    public String filter(URL sourceUrl, Metadata sourceMetadata, String urlToFilter) {

        urlToFilter = urlToFilter.trim();

        final String originalURL = urlToFilter;

        if (removeAnchorPart) {
            try {
                URL theURL = new URL(urlToFilter);
                String anchor = theURL.getRef();
                if (anchor != null) urlToFilter = urlToFilter.replace("#" + anchor, "");
            } catch (MalformedURLException e) {
                return null;
            }
        }

        if (unmangleQueryString) {
            urlToFilter = unmangleQueryString(urlToFilter);
        }

        if (!queryElementsToRemove.isEmpty() || removeHashes) {
            urlToFilter = processQueryElements(urlToFilter);
        }

        try {
            URL theURL = new URL(urlToFilter);
            String file = theURL.getFile();
            String protocol = theURL.getProtocol();
            String host = theURL.getHost();
            boolean hasChanged = false;

            // lowercased protocol
            if (!urlToFilter.startsWith(protocol)) {
                hasChanged = true;
            }

            if (host != null) {
                String newHost = host.toLowerCase(Locale.ROOT);
                if (hostIDNtoASCII && !isAscii(newHost)) {
                    try {
                        newHost = IDN.toASCII(newHost);
                    } catch (IllegalArgumentException ex) {
                        // eg. if the input string contains non-convertible
                        // Unicode codepoints
                        LOG.error("Failed to convert IDN host {} in {}", newHost, urlToFilter);
                        return null;
                    }
                }
                if (!host.equals(newHost)) {
                    host = newHost;
                    hasChanged = true;
                }
            }

            int port = theURL.getPort();
            // properly encode characters in path/file using percent-encoding
            String file2 = unescapePath(file);
            file2 = escapePath(file2);
            if (!file.equals(file2)) {
                hasChanged = true;
            }
            if (hasChanged) {
                urlToFilter = new URL(protocol, host, port, file2).toString();
            }
        } catch (MalformedURLException e) {
            return null;
        }

        if (checkValidURI) {
            try {
                URI uri = URI.create(urlToFilter);
                urlToFilter = uri.normalize().toString();
            } catch (java.lang.IllegalArgumentException e) {
                LOG.info("Invalid URI {} from {} ", urlToFilter, originalURL);
                return null;
            }
        }

        return urlToFilter;
    }

    @Override
    public void configure(Map stormConf, JsonNode paramNode) {
        JsonNode node = paramNode.get("removeAnchorPart");
        if (node != null) {
            removeAnchorPart = node.booleanValue();
        }

        node = paramNode.get("unmangleQueryString");
        if (node != null) {
            unmangleQueryString = node.booleanValue();
        }

        node = paramNode.get("queryElementsToRemove");
        if (node != null) {
            if (!node.isArray()) {
                LOG.warn(
                        "Failed to configure queryElementsToRemove.  Not an array: {}",
                        node.toString());
            } else {
                ArrayNode array = (ArrayNode) node;
                for (JsonNode element : array) {
                    queryElementsToRemove.add(element.asText());
                }
            }
        }

        node = paramNode.get("checkValidURI");
        if (node != null) {
            checkValidURI = node.booleanValue();
        }

        node = paramNode.get("removeHashes");
        if (node != null) {
            removeHashes = node.booleanValue();
        }

        node = paramNode.get("hostIDNtoASCII");
        if (node != null) {
            hostIDNtoASCII = node.booleanValue();
        }
    }

    /**
     * Basic filter to remove query parameters from urls so parameters that don't change the content
     * of the page can be removed. An example would be a google analytics query parameter like
     * "utm_campaign" which might have several different values for a url that points to the same
     * content. This is also called when removing attributes where the value is a hash.
     */
    private String processQueryElements(String urlToFilter) {
        try {
            // Handle illegal characters by making a url first
            // this will clean illegal characters like |
            URL url = new URL(urlToFilter);

            String query = url.getQuery();
            String path = url.getPath();

            // check if the last element of the path contains parameters
            // if so convert them to query elements
            if (path.contains(";")) {
                String[] pathElements = path.split("/");
                String last = pathElements[pathElements.length - 1];
                // replace last value by part without params
                int semicolon = last.indexOf(";");
                if (semicolon != -1) {
                    pathElements[pathElements.length - 1] = last.substring(0, semicolon);
                    String params = last.substring(semicolon + 1).replaceAll(";", "&");
                    if (query == null) {
                        query = params;
                    } else {
                        query += "&" + params;
                    }
                    // rebuild the path
                    StringBuilder newPath = new StringBuilder();
                    for (String p : pathElements) {
                        if (StringUtils.isNotBlank(p)) {
                            newPath.append("/").append(p);
                        }
                    }
                    path = newPath.toString();
                }
            }

            if (StringUtils.isEmpty(query)) {
                return urlToFilter;
            }

            List<NameValuePair> pairs = URLEncodedUtils.parse(query, StandardCharsets.UTF_8);
            Iterator<NameValuePair> pairsIterator = pairs.iterator();
            while (pairsIterator.hasNext()) {
                NameValuePair param = pairsIterator.next();
                if (queryElementsToRemove.contains(param.getName())) {
                    pairsIterator.remove();
                } else if (removeHashes && param.getValue() != null) {
                    Matcher m = thirtytwobithash.matcher(param.getValue());
                    if (m.matches()) {
                        pairsIterator.remove();
                    }
                }
            }

            StringBuilder newFile = new StringBuilder();
            if (StringUtils.isNotBlank(path)) {
                newFile.append(path);
            }
            if (!pairs.isEmpty()) {
                Collections.sort(pairs, comp);
                String newQueryString = URLEncodedUtils.format(pairs, StandardCharsets.UTF_8);
                newFile.append('?').append(newQueryString);
            }
            if (url.getRef() != null) {
                newFile.append('#').append(url.getRef());
            }

            return new URL(url.getProtocol(), url.getHost(), url.getPort(), newFile.toString())
                    .toString();
        } catch (MalformedURLException e) {
            LOG.warn("Invalid urlToFilter {}. {}", urlToFilter, e);
            return null;
        }
    }

    Comparator<NameValuePair> comp =
            new Comparator<NameValuePair>() {
                @Override
                public int compare(NameValuePair p1, NameValuePair p2) {
                    return p1.getName().compareTo(p2.getName());
                }
            };

    /**
     * A common error to find is a query string that starts with an & instead of a ? This will fix
     * that error. So http://foo.com&a=b will be changed to http://foo.com?a=b.
     *
     * @param urlToFilter
     * @return corrected url
     */
    private String unmangleQueryString(String urlToFilter) {
        int firstAmp = urlToFilter.indexOf('&');
        if (firstAmp > 0) {
            int firstQuestionMark = urlToFilter.indexOf('?');
            if (firstQuestionMark == -1) {
                return urlToFilter.replaceFirst("&", "?");
            }
        }
        return urlToFilter;
    }

    /**
     * Remove % encoding from path segment in URL for characters which should be unescaped according
     * to <a href="https://tools.ietf.org/html/rfc3986#section-2.2">RFC3986</a> as well as
     * non-standard implementations of percent encoding, see <https://en.
     * wikipedia.org/wiki/Percent-encoding#Non-standard_implementations>.
     */
    private String unescapePath(String path) {
        Matcher matcher = illegalEscapePattern.matcher(path);

        StringBuilder sb = null;
        int end = 0;

        while (matcher.find()) {
            if (sb == null) {
                sb = new StringBuilder();
            }
            // Append everything up to this group
            sb.append(path.substring(end, matcher.start()));
            String group = matcher.group(1);
            int letter = Integer.valueOf(group, 16);
            sb.append((char) letter);
            end = matcher.end();
        }

        // we got a replacement
        if (sb != null) {
            // append whatever is left
            sb.append(path.substring(end));
            path = sb.toString();
            end = 0;
        }

        matcher = unescapeRulePattern.matcher(path);

        if (!matcher.find()) {
            return path;
        }

        sb = new StringBuilder();

        // Traverse over all encoded groups
        do {
            // Append everything up to this group
            sb.append(path.substring(end, matcher.start()));

            // Get the integer representation of this hexadecimal encoded
            // character
            int letter = Integer.valueOf(matcher.group(1), 16);
            if (letter < 128 && unescapedCharacters[letter]) {
                // character should be unescaped in URLs
                sb.append((char) letter);
            } else {
                // Append the whole sequence as uppercase
                sb.append(matcher.group().toUpperCase(Locale.ROOT));
            }

            end = matcher.end();
        } while (matcher.find());

        // Append the rest if there's anything left
        sb.append(path.substring(end));

        return sb.toString();
    }

    /**
     * Convert path segment of URL from Unicode to UTF-8 and escape all characters which should be
     * escaped according to <a href="https://tools.ietf.org/html/rfc3986#section-2.2">RFC3986</a>..
     */
    private String escapePath(String path) {
        StringBuilder sb = new StringBuilder(path.length());

        // Traverse over all bytes in this URL
        for (byte b : path.getBytes(utf8)) {
            // Is this a control character?
            if (b < 33 || b == 91 || b == 92 || b == 93 || b == 124) {
                // Start escape sequence
                sb.append('%');

                // Get this byte's hexadecimal representation
                String hex = Integer.toHexString(b & 0xFF).toUpperCase(Locale.ROOT);

                // Do we need to prepend a zero?
                if (hex.length() % 2 != 0) {
                    sb.append('0');
                    sb.append(hex);
                } else {
                    // No, append this hexadecimal representation
                    sb.append(hex);
                }
            } else {
                // No, just append this character as-is
                sb.append((char) b);
            }
        }

        return sb.toString();
    }

    private boolean isAscii(String str) {
        char[] chars = str.toCharArray();
        for (char c : chars) {
            if (c > 127) {
                return false;
            }
        }
        return true;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.filtering.basic;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.filtering.URLFilter;
import com.fasterxml.jackson.databind.JsonNode;
import java.net.URL;
import java.util.HashMap;
import java.util.Map;

/** Simple URL filters : can be used early in the filtering chain */
public class BasicURLFilter implements URLFilter {

    private int maxPathRepetition = 3;
    private int maxLength = -1;

    public String filter(URL sourceUrl, Metadata sourceMetadata, String urlToFilter) {

        if (urlToFilter == null) {
            return null;
        }

        if (maxLength > 0 && urlToFilter.length() > maxLength) {
            return null;
        }
        if (maxPathRepetition > 1) {
            urlToFilter = filterPathRepet(urlToFilter);
        }
        return urlToFilter;
    }

    public final String filterPathRepet(String urlToFilter) {
        // check whether a path element is repeated N times
        String[] paths = urlToFilter.split("/");
        if (paths.length <= 4) return urlToFilter;

        Map<String, Integer> count = new HashMap<>();
        for (String s : paths) {
            if (s.length() == 0) {
                continue;
            }
            Integer c = count.get(s);
            if (c == null) {
                c = 1;
            } else {
                c = c + 1;
                if (c == maxPathRepetition) {
                    return null;
                }
            }
            count.put(s, c);
        }

        return urlToFilter;
    }

    @Override
    public void configure(Map stormConf, JsonNode filterParams) {
        JsonNode repet = filterParams.get("maxPathRepetition");
        if (repet != null) {
            maxPathRepetition = repet.asInt(3);
        }

        JsonNode length = filterParams.get("maxLength");
        if (length != null) {
            maxLength = length.asInt(-1);
        }
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.protocol;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.proxy.ProxyManager;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import com.digitalpebble.stormcrawler.util.StringTabScheme;
import crawlercommons.robots.BaseRobotRules;
import java.nio.ByteBuffer;
import java.nio.charset.StandardCharsets;
import java.util.HashSet;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.Set;
import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.CommandLineParser;
import org.apache.commons.cli.DefaultParser;
import org.apache.commons.cli.Options;
import org.apache.commons.lang.StringUtils;
import org.apache.storm.Config;
import org.apache.storm.utils.Utils;
import org.slf4j.LoggerFactory;

public abstract class AbstractHttpProtocol implements Protocol {

    private static final org.slf4j.Logger LOG = LoggerFactory.getLogger(AbstractHttpProtocol.class);

    private com.digitalpebble.stormcrawler.protocol.HttpRobotRulesParser robots;

    protected boolean skipRobots = false;

    protected boolean storeHTTPHeaders = false;

    protected boolean useCookies = false;

    protected List<String> protocolVersions;

    protected static final String RESPONSE_COOKIES_HEADER = "set-cookie";

    protected String protocolMDprefix = "";

    public ProxyManager proxyManager;

    protected final List<KeyValue> customHeaders = new LinkedList<>();

    protected static class KeyValue {
        private String k;
        private String v;

        public String getKey() {
            return k;
        }

        public String getValue() {
            return v;
        }

        public KeyValue(String k, String v) {
            super();
            this.k = k;
            this.v = v;
        }

        static KeyValue build(String h) {
            int pos = h.indexOf("=");
            if (pos == -1) return new KeyValue(h.trim(), "");
            if (pos + 1 == h.length()) return new KeyValue(h.trim(), "");
            return new KeyValue(h.substring(0, pos).trim(), h.substring(pos + 1).trim());
        }
    }

    @Override
    public void configure(Config conf) {
        this.skipRobots = ConfUtils.getBoolean(conf, "http.robots.file.skip", false);

        this.storeHTTPHeaders = ConfUtils.getBoolean(conf, "http.store.headers", false);
        this.useCookies = ConfUtils.getBoolean(conf, "http.use.cookies", false);
        this.protocolVersions = ConfUtils.loadListFromConf("http.protocol.versions", conf);

        List<String> headers = ConfUtils.loadListFromConf("http.custom.headers", conf);
        for (String h : headers) {
            customHeaders.add(KeyValue.build(h));
        }

        robots = new HttpRobotRulesParser(conf);
        protocolMDprefix =
                ConfUtils.getString(
                        conf, ProtocolResponse.PROTOCOL_MD_PREFIX_PARAM, protocolMDprefix);

        String proxyManagerImplementation =
                ConfUtils.getString(
                        conf,
                        "http.proxy.manager",
                        // determine whether to set default as
                        // SingleProxyManager by
                        // checking whether legacy proxy field is set
                        (ConfUtils.getString(conf, "http.proxy.host", null) != null)
                                ? "com.digitalpebble.stormcrawler.proxy.SingleProxyManager"
                                : null);

        // conditionally load proxy manager
        if (proxyManagerImplementation != null) {
            // create class to hold the proxy manager class loaded from the
            // config
            Class proxyManagerClass;
            try {
                proxyManagerClass = Class.forName(proxyManagerImplementation);
                boolean interfaceOK = ProxyManager.class.isAssignableFrom(proxyManagerClass);
                if (!interfaceOK) {
                    throw new RuntimeException(
                            "Class "
                                    + proxyManagerImplementation
                                    + " does not implement ProxyManager");
                }
            } catch (ClassNotFoundException e) {
                throw new RuntimeException("Can't load class " + proxyManagerImplementation);
            }

            LOG.info("loaded proxy manager class: {}", proxyManagerClass.getName());

            try {
                // create new proxy manager from file
                proxyManager = (ProxyManager) proxyManagerClass.newInstance();
                proxyManager.configure(conf);
            } catch (RuntimeException | InstantiationException | IllegalAccessException e) {
                LOG.error(
                        "failed to create proxy manager `" + proxyManagerClass.getName() + "`", e);
            }
        }
    }

    @Override
    public BaseRobotRules getRobotRules(String url) {
        if (this.skipRobots) return RobotRulesParser.EMPTY_RULES;
        return robots.getRobotRulesSet(this, url);
    }

    @Override
    public void cleanup() {}

    public static String getAgentString(Config conf) {
        String agent = ConfUtils.getString(conf, "http.agent");
        if (agent != null && !agent.isEmpty()) {
            return agent;
        }
        return getAgentString(
                ConfUtils.getString(conf, "http.agent.name"),
                ConfUtils.getString(conf, "http.agent.version"),
                ConfUtils.getString(conf, "http.agent.description"),
                ConfUtils.getString(conf, "http.agent.url"),
                ConfUtils.getString(conf, "http.agent.email"));
    }

    private static String getAgentString(
            String agentName,
            String agentVersion,
            String agentDesc,
            String agentURL,
            String agentEmail) {

        StringBuilder buf = new StringBuilder();

        buf.append(agentName);

        if (StringUtils.isNotBlank(agentVersion)) {
            buf.append("/");
            buf.append(agentVersion);
        }

        boolean hasAgentDesc = StringUtils.isNotBlank(agentDesc);
        boolean hasAgentURL = StringUtils.isNotBlank(agentURL);
        boolean hasAgentEmail = StringUtils.isNotBlank(agentEmail);

        if (hasAgentDesc || hasAgentEmail || hasAgentURL) {
            buf.append(" (");

            if (hasAgentDesc) {
                buf.append(agentDesc);
                if (hasAgentURL || hasAgentEmail) buf.append("; ");
            }

            if (hasAgentURL) {
                buf.append(agentURL);
                if (hasAgentEmail) buf.append("; ");
            }

            if (hasAgentEmail) {
                buf.append(agentEmail);
            }

            buf.append(")");
        }

        return buf.toString();
    }

    /** Called by extensions of this class * */
    protected static void main(AbstractHttpProtocol protocol, String args[]) throws Exception {
        Config conf = new Config();

        // loads the default configuration file
        Map defaultSCConfig = Utils.findAndReadConfigFile("crawler-default.yaml", false);
        conf.putAll(ConfUtils.extractConfigElement(defaultSCConfig));

        Options options = new Options();
        options.addOption("c", true, "configuration file");

        CommandLineParser parser = new DefaultParser();
        CommandLine cmd = parser.parse(options, args);

        if (cmd.hasOption("c")) {
            String confFile = cmd.getOptionValue("c");
            ConfUtils.loadConf(confFile, conf);
        }

        protocol.configure(conf);

        Set<Runnable> threads = new HashSet<>();

        class Fetchable implements Runnable {
            String url;
            Metadata md;

            Fetchable(String line) {
                StringTabScheme scheme = new StringTabScheme();
                List<Object> tuple =
                        scheme.deserialize(ByteBuffer.wrap(line.getBytes(StandardCharsets.UTF_8)));
                this.url = (String) tuple.get(0);
                this.md = (Metadata) tuple.get(1);
            }

            public void run() {

                StringBuilder stringB = new StringBuilder();
                stringB.append(url).append("\n");

                if (!protocol.skipRobots) {
                    BaseRobotRules rules = protocol.getRobotRules(url);
                    stringB.append("robots allowed: ").append(rules.isAllowed(url)).append("\n");
                    if (rules instanceof RobotRules) {
                        stringB.append("robots requests: ")
                                .append(((RobotRules) rules).getContentLengthFetched().length)
                                .append("\n");
                    }
                    stringB.append("sitemaps identified: ")
                            .append(rules.getSitemaps().size())
                            .append("\n");
                }

                long start = System.currentTimeMillis();
                ProtocolResponse response;
                try {
                    response = protocol.getProtocolOutput(url, md);
                    stringB.append(response.getMetadata()).append("\n");
                    stringB.append("status code: ").append(response.getStatusCode()).append("\n");
                    stringB.append("content length: ")
                            .append(response.getContent().length)
                            .append("\n");
                    long timeFetching = System.currentTimeMillis() - start;
                    stringB.append("fetched in : ").append(timeFetching).append(" msec");
                    System.out.println(stringB);
                } catch (Exception e) {
                    e.printStackTrace();
                } finally {
                    threads.remove(this);
                }
            }
        }

        for (String arg : cmd.getArgs()) {
            Fetchable p = new Fetchable(arg);
            threads.add(p);
            new Thread(p).start();
        }

        while (threads.size() > 0) {
            Thread.sleep(1000);
        }

        protocol.cleanup();
        System.exit(0);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.protocol.selenium;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.protocol.ProtocolResponse;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.node.NullNode;
import java.io.IOException;
import java.io.InputStream;
import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import org.apache.commons.lang.StringUtils;
import org.openqa.selenium.remote.RemoteWebDriver;
import org.slf4j.LoggerFactory;

/** Wrapper for the NavigationFilter defined in a JSON configuration */
public class NavigationFilters extends NavigationFilter {

    public static final NavigationFilters emptyNavigationFilters = new NavigationFilters();

    private static final org.slf4j.Logger LOG = LoggerFactory.getLogger(NavigationFilters.class);

    private NavigationFilter[] filters;

    private NavigationFilters() {
        filters = new NavigationFilter[0];
    }

    public ProtocolResponse filter(RemoteWebDriver driver, Metadata metadata) {
        for (NavigationFilter filter : filters) {
            ProtocolResponse response = filter.filter(driver, metadata);
            if (response != null) return response;
        }
        return null;
    }

    /**
     * Loads and configure the NavigationFilters based on the storm config if there is one otherwise
     * returns an emptyNavigationFilters.
     */
    @SuppressWarnings("rawtypes")
    public static NavigationFilters fromConf(Map stormConf) {
        String configfile = ConfUtils.getString(stormConf, "navigationfilters.config.file");
        if (StringUtils.isNotBlank(configfile)) {
            try {
                return new NavigationFilters(stormConf, configfile);
            } catch (IOException e) {
                String message =
                        "Exception caught while loading the NavigationFilters from " + configfile;
                LOG.error(message);
                throw new RuntimeException(message, e);
            }
        }

        return NavigationFilters.emptyNavigationFilters;
    }

    /**
     * loads the filters from a JSON configuration file
     *
     * @throws IOException
     */
    @SuppressWarnings("rawtypes")
    public NavigationFilters(Map stormConf, String configFile) throws IOException {
        // load the JSON configFile
        // build a JSON object out of it
        JsonNode confNode = null;
        InputStream confStream = null;
        try {
            confStream = getClass().getClassLoader().getResourceAsStream(configFile);

            ObjectMapper mapper = new ObjectMapper();
            confNode = mapper.readValue(confStream, JsonNode.class);
        } catch (Exception e) {
            throw new IOException("Unable to build JSON object from file", e);
        } finally {
            if (confStream != null) {
                confStream.close();
            }
        }

        configure(stormConf, confNode);
    }

    @SuppressWarnings("rawtypes")
    @Override
    public void configure(Map stormConf, JsonNode filtersConf) {
        // initialises the filters
        List<NavigationFilter> filterLists = new ArrayList<>();

        // get the filters part
        String name = getClass().getCanonicalName();
        filtersConf = filtersConf.get(name);

        if (filtersConf == null) {
            LOG.info("No field {} in JSON config. Skipping", name);
            filters = new NavigationFilter[0];
            return;
        }

        // conf node contains a list of objects
        Iterator<JsonNode> filterIter = filtersConf.elements();
        while (filterIter.hasNext()) {
            JsonNode afilterConf = filterIter.next();
            String filterName = "<unnamed>";
            JsonNode nameNode = afilterConf.get("name");
            if (nameNode != null) {
                filterName = nameNode.textValue();
            }
            JsonNode classNode = afilterConf.get("class");
            if (classNode == null) {
                LOG.error("Filter {} doesn't specified a 'class' attribute", filterName);
                continue;
            }
            String className = classNode.textValue().trim();
            filterName += '[' + className + ']';
            // check that it is available and implements the interface
            // NavigationFilter
            try {
                Class<?> filterClass = Class.forName(className);
                boolean subClassOK = NavigationFilter.class.isAssignableFrom(filterClass);
                if (!subClassOK) {
                    LOG.error("Filter {} does not extend NavigationFilter", filterName);
                    continue;
                }
                NavigationFilter filterInstance = (NavigationFilter) filterClass.newInstance();

                JsonNode paramNode = afilterConf.get("params");
                if (paramNode != null) {
                    filterInstance.configure(stormConf, paramNode);
                } else {
                    // Pass in a nullNode if missing
                    filterInstance.configure(stormConf, NullNode.getInstance());
                }

                filterLists.add(filterInstance);
                LOG.info("Setup {}", filterName);
            } catch (Exception e) {
                LOG.error("Can't setup {}: {}", filterName, e);
                throw new RuntimeException("Can't setup " + filterName, e);
            }
        }

        filters = filterLists.toArray(new NavigationFilter[filterLists.size()]);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.protocol.selenium;

import com.digitalpebble.stormcrawler.util.ConfUtils;
import java.net.URL;
import java.time.Duration;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import org.apache.storm.Config;
import org.openqa.selenium.WebDriver.Timeouts;
import org.openqa.selenium.remote.DesiredCapabilities;
import org.openqa.selenium.remote.RemoteWebDriver;

/**
 * Delegates the requests to one or more remote selenium servers. The processes must be started /
 * stopped separately. The URLs to connect to are specified with the config 'selenium.addresses'.
 */
public class RemoteDriverProtocol extends SeleniumProtocol {

    @Override
    public void configure(Config conf) {
        super.configure(conf);

        // see https://github.com/SeleniumHQ/selenium/wiki/DesiredCapabilities
        DesiredCapabilities capabilities = new DesiredCapabilities();
        capabilities.setJavascriptEnabled(true);

        String userAgentString = getAgentString(conf);

        // custom capabilities
        Map<String, Object> confCapabilities =
                (Map<String, Object>) conf.get("selenium.capabilities");
        if (confCapabilities != null) {
            Iterator<Entry<String, Object>> iter = confCapabilities.entrySet().iterator();
            while (iter.hasNext()) {
                Entry<String, Object> entry = iter.next();
                Object val = entry.getValue();
                // substitute variable $useragent for the real value
                if (val instanceof String && "$useragent".equalsIgnoreCase(val.toString())) {
                    val = userAgentString;
                }
                capabilities.setCapability(entry.getKey(), val);
            }
        }

        // load adresses from config
        List<String> addresses = ConfUtils.loadListFromConf("selenium.addresses", conf);
        if (addresses.size() == 0) {
            throw new RuntimeException("No value found for selenium.addresses");
        }
        try {
            for (String cdaddress : addresses) {
                RemoteWebDriver driver = new RemoteWebDriver(new URL(cdaddress), capabilities);
                Timeouts touts = driver.manage().timeouts();
                int implicitWait = ConfUtils.getInt(conf, "selenium.implicitlyWait", 0);
                int pageLoadTimeout = ConfUtils.getInt(conf, "selenium.pageLoadTimeout", 0);
                int scriptTimeout = ConfUtils.getInt(conf, "selenium.scriptTimeout", 0);
                touts.implicitlyWait(Duration.ofMillis(implicitWait));
                touts.pageLoadTimeout(Duration.ofMillis(pageLoadTimeout));
                touts.scriptTimeout(Duration.ofMillis(scriptTimeout));
                drivers.add(driver);
            }
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }

    public static void main(String[] args) throws Exception {
        RemoteDriverProtocol.main(new RemoteDriverProtocol(), args);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.protocol.selenium;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.protocol.AbstractHttpProtocol;
import com.digitalpebble.stormcrawler.protocol.HttpHeaders;
import com.digitalpebble.stormcrawler.protocol.ProtocolResponse;
import java.util.concurrent.LinkedBlockingQueue;
import org.apache.storm.Config;
import org.openqa.selenium.remote.RemoteWebDriver;
import org.slf4j.LoggerFactory;

public abstract class SeleniumProtocol extends AbstractHttpProtocol {

    protected static final org.slf4j.Logger LOG = LoggerFactory.getLogger(SeleniumProtocol.class);

    protected LinkedBlockingQueue<RemoteWebDriver> drivers;

    private NavigationFilters filters;

    @Override
    public void configure(Config conf) {
        super.configure(conf);
        drivers = new LinkedBlockingQueue<>();
        filters = NavigationFilters.fromConf(conf);
    }

    public ProtocolResponse getProtocolOutput(String url, Metadata metadata) throws Exception {
        RemoteWebDriver driver = null;
        while ((driver = getDriver()) == null) {}
        try {
            // This will block for the page load and any
            // associated AJAX requests
            driver.get(url);

            String u = driver.getCurrentUrl();

            // call the filters
            ProtocolResponse response = filters.filter(driver, metadata);
            if (response != null) {
                return response;
            }

            // if the URL is different then we must have hit a redirection
            if (!u.equalsIgnoreCase(url)) {
                byte[] content = new byte[] {};
                Metadata m = new Metadata();
                m.addValue(HttpHeaders.LOCATION, u);
                return new ProtocolResponse(content, 307, m);
            }

            // if no filters got triggered
            byte[] content = driver.getPageSource().getBytes();
            return new ProtocolResponse(content, 200, new Metadata());

        } finally {
            // finished with this driver - return it to the queue
            drivers.put(driver);
        }
    }

    /** Returns the first available driver * */
    private final RemoteWebDriver getDriver() {
        try {
            return drivers.take();
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        }
        return null;
    }

    @Override
    public void cleanup() {
        LOG.info("Cleanup called on Selenium protocol drivers");
        synchronized (drivers) {
            drivers.forEach(
                    (d) -> {
                        d.close();
                    });
        }
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.protocol.selenium;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.protocol.Protocol;
import com.digitalpebble.stormcrawler.protocol.ProtocolResponse;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import crawlercommons.robots.BaseRobotRules;
import org.apache.storm.Config;

/**
 * Alternative implementation of RemoteDriverProtocol which delegates the calls to a different
 * implementation if the URL does not have a value for the protocol.use.selenium in its metadata.
 * Allows to use Selenium for some of the URLs only.
 *
 * @deprecated use DelegatorProtocol instead
 */
public class DelegatorRemoteDriverProtocol extends RemoteDriverProtocol {

    private Protocol directProtocol;

    // metadata key value to indicate that we should use Selenium
    // rely on the direct protocol instead
    public static final String USE_SELENIUM_KEY = "protocol.use.selenium";

    public static final String PROTOCOL_IMPL_CONFIG = "selenium.delegated.protocol";

    @Override
    public void configure(Config conf) {
        super.configure(conf);
        String protocolimplementation = ConfUtils.getString(conf, PROTOCOL_IMPL_CONFIG);
        try {
            Class protocolClass = Class.forName(protocolimplementation);
            boolean interfaceOK = Protocol.class.isAssignableFrom(protocolClass);
            if (!interfaceOK) {
                throw new RuntimeException(
                        "Class " + protocolimplementation + " does not implement Protocol");
            }
            directProtocol = (Protocol) protocolClass.newInstance();
            directProtocol.configure(conf);
        } catch (Exception e) {
            throw new RuntimeException(
                    "DelegatorRemoteDriverProtocol needs a valid protocol class for the config "
                            + PROTOCOL_IMPL_CONFIG
                            + "but has :"
                            + protocolimplementation);
        }
    }

    @Override
    public ProtocolResponse getProtocolOutput(String url, Metadata metadata) throws Exception {
        if (metadata.getFirstValue(USE_SELENIUM_KEY) != null) {
            return super.getProtocolOutput(url, metadata);
        } else {
            return directProtocol.getProtocolOutput(url, metadata);
        }
    }

    @Override
    public void cleanup() {
        super.cleanup();
        directProtocol.cleanup();
    }

    @Override
    public BaseRobotRules getRobotRules(String url) {
        return directProtocol.getRobotRules(url);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.protocol.selenium;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.protocol.ProtocolResponse;
import com.fasterxml.jackson.databind.JsonNode;
import java.util.Map;
import org.openqa.selenium.remote.RemoteWebDriver;

public abstract class NavigationFilter {
    /**
     * Called when this filter is being initialised
     *
     * @param stormConf The Storm configuration used for the parsing bolt
     * @param filterParams the filter specific configuration. Never null
     */
    public void configure(@SuppressWarnings("rawtypes") Map stormConf, JsonNode filterParams) {}

    /** The end result comes from the first filter to return non-null * */
    public abstract ProtocolResponse filter(RemoteWebDriver driver, Metadata metadata);
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.protocol;

import crawlercommons.robots.BaseRobotRules;
import java.util.List;

/**
 * Wrapper for BaseRobotRules which tracks the number of requests and length of the responses needed
 * to get the rules. If the array returned by getContentLengthFetched() is empty, then the rules
 * were obtained from the cache.
 */
public class RobotRules extends crawlercommons.robots.BaseRobotRules {

    private BaseRobotRules base;
    private int[] bytesFetched = new int[] {};

    public RobotRules(BaseRobotRules base) {
        this.base = base;
    }

    @Override
    public boolean isAllowed(String url) {
        return base.isAllowed(url);
    }

    @Override
    public boolean isAllowAll() {
        return base.isAllowAll();
    }

    @Override
    public boolean isAllowNone() {
        return base.isAllowNone();
    }

    /** Returns the number of bytes fetched per request when not cached * */
    public int[] getContentLengthFetched() {
        return bytesFetched;
    }

    /** Returns the number of bytes fetched per request when not cached * */
    public void setContentLengthFetched(int[] bytesFetched) {
        this.bytesFetched = bytesFetched;
    }

    @Override
    public long getCrawlDelay() {
        return base.getCrawlDelay();
    }

    @Override
    public boolean isDeferVisits() {
        return base.isDeferVisits();
    }

    @Override
    public List<String> getSitemaps() {
        return base.getSitemaps();
    }

    @Override
    public int hashCode() {
        return base.hashCode();
    }

    @Override
    public boolean equals(Object obj) {
        return base.equals(obj);
    }

    @Override
    public void setCrawlDelay(long crawlDelay) {
        base.setCrawlDelay(crawlDelay);
    }

    @Override
    public void setDeferVisits(boolean deferVisits) {
        base.setDeferVisits(deferVisits);
    }

    @Override
    public void addSitemap(String sitemap) {
        base.addSitemap(sitemap);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.protocol;

import com.digitalpebble.stormcrawler.Metadata;
import crawlercommons.robots.BaseRobotRules;
import java.util.Collection;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import org.apache.storm.Config;
import org.slf4j.LoggerFactory;

/**
 * Protocol implementation that enables selection from a collection of sub-protocols using filters
 * based on each call's metadata
 *
 * <p>Is configured like this
 *
 * <pre>
 * protocol.delegator.config:
 * - className: "com.digitalpebble.stormcrawler.protocol.httpclient.HttpProtocol"
 * filters:
 * domain: "example.com"
 * depth: "3"
 * test: "true"
 * - className: "com.digitalpebble.stormcrawler.protocol.okhttp.HttpProtocol"
 * filters:
 * js: "true"
 * - className: "com.digitalpebble.stormcrawler.protocol.okhttp.HttpProtocol"
 * </pre>
 *
 * The last one in the list must not have filters as it is used as a default value. The protocols
 * are tried for matches in the order in which they are listed in the configuration. The first to
 * match gets used to fetch a URL.
 *
 * @since 2.2
 */
public class DelegatorProtocol implements Protocol {

    private static final String DELEGATOR_CONFIG_KEY = "protocol.delegator.config";

    protected static final org.slf4j.Logger LOG = LoggerFactory.getLogger(DelegatorProtocol.class);

    static class Filter {

        String key;
        String value;

        public Filter(String k, String v) {
            key = k;
            value = v;
        }
    }

    static class FilteredProtocol {

        final Protocol protoInstance;
        final List<Filter> filters = new LinkedList<>();

        Protocol getProtocolInstance() {
            return protoInstance;
        }

        public FilteredProtocol(String protocolimplementation, Object f, Config config) {
            // load the protocol
            Class protocolClass;
            try {
                protocolClass = Class.forName(protocolimplementation);
                boolean interfaceOK = Protocol.class.isAssignableFrom(protocolClass);
                if (!interfaceOK) {
                    throw new RuntimeException(
                            "Class " + protocolimplementation + " does not implement Protocol");
                }
                this.protoInstance = (Protocol) protocolClass.newInstance();
                this.protoInstance.configure(config);
            } catch (ClassNotFoundException e) {
                throw new RuntimeException("Can't load class " + protocolimplementation);
            } catch (InstantiationException e) {
                throw new RuntimeException("Can't instanciate class " + protocolimplementation);
            } catch (IllegalAccessException e) {
                throw new RuntimeException(
                        "IllegalAccessException for class " + protocolimplementation);
            }

            // instantiate filters
            if (f != null) {
                if (f instanceof Map) {
                    ((Map<String, String>) f)
                            .forEach(
                                    (k, v) -> {
                                        filters.add(new Filter(k, v));
                                    });
                } else {
                    throw new RuntimeException("Can't instanciate filter " + f);
                }
            }

            // log filters found
            LOG.info("Loaded {} filters for {}", filters.size(), protocolimplementation);
        }

        public ProtocolResponse getProtocolOutput(String url, Metadata metadata) throws Exception {
            return protoInstance.getProtocolOutput(url, metadata);
        }

        public BaseRobotRules getRobotRules(String url) {
            return protoInstance.getRobotRules(url);
        }

        public void cleanup() {
            protoInstance.cleanup();
        }

        boolean isMatch(final Metadata metadata) {
            // if this FP has no filters - it can handle anything
            if (filters.isEmpty()) return true;

            // check that all its filters are satisfied
            for (Filter f : filters) {
                if (f.value == null || f.value.equals("")) {
                    // just interested in the fact that the key exists
                    if (!metadata.containsKey(f.key)) {
                        LOG.trace("Key {} not found in metadata {}", f.key, metadata);
                        return false;
                    }
                } else {
                    // interested in the value associated with the key
                    if (!metadata.containsKeyWithValue(f.key, f.value)) {
                        LOG.trace(
                                "Key {} not found with value {} in metadata {}",
                                f.key,
                                f.value,
                                metadata);
                        return false;
                    }
                }
            }

            return true;
        }
    }

    private final LinkedList<FilteredProtocol> protocols = new LinkedList<>();

    @Override
    public void configure(Config conf) {
        Object obj = conf.get(DELEGATOR_CONFIG_KEY);

        if (obj == null)
            throw new RuntimeException("DelegatorProtocol declared but no config set for it");

        // should contain a list of maps
        // each map having a className and optionally a number of filters
        if (obj instanceof Collection) {
            for (Map subConf : (Collection<Map>) obj) {
                String className = (String) subConf.get("className");
                Object filters = subConf.get("filters");
                protocols.add(new FilteredProtocol(className, filters, conf));
            }
        } else { // single value?
            throw new RuntimeException(
                    "DelegatorProtocol declared but single object found in config " + obj);
        }

        // check that the last protocol has no filter
        if (!protocols.peekLast().filters.isEmpty()) {
            throw new RuntimeException(
                    "The last sub protocol has filters but must not as it acts as the default");
        }
    }

    final FilteredProtocol getProtocolFor(String url, Metadata metadata) {

        for (FilteredProtocol p : protocols) {
            if (p.isMatch(metadata)) {
                return p;
            }
        }

        return null;
    }

    @Override
    public BaseRobotRules getRobotRules(String url) {

        FilteredProtocol proto = getProtocolFor(url, Metadata.empty);
        if (proto == null) {
            throw new RuntimeException("No sub protocol eligible to retrieve robots");
        }
        return proto.getRobotRules(url);
    }

    @Override
    public ProtocolResponse getProtocolOutput(String url, Metadata metadata) throws Exception {

        // go through the filtered protocols to find which one to use
        FilteredProtocol proto = getProtocolFor(url, metadata);
        if (proto == null) {
            throw new RuntimeException(
                    "No sub protocol eligible to retrieve " + url + "given " + metadata.toString());
        }
        // execute and return protocol with url-meta combo
        return proto.getProtocolOutput(url, metadata);
    }

    @Override
    public void cleanup() {
        for (FilteredProtocol p : protocols) p.cleanup();
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.protocol;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import com.github.benmanes.caffeine.cache.Cache;
import com.google.common.primitives.Ints;
import crawlercommons.robots.BaseRobotRules;
import java.net.URL;
import java.util.LinkedList;
import java.util.List;
import java.util.Locale;
import org.apache.commons.lang.StringUtils;
import org.apache.storm.Config;

/**
 * This class is used for parsing robots for urls belonging to HTTP protocol. It extends the generic
 * {@link RobotRulesParser} class and contains Http protocol specific implementation for obtaining
 * the robots file.
 */
public class HttpRobotRulesParser extends RobotRulesParser {

    protected boolean allowForbidden = false;

    protected Metadata fetchRobotsMd;

    HttpRobotRulesParser() {}

    public HttpRobotRulesParser(Config conf) {
        setConf(conf);
    }

    @Override
    public void setConf(Config conf) {
        super.setConf(conf);
        allowForbidden = ConfUtils.getBoolean(conf, "http.robots.403.allow", true);
        fetchRobotsMd = new Metadata();
        /* http.content.limit for fetching the robots.txt */
        int robotsTxtContentLimit = ConfUtils.getInt(conf, "http.robots.content.limit", -1);
        fetchRobotsMd.addValue("http.content.limit", Integer.toString(robotsTxtContentLimit));
    }

    /** Compose unique key to store and access robot rules in cache for given URL */
    protected static String getCacheKey(URL url) {
        String protocol = url.getProtocol().toLowerCase(Locale.ROOT);
        String host = url.getHost().toLowerCase(Locale.ROOT);

        int port = url.getPort();
        if (port == -1) {
            port = url.getDefaultPort();
        }
        /*
         * Robot rules apply only to host, protocol, and port where robots.txt
         * is hosted (cf. NUTCH-1752). Consequently
         */
        return protocol + ":" + host + ":" + port;
    }

    /**
     * Returns the robots rules from the cache or empty rules if not found
     *
     * @see com.digitalpebble.stormcrawler.filtering.robots.RobotsFilter
     */
    public BaseRobotRules getRobotRulesSetFromCache(URL url) {
        String cacheKey = getCacheKey(url);
        BaseRobotRules robotRules = CACHE.getIfPresent(cacheKey);
        if (robotRules != null) {
            return robotRules;
        }
        return EMPTY_RULES;
    }

    /**
     * Get the rules from robots.txt which applies for the given {@code url}. Robot rules are cached
     * for a unique combination of host, protocol, and port. If no rules are found in the cache, a
     * HTTP request is send to fetch {{protocol://host:port/robots.txt}}. The robots.txt is then
     * parsed and the rules are cached to avoid re-fetching and re-parsing it again.
     *
     * @param http The {@link Protocol} object
     * @param url URL robots.txt applies to
     * @return {@link BaseRobotRules} holding the rules from robots.txt
     */
    @Override
    public BaseRobotRules getRobotRulesSet(Protocol http, URL url) {

        String cacheKey = getCacheKey(url);

        // check in the error cache first
        BaseRobotRules robotRules = ERRORCACHE.getIfPresent(cacheKey);
        if (robotRules != null) {
            return robotRules;
        }

        // now try the proper cache
        robotRules = CACHE.getIfPresent(cacheKey);
        if (robotRules != null) {
            return robotRules;
        }

        boolean cacheRule = true;
        URL redir = null;

        String keyredir = null;

        LOG.debug("Cache miss {} for {}", cacheKey, url);
        List<Integer> bytesFetched = new LinkedList<>();
        try {
            ProtocolResponse response =
                    http.getProtocolOutput(new URL(url, "/robots.txt").toString(), fetchRobotsMd);
            int code = response.getStatusCode();
            bytesFetched.add(response.getContent() != null ? response.getContent().length : 0);
            // try one level of redirection ?
            if (code == 301 || code == 302 || code == 307 || code == 308) {
                String redirection = response.getMetadata().getFirstValue(HttpHeaders.LOCATION);
                if (StringUtils.isNotBlank(redirection)) {
                    if (!redirection.startsWith("http")) {
                        // RFC says it should be absolute, but apparently it
                        // isn't
                        redir = new URL(url, redirection);
                    } else {
                        redir = new URL(redirection);
                    }
                    if (redir.getPath().equals("/robots.txt")) {
                        // only if the path of the redirect target is
                        // `/robots.txt` we can get the rules from the cache
                        // under the host key of the redirect target
                        keyredir = getCacheKey(redir);
                        RobotRules cachedRediRobotRules = CACHE.getIfPresent(keyredir);
                        if (cachedRediRobotRules != null) {
                            // cache also for the source host
                            LOG.debug(
                                    "Found robots for {} (redirected) under key {} in cache",
                                    redir,
                                    keyredir);
                            LOG.debug(
                                    "Caching redirected robots from key {} under key {}",
                                    keyredir,
                                    cacheKey);
                            CACHE.put(cacheKey, cachedRediRobotRules);
                            return cachedRediRobotRules;
                        }
                    } else {
                        LOG.debug(
                                "Robots for {} redirected to {} (not cached for target host because not at root)",
                                url,
                                redir);
                    }

                    response = http.getProtocolOutput(redir.toString(), Metadata.empty);
                    code = response.getStatusCode();
                    bytesFetched.add(
                            response.getContent() != null ? response.getContent().length : 0);
                }
            }
            if (code == 200) // found rules: parse them
            {
                String ct = response.getMetadata().getFirstValue(HttpHeaders.CONTENT_TYPE);
                robotRules = parseRules(url.toString(), response.getContent(), ct, agentNames);
            } else if ((code == 403) && (!allowForbidden)) {
                robotRules = FORBID_ALL_RULES; // use forbid all
            } else if (code >= 500) {
                cacheRule = false;
                robotRules = EMPTY_RULES;
            } else robotRules = EMPTY_RULES; // use default rules
        } catch (Throwable t) {
            LOG.info("Couldn't get robots.txt for {} : {}", url, t.toString());
            cacheRule = false;
            robotRules = EMPTY_RULES;
        }

        Cache<String, RobotRules> cacheToUse = CACHE;
        String cacheName = "success";
        if (!cacheRule) {
            cacheToUse = ERRORCACHE;
            cacheName = "error";
        }

        RobotRules cached = new RobotRules(robotRules);

        LOG.debug("Caching robots for {} under key {} in cache {}", url, cacheKey, cacheName);
        cacheToUse.put(cacheKey, cached);

        // cache robot rules for redirections
        // get here only if the target has not been found in the cache
        if (keyredir != null) {
            // keyredir isn't null only if the robots.txt file of the target is
            // at the root
            LOG.debug("Caching robots for {} under key {} in cache {}", redir, keyredir, cacheName);
            cacheToUse.put(keyredir, cached);
        }

        RobotRules live = new RobotRules(robotRules);
        live.setContentLengthFetched(Ints.toArray(bytesFetched));
        return live;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.protocol;

import com.digitalpebble.stormcrawler.util.ConfUtils;
import com.github.benmanes.caffeine.cache.Cache;
import com.github.benmanes.caffeine.cache.Caffeine;
import crawlercommons.robots.BaseRobotRules;
import crawlercommons.robots.SimpleRobotRules;
import crawlercommons.robots.SimpleRobotRules.RobotRulesMode;
import crawlercommons.robots.SimpleRobotRulesParser;
import java.net.URL;
import java.util.ArrayList;
import java.util.StringTokenizer;
import javax.security.auth.login.Configuration;
import org.apache.storm.Config;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * This class uses crawler-commons for handling the parsing of {@code robots.txt} files. It emits
 * SimpleRobotRules objects, which describe the download permissions as described in
 * SimpleRobotRulesParser.
 */
public abstract class RobotRulesParser {

    public static final Logger LOG = LoggerFactory.getLogger(RobotRulesParser.class);

    protected static Cache<String, RobotRules> CACHE;

    // if a server or client error happened while fetching the robots
    // cache the result for a shorter period before trying again
    protected static Cache<String, RobotRules> ERRORCACHE;

    /**
     * Parameter name to configure the cache for robots @see http://docs.guava-libraries.googlecode
     * .com/git/javadoc/com/google/common/cache/CacheBuilderSpec.html Default value is
     * "maximumSize=10000,expireAfterWrite=6h"
     */
    public static final String cacheConfigParamName = "robots.cache.spec";

    /**
     * Parameter name to configure the cache for robots errors @see
     * http://docs.guava-libraries.googlecode
     * .com/git/javadoc/com/google/common/cache/CacheBuilderSpec.html Default value is
     * "maximumSize=10000,expireAfterWrite=1h"
     */
    public static final String errorcacheConfigParamName = "robots.error.cache.spec";

    /**
     * A {@link BaseRobotRules} object appropriate for use when the {@code robots.txt} file is empty
     * or missing; all requests are allowed.
     */
    public static final BaseRobotRules EMPTY_RULES = new SimpleRobotRules(RobotRulesMode.ALLOW_ALL);

    /**
     * A {@link BaseRobotRules} object appropriate for use when the {@code robots.txt} file is not
     * fetched due to a {@code 403/Forbidden} response; all requests are disallowed.
     */
    public static final BaseRobotRules FORBID_ALL_RULES =
            new SimpleRobotRules(RobotRulesMode.ALLOW_NONE);

    private static SimpleRobotRulesParser robotParser = new SimpleRobotRulesParser();

    static {
        robotParser.setMaxCrawlDelay(Long.MAX_VALUE);
    }

    protected String agentNames;

    public RobotRulesParser() {}

    /** Set the {@link Configuration} object */
    public void setConf(Config conf) {

        // Grab the agent names we advertise to robots files.
        String agentName = ConfUtils.getString(conf, "http.agent.name");
        if (null == agentName) {
            throw new RuntimeException("Agent name not configured!");
        }

        String configuredAgentNames = ConfUtils.getString(conf, "http.robots.agents", "");
        StringTokenizer tok = new StringTokenizer(configuredAgentNames, ",");
        ArrayList<String> agents = new ArrayList<>();
        while (tok.hasMoreTokens()) {
            agents.add(tok.nextToken().trim());
        }

        /**
         * If there are no agents for robots-parsing, use the default agent-string. If both are
         * present, our agent-string should be the first one we advertise to robots-parsing.
         */
        if (agents.isEmpty()) {
            LOG.info(
                    "No agents listed in 'http.robots.agents' property! Using http.agent.name [{}]",
                    agentName);
            this.agentNames = agentName;
        } else {
            int index = 0;
            if ((agents.get(0)).equalsIgnoreCase(agentName)) {
                index++;
            } else {
                LOG.info(
                        "Agent we advertise ({}) not listed first in 'http.robots.agents' property!",
                        agentName);
            }

            StringBuilder combinedAgentsString = new StringBuilder(agentName);
            // append all the agents from the http.robots.agents property
            for (; index < agents.size(); index++) {
                combinedAgentsString.append(", ").append(agents.get(index));
            }

            this.agentNames = combinedAgentsString.toString();
        }

        String spec =
                ConfUtils.getString(
                        conf, cacheConfigParamName, "maximumSize=10000,expireAfterWrite=6h");
        CACHE = Caffeine.from(spec).build();

        spec =
                ConfUtils.getString(
                        conf, errorcacheConfigParamName, "maximumSize=10000,expireAfterWrite=1h");
        ERRORCACHE = Caffeine.from(spec).build();
    }

    /**
     * Parses the robots content using the {@link SimpleRobotRulesParser} from crawler commons
     *
     * @param url A string containing url
     * @param content Contents of the robots file in a byte array
     * @param contentType The
     * @param robotName A string containing value of
     * @return BaseRobotRules object
     */
    public BaseRobotRules parseRules(
            String url, byte[] content, String contentType, String robotName) {
        return robotParser.parseContent(url, content, contentType, robotName);
    }

    public BaseRobotRules getRobotRulesSet(Protocol protocol, String url) {
        URL u;
        try {
            u = new URL(url);
        } catch (Exception e) {
            return EMPTY_RULES;
        }
        return getRobotRulesSet(protocol, u);
    }

    public abstract BaseRobotRules getRobotRulesSet(Protocol protocol, URL url);
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.protocol;

import java.time.ZoneId;
import java.time.ZoneOffset;
import java.time.ZonedDateTime;
import java.time.format.DateTimeFormatter;
import java.time.format.DateTimeParseException;
import java.util.Locale;

/**
 * A collection of HTTP header names and utilities around header values.
 *
 * @see <a href="http://rfc-ref.org/RFC-TEXTS/2616/">Hypertext Transfer Protocol -- HTTP/1.1 (RFC
 *     2616)</a>
 */
public interface HttpHeaders {

    public static final String TRANSFER_ENCODING = "transfer-encoding";

    public static final String CONTENT_ENCODING = "content-encoding";

    public static final String CONTENT_LANGUAGE = "content-language";

    public static final String CONTENT_LENGTH = "content-length";

    public static final String CONTENT_LOCATION = "content-location";

    public static final String CONTENT_DISPOSITION = "content-disposition";

    public static final String CONTENT_MD5 = "content-md5";

    public static final String CONTENT_TYPE = "content-type";

    public static final String LAST_MODIFIED = "last-modified";

    public static final String LOCATION = "location";

    /**
     * Formatter for dates in HTTP headers, used to fill the &quot;If-Modified-Since&quot; request
     * header field, e.g.
     *
     * <pre>
     * Sun, 06 Nov 1994 08:49:37 GMT
     * </pre>
     *
     * See <a href= "https://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.3.1">sec. 3.3.1 in
     * RFC 2616</a> and <a href="https://tools.ietf.org/html/rfc7231#section-7.1.1.1">sec. 7.1.1.1
     * in RFC 7231</a>. The latter specifies the format defined in RFC 1123 as the
     * &quot;preferred&quot; format.
     */
    public static final DateTimeFormatter HTTP_DATE_FORMATTER =
            DateTimeFormatter.ofPattern("EEE, dd MMM yyyy HH:mm:ss 'GMT'", Locale.ROOT)
                    .withZone(ZoneId.of(ZoneOffset.UTC.toString()));

    /** Formatter to parse ISO-formatted dates persisted in status index */
    public static final DateTimeFormatter ISO_INSTANT_FORMATTER =
            DateTimeFormatter.ISO_INSTANT.withZone(ZoneId.of(ZoneOffset.UTC.toString()));

    /**
     * Format an ISO date string as HTTP date used in HTTP headers, e.g.,
     *
     * <pre>
     * 1994-11-06T08:49:37.000Z
     * </pre>
     *
     * is formatted to
     *
     * <pre>
     * Sun, 06 Nov 1994 08:49:37 GMT
     * </pre>
     *
     * See {@link #HTTP_DATE_FORMATTER.}
     */
    public static String formatHttpDate(String isoDate) {
        try {
            ZonedDateTime date = ISO_INSTANT_FORMATTER.parse(isoDate, ZonedDateTime::from);
            return HTTP_DATE_FORMATTER.format(date);
        } catch (DateTimeParseException e) {
            // not an ISO date
            return "";
        }
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.protocol;

import com.digitalpebble.stormcrawler.Metadata;
import crawlercommons.robots.BaseRobotRules;
import org.apache.storm.Config;

public interface Protocol {

    public void configure(Config conf);

    /**
     * Fetches the content and additional metadata
     *
     * <p>IMPORTANT: the metadata returned within the response should only be new <i>additional</i>,
     * no need to return the metadata passed in.
     *
     * @param url the location of the content
     * @param metadata extra information
     * @return the content and optional metadata fetched via this protocol
     * @throws Exception
     */
    public ProtocolResponse getProtocolOutput(String url, Metadata metadata) throws Exception;

    public BaseRobotRules getRobotRules(String url);

    public void cleanup();
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.protocol.okhttp;

import java.net.InetAddress;
import java.util.List;
import java.util.Map;
import okhttp3.Call;
import okhttp3.EventListener;
import org.slf4j.LoggerFactory;

public class DNSResolutionListener extends EventListener {

    private static final org.slf4j.Logger LOG =
            LoggerFactory.getLogger(DNSResolutionListener.class);

    private long dnsStartMillis;

    final Map<String, Long> times;

    public DNSResolutionListener(final Map<String, Long> times) {
        this.times = times;
    }

    @Override
    public void dnsEnd(Call call, String domainName, List<InetAddress> inetAddressList) {
        long timeSpent = System.currentTimeMillis() - dnsStartMillis;
        LOG.debug("DNS resolution for {} took {} millisecs", domainName, timeSpent);
        times.put(call.toString(), timeSpent);
    }

    @Override
    public void dnsStart(Call call, String domainName) {
        dnsStartMillis = System.currentTimeMillis();
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.protocol.okhttp;

import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.protocol.AbstractHttpProtocol;
import com.digitalpebble.stormcrawler.protocol.HttpHeaders;
import com.digitalpebble.stormcrawler.protocol.ProtocolResponse;
import com.digitalpebble.stormcrawler.protocol.ProtocolResponse.TrimmedContentReason;
import com.digitalpebble.stormcrawler.proxy.SCProxy;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import com.digitalpebble.stormcrawler.util.CookieConverter;
import java.io.IOException;
import java.net.InetSocketAddress;
import java.net.MalformedURLException;
import java.net.Proxy;
import java.net.URL;
import java.security.cert.CertificateException;
import java.util.ArrayList;
import java.util.Base64;
import java.util.HashMap;
import java.util.LinkedList;
import java.util.List;
import java.util.Locale;
import java.util.Map;
import java.util.concurrent.TimeUnit;
import javax.net.ssl.HostnameVerifier;
import javax.net.ssl.SSLContext;
import javax.net.ssl.SSLSession;
import javax.net.ssl.SSLSocketFactory;
import javax.net.ssl.TrustManager;
import javax.net.ssl.X509TrustManager;
import okhttp3.Call;
import okhttp3.Connection;
import okhttp3.ConnectionPool;
import okhttp3.Credentials;
import okhttp3.EventListener;
import okhttp3.EventListener.Factory;
import okhttp3.Headers;
import okhttp3.Interceptor;
import okhttp3.MediaType;
import okhttp3.OkHttpClient;
import okhttp3.Protocol;
import okhttp3.Request;
import okhttp3.Request.Builder;
import okhttp3.RequestBody;
import okhttp3.Response;
import okhttp3.ResponseBody;
import okhttp3.Route;
import okhttp3.brotli.BrotliInterceptor;
import okio.BufferedSource;
import org.apache.commons.lang.StringUtils;
import org.apache.commons.lang.mutable.MutableObject;
import org.apache.http.cookie.Cookie;
import org.apache.storm.Config;
import org.slf4j.LoggerFactory;

public class HttpProtocol extends AbstractHttpProtocol {

    private static final org.slf4j.Logger LOG = LoggerFactory.getLogger(HttpProtocol.class);

    private final MediaType JSON = MediaType.parse("application/json; charset=utf-8");

    private OkHttpClient client;

    private int globalMaxContent;

    private int completionTimeout = -1;

    /** Accept partially fetched content as trimmed content */
    private boolean partialContentAsTrimmed = false;

    private final List<KeyValue> customRequestHeaders = new LinkedList<>();

    // track the time spent for each URL in DNS resolution
    private final Map<String, Long> DNStimes = new HashMap<>();

    private OkHttpClient.Builder builder;

    private static final TrustManager[] trustAllCerts =
            new TrustManager[] {
                new X509TrustManager() {
                    @Override
                    public void checkClientTrusted(
                            java.security.cert.X509Certificate[] chain, String authType)
                            throws CertificateException {}

                    @Override
                    public void checkServerTrusted(
                            java.security.cert.X509Certificate[] chain, String authType)
                            throws CertificateException {}

                    @Override
                    public java.security.cert.X509Certificate[] getAcceptedIssuers() {
                        return new java.security.cert.X509Certificate[] {};
                    }
                }
            };

    private static final SSLContext trustAllSslContext;

    static {
        try {
            trustAllSslContext = SSLContext.getInstance("SSL");
            trustAllSslContext.init(null, trustAllCerts, new java.security.SecureRandom());
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }

    private static final SSLSocketFactory trustAllSslSocketFactory =
            trustAllSslContext.getSocketFactory();

    @Override
    public void configure(Config conf) {
        super.configure(conf);

        globalMaxContent = ConfUtils.getInt(conf, "http.content.limit", -1);

        int timeout = ConfUtils.getInt(conf, "http.timeout", 10000);

        this.completionTimeout =
                ConfUtils.getInt(conf, "topology.message.timeout.secs", completionTimeout);

        this.partialContentAsTrimmed =
                ConfUtils.getBoolean(conf, "http.content.partial.as.trimmed", false);

        builder =
                new OkHttpClient.Builder()
                        .retryOnConnectionFailure(true)
                        .followRedirects(false)
                        .connectTimeout(timeout, TimeUnit.MILLISECONDS)
                        .writeTimeout(timeout, TimeUnit.MILLISECONDS)
                        .readTimeout(timeout, TimeUnit.MILLISECONDS);

        // protocols in order of preference, see
        // https://square.github.io/okhttp/4.x/okhttp/okhttp3/-ok-http-client/-builder/protocols/
        List<okhttp3.Protocol> protocols = new ArrayList<>();
        for (String pVersion : protocolVersions) {
            switch (pVersion) {
                case "h2":
                    protocols.add(okhttp3.Protocol.HTTP_2);
                    break;
                case "h2c":
                    if (protocolVersions.size() > 1) {
                        LOG.error("h2c ignored, it cannot be combined with any other protocol");
                    } else {
                        protocols.add(okhttp3.Protocol.H2_PRIOR_KNOWLEDGE);
                    }
                    break;
                case "http/1.1":
                    protocols.add(okhttp3.Protocol.HTTP_1_1);
                    break;
                case "http/1.0":
                    LOG.warn("http/1.0 ignored, not supported by okhttp for requests");
                    break;
                default:
                    LOG.error("{}: unknown protocol version", pVersion);
                    break;
            }
        }
        if (protocols.size() > 0) {
            LOG.info("Using protocol versions: {}", protocols);
            builder.protocols(protocols);
        }

        String userAgent = getAgentString(conf);
        if (StringUtils.isNotBlank(userAgent)) {
            customRequestHeaders.add(new KeyValue("User-Agent", userAgent));
        }

        String accept = ConfUtils.getString(conf, "http.accept");
        if (StringUtils.isNotBlank(accept)) {
            customRequestHeaders.add(new KeyValue("Accept", accept));
        }

        String acceptLanguage = ConfUtils.getString(conf, "http.accept.language");
        if (StringUtils.isNotBlank(acceptLanguage)) {
            customRequestHeaders.add(new KeyValue("Accept-Language", acceptLanguage));
        }

        String basicAuthUser = ConfUtils.getString(conf, "http.basicauth.user", null);

        // use a basic auth?
        if (StringUtils.isNotBlank(basicAuthUser)) {
            String basicAuthPass = ConfUtils.getString(conf, "http.basicauth.password", "");
            String encoding =
                    Base64.getEncoder()
                            .encodeToString((basicAuthUser + ":" + basicAuthPass).getBytes());
            customRequestHeaders.add(new KeyValue("Authorization", "Basic " + encoding));
        }

        customHeaders.forEach(customRequestHeaders::add);

        if (storeHTTPHeaders) {
            builder.addNetworkInterceptor(new HTTPHeadersInterceptor());
        }

        if (ConfUtils.getBoolean(conf, "http.trust.everything", true)) {
            builder.sslSocketFactory(trustAllSslSocketFactory, (X509TrustManager) trustAllCerts[0]);
            builder.hostnameVerifier(
                    new HostnameVerifier() {
                        @Override
                        public boolean verify(String hostname, SSLSession session) {
                            return true;
                        }
                    });
        }

        builder.eventListenerFactory(
                new Factory() {
                    @Override
                    public EventListener create(Call call) {
                        return new DNSResolutionListener(DNStimes);
                    }
                });

        // enable support for Brotli compression (Content-Encoding)
        builder.addInterceptor(BrotliInterceptor.INSTANCE);

        Map<String, Object> connectionPoolConf =
                (Map<String, Object>) conf.get("okhttp.protocol.connection.pool");
        if (connectionPoolConf != null) {
            int size = ConfUtils.getInt(connectionPoolConf, "max.idle.connections", 5);
            int time = ConfUtils.getInt(connectionPoolConf, "connection.keep.alive", 300);
            builder.connectionPool(new ConnectionPool(size, time, TimeUnit.SECONDS));
            LOG.info(
                    "Using connection pool with max. {} idle connections "
                            + "and {} sec. connection keep-alive time",
                    size,
                    time);
        }

        client = builder.build();
    }

    private void addCookiesToRequest(Builder rb, String url, Metadata md) {
        String[] cookieStrings = md.getValues(RESPONSE_COOKIES_HEADER, protocolMDprefix);
        if (cookieStrings == null || cookieStrings.length == 0) {
            return;
        }
        try {
            List<Cookie> cookies = CookieConverter.getCookies(cookieStrings, new URL(url));
            for (Cookie c : cookies) {
                rb.addHeader("Cookie", c.getName() + "=" + c.getValue());
            }
        } catch (MalformedURLException e) { // Bad url , nothing to do
        }
    }

    @Override
    public ProtocolResponse getProtocolOutput(String url, final Metadata metadata)
            throws Exception {
        // create default local client
        OkHttpClient localClient = client;

        // conditionally add a dynamic proxy
        if (proxyManager != null) {
            // retrieve proxy from proxy manager
            SCProxy prox = proxyManager.getProxy(metadata);

            // conditionally configure proxy authentication
            if (StringUtils.isNotBlank(prox.getAddress())) {
                // format SCProxy into native Java proxy
                Proxy proxy =
                        new Proxy(
                                Proxy.Type.valueOf(prox.getProtocol().toUpperCase()),
                                new InetSocketAddress(
                                        prox.getAddress(), Integer.parseInt(prox.getPort())));

                // set proxy in builder
                builder.proxy(proxy);

                // conditionally add proxy authentication
                if (StringUtils.isNotBlank(prox.getUsername())) {
                    // add proxy authentication header to builder
                    builder.proxyAuthenticator(
                            (Route route, Response response) -> {
                                String credential =
                                        Credentials.basic(prox.getUsername(), prox.getPassword());
                                return response.request()
                                        .newBuilder()
                                        .header("Proxy-Authorization", credential)
                                        .build();
                            });
                }
            }

            // save start time for debugging speed impact of client build
            long buildStart = System.currentTimeMillis();

            // create new local client from builder using proxy
            localClient = builder.build();

            LOG.debug(
                    "time to build okhttp client with proxy: {}ms",
                    System.currentTimeMillis() - buildStart);

            LOG.debug("fetching with proxy {} - {} ", url, prox.toString());
        }

        Builder rb = new Request.Builder().url(url);
        customRequestHeaders.forEach(
                (k) -> {
                    rb.header(k.getKey(), k.getValue());
                });

        int pageMaxContent = globalMaxContent;

        if (metadata != null) {
            String lastModified = metadata.getFirstValue(HttpHeaders.LAST_MODIFIED);
            if (StringUtils.isNotBlank(lastModified)) {
                rb.header("If-Modified-Since", HttpHeaders.formatHttpDate(lastModified));
            }

            String ifNoneMatch = metadata.getFirstValue("etag", protocolMDprefix);
            if (StringUtils.isNotBlank(ifNoneMatch)) {
                rb.header("If-None-Match", ifNoneMatch);
            }

            String accept = metadata.getFirstValue("http.accept");
            if (StringUtils.isNotBlank(accept)) {
                rb.header("Accept", accept);
            }

            String acceptLanguage = metadata.getFirstValue("http.accept.language");
            if (StringUtils.isNotBlank(acceptLanguage)) {
                rb.header("Accept-Language", acceptLanguage);
            }

            String pageMaxContentStr = metadata.getFirstValue("http.content.limit");
            if (StringUtils.isNotBlank(pageMaxContentStr)) {
                try {
                    pageMaxContent = Integer.parseInt(pageMaxContentStr);
                } catch (NumberFormatException e) {
                    LOG.warn("Invalid http.content.limit in metadata: {}", pageMaxContentStr);
                }
            }

            if (useCookies) {
                addCookiesToRequest(rb, url, metadata);
            }

            String postJSONData = metadata.getFirstValue("http.post.json");
            if (StringUtils.isNotBlank(postJSONData)) {
                RequestBody body = RequestBody.create(postJSONData, JSON);
                rb.post(body);
            }

            String useHead = metadata.getFirstValue("http.method.head");
            if ("true".equalsIgnoreCase(useHead)) {
                rb.head();
            }
        }

        Request request = rb.build();

        Call call = localClient.newCall(request);

        try (Response response = call.execute()) {

            byte[] bytes = new byte[] {};

            Metadata responsemetadata = new Metadata();
            Headers headers = response.headers();

            for (int i = 0, size = headers.size(); i < size; i++) {
                String key = headers.name(i);
                String value = headers.value(i);

                if (key.equals(ProtocolResponse.REQUEST_HEADERS_KEY)
                        || key.equals(ProtocolResponse.RESPONSE_HEADERS_KEY)) {
                    value = new String(Base64.getDecoder().decode(value));
                }

                responsemetadata.addValue(key.toLowerCase(Locale.ROOT), value);
            }

            MutableObject trimmed = new MutableObject(TrimmedContentReason.NOT_TRIMMED);
            bytes = toByteArray(response.body(), pageMaxContent, trimmed);
            if (trimmed.getValue() != TrimmedContentReason.NOT_TRIMMED) {
                if (!call.isCanceled()) {
                    call.cancel();
                }
                responsemetadata.setValue(ProtocolResponse.TRIMMED_RESPONSE_KEY, "true");
                responsemetadata.setValue(
                        ProtocolResponse.TRIMMED_RESPONSE_REASON_KEY,
                        trimmed.getValue().toString().toLowerCase(Locale.ROOT));
                LOG.warn("HTTP content trimmed to {}", bytes.length);
            }

            Long DNSResolution = DNStimes.remove(call.toString());
            if (DNSResolution != null) {
                responsemetadata.setValue("metrics.dns.resolution.msec", DNSResolution.toString());
            }

            return new ProtocolResponse(bytes, response.code(), responsemetadata);
        }
    }

    private final byte[] toByteArray(
            final ResponseBody responseBody, int maxContent, MutableObject trimmed)
            throws IOException {

        if (responseBody == null) {
            return new byte[] {};
        }

        int maxContentBytes = Constants.MAX_ARRAY_SIZE;
        if (maxContent != -1) {
            maxContentBytes = Math.min(maxContentBytes, maxContent);
        }

        long endDueFor = -1;
        if (completionTimeout != -1) {
            endDueFor = System.currentTimeMillis() + (completionTimeout * 1000);
        }

        BufferedSource source = responseBody.source();
        int bytesRequested = 0;
        int bufferGrowStepBytes = 8192;

        while (source.getBuffer().size() <= maxContentBytes) {
            bytesRequested +=
                    Math.min(
                            bufferGrowStepBytes,
                            /*
                             * request one byte more than required to reliably detect truncated
                             * content, but beware of integer overflows
                             */
                            (maxContentBytes == Constants.MAX_ARRAY_SIZE
                                            ? maxContentBytes
                                            : (1 + maxContentBytes))
                                    - bytesRequested);
            boolean success = false;
            try {
                success = source.request(bytesRequested);
            } catch (IOException e) {
                // requesting more content failed, e.g. by a socket timeout
                if (partialContentAsTrimmed && source.getBuffer().size() > 0) {
                    // treat already fetched content as trimmed
                    trimmed.setValue(TrimmedContentReason.DISCONNECT);
                    LOG.debug("Exception while fetching {}", e);
                } else {
                    throw e;
                }
            }
            if (!success) {
                // source exhausted, no more data to read
                break;
            }

            if (endDueFor != -1 && endDueFor <= System.currentTimeMillis()) {
                // check whether we hit the completion timeout
                trimmed.setValue(TrimmedContentReason.TIME);
                break;
            }

            // okhttp may fetch more content than requested, quickly "increment"
            // bytes
            bytesRequested = (int) source.getBuffer().size();
        }
        int bytesBuffered = (int) source.getBuffer().size();
        int bytesToCopy = bytesBuffered;
        if (maxContent != -1 && bytesToCopy > maxContent) {
            // okhttp's internal buffer is larger than maxContent
            trimmed.setValue(TrimmedContentReason.LENGTH);
            bytesToCopy = maxContentBytes;
        }
        byte[] arr = new byte[bytesToCopy];
        source.getBuffer().readFully(arr);
        return arr;
    }

    class HTTPHeadersInterceptor implements Interceptor {

        private String getNormalizedProtocolName(Protocol protocol) {
            String name = protocol.toString().toUpperCase(Locale.ROOT);
            if ("H2".equals(name)) {
                // back-ward compatible protocol version name
                name = "HTTP/2";
            }
            return name;
        }

        @Override
        public Response intercept(Interceptor.Chain chain) throws IOException {

            long startFetchTime = System.currentTimeMillis();

            Connection connection = chain.connection();
            String ipAddress = connection.socket().getInetAddress().getHostAddress();
            Request request = chain.request();

            StringBuilder requestverbatim = new StringBuilder();

            int position = request.url().toString().indexOf(request.url().host());
            String u = request.url().toString().substring(position + request.url().host().length());

            String httpProtocol = getNormalizedProtocolName(connection.protocol());

            requestverbatim
                    .append(request.method())
                    .append(" ")
                    .append(u)
                    .append(" ")
                    .append(httpProtocol)
                    .append("\r\n");

            Headers headers = request.headers();

            for (int i = 0, size = headers.size(); i < size; i++) {
                String key = headers.name(i);
                String value = headers.value(i);
                requestverbatim.append(key).append(": ").append(value).append("\r\n");
            }

            requestverbatim.append("\r\n");

            Response response = chain.proceed(request);

            StringBuilder responseverbatim = new StringBuilder();

            /*
             * Note: the protocol version between request and response may
             * differ, a server may respond with HTTP/1.0 on a HTTP/1.1 request
             */
            httpProtocol = getNormalizedProtocolName(response.protocol());

            responseverbatim
                    .append(httpProtocol)
                    .append(" ")
                    .append(response.code())
                    .append(" ")
                    .append(response.message())
                    .append("\r\n");

            headers = response.headers();

            for (int i = 0, size = headers.size(); i < size; i++) {
                String key = headers.name(i);
                String value = headers.value(i);
                responseverbatim.append(key).append(": ").append(value).append("\r\n");
            }

            responseverbatim.append("\r\n");

            byte[] encodedBytesResponse =
                    Base64.getEncoder().encode(responseverbatim.toString().getBytes());

            byte[] encodedBytesRequest =
                    Base64.getEncoder().encode(requestverbatim.toString().getBytes());

            // returns a modified version of the response
            return response.newBuilder()
                    .header(ProtocolResponse.REQUEST_HEADERS_KEY, new String(encodedBytesRequest))
                    .header(ProtocolResponse.RESPONSE_HEADERS_KEY, new String(encodedBytesResponse))
                    .header(ProtocolResponse.RESPONSE_IP_KEY, ipAddress)
                    .header(ProtocolResponse.REQUEST_TIME_KEY, Long.toString(startFetchTime))
                    .build();
        }
    }

    public static void main(String args[]) throws Exception {
        HttpProtocol.main(new HttpProtocol(), args);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.protocol;

import com.digitalpebble.stormcrawler.Metadata;

public class ProtocolResponse {

    /**
     * Key which holds the verbatim HTTP request headers in metadata (if supported by Protocol
     * implementation and if http.store.headers is true).
     */
    public static final String REQUEST_HEADERS_KEY = "_request.headers_";
    /** Key which holds the verbatim HTTP response headers in metadata. */
    public static final String RESPONSE_HEADERS_KEY = "_response.headers_";
    /**
     * Key which holds the IP address of the server the request was sent to (response received from)
     * in metadata.
     */
    public static final String RESPONSE_IP_KEY = "_response.ip_";
    /** Key which holds the request time (begin of request) in metadata. */
    public static final String REQUEST_TIME_KEY = "_request.time_";
    /**
     * Metadata key which holds a boolean value in metadata whether the response content is trimmed
     * or not.
     */
    public static final String TRIMMED_RESPONSE_KEY = "http.trimmed";
    /**
     * Metadata key which holds the reason why content has been trimmed, see {@link
     * TrimmedContentReason}.
     */
    public static final String TRIMMED_RESPONSE_REASON_KEY = "http.trimmed.reason";

    /**
     * @since 1.17
     * @see https://github.com/DigitalPebble/storm-crawler/issues/776 *
     */
    public static final String PROTOCOL_MD_PREFIX_PARAM = "protocol.md.prefix";

    /** Enum of reasons which may cause that protocol content is trimmed. */
    public static enum TrimmedContentReason {
        NOT_TRIMMED,
        /** fetch exceeded configured http.content.limit */
        LENGTH,
        /** fetch exceeded configured max. time for fetch */
        TIME,
        /** network disconnect or timeout during fetch */
        DISCONNECT,
        /** implementation internal reason */
        INTERNAL,
        /** unknown reason */
        UNSPECIFIED
    };

    private final byte[] content;
    private final int statusCode;
    private final Metadata metadata;

    public ProtocolResponse(byte[] c, int s, Metadata md) {
        content = c;
        statusCode = s;
        metadata = md == null ? new Metadata() : md;
    }

    public byte[] getContent() {
        return content;
    }

    public int getStatusCode() {
        return statusCode;
    }

    public Metadata getMetadata() {
        return metadata;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.protocol;

import com.digitalpebble.stormcrawler.util.ConfUtils;
import java.net.URL;
import java.util.HashMap;
import org.apache.commons.lang.StringUtils;
import org.apache.storm.Config;

public class ProtocolFactory {

    private final HashMap<String, Protocol[]> cache = new HashMap<>();

    private ProtocolFactory() {}

    private static ProtocolFactory single_instance = null;

    public static ProtocolFactory getInstance(Config conf) {

        if (single_instance != null) return single_instance;

        single_instance = new ProtocolFactory();

        // load the list of protocols
        String[] protocols = ConfUtils.getString(conf, "protocols", "http,https").split(" *, *");

        int protocolInstanceNum = ConfUtils.getInt(conf, "protocol.instances.num", 1);

        // load the class names for each protocol
        // e.g. http.protocol.implementation
        for (String protocol : protocols) {
            String paramName = protocol + ".protocol.implementation";
            String protocolimplementation = ConfUtils.getString(conf, paramName);
            if (StringUtils.isBlank(protocolimplementation)) {
                // set the default values
                if (protocol.equalsIgnoreCase("http")) {
                    protocolimplementation =
                            "com.digitalpebble.stormcrawler.protocol.httpclient.HttpProtocol";
                } else if (protocol.equalsIgnoreCase("https")) {
                    protocolimplementation =
                            "com.digitalpebble.stormcrawler.protocol.httpclient.HttpProtocol";
                } else throw new RuntimeException(paramName + "should not have an empty value");
            }
            // we have a value -> is it correct?
            Class protocolClass;
            try {
                protocolClass = Class.forName(protocolimplementation);
                boolean interfaceOK = Protocol.class.isAssignableFrom(protocolClass);
                if (!interfaceOK) {
                    throw new RuntimeException(
                            "Class " + protocolimplementation + " does not implement Protocol");
                }
                Protocol[] protocolInstances = new Protocol[protocolInstanceNum];
                for (int i = 0; i < protocolInstanceNum; i++) {
                    Protocol protoInstance = (Protocol) protocolClass.newInstance();
                    protoInstance.configure(conf);
                    protocolInstances[i] = protoInstance;
                }
                single_instance.cache.put(protocol, protocolInstances);
            } catch (ClassNotFoundException e) {
                throw new RuntimeException("Can't load class " + protocolimplementation);
            } catch (InstantiationException e) {
                throw new RuntimeException("Can't instanciate class " + protocolimplementation);
            } catch (IllegalAccessException e) {
                throw new RuntimeException(
                        "IllegalAccessException for class " + protocolimplementation);
            }
        }

        return single_instance;
    }

    public synchronized void cleanup() {
        cache.forEach(
                (k, v) -> {
                    for (Protocol p : v) p.cleanup();
                });
    }

    /** Returns an instance of the protocol to use for a given URL */
    public synchronized Protocol getProtocol(URL url) {
        // get the protocol
        String protocol = url.getProtocol();

        // select client from pool
        int hash = url.getHost().hashCode();
        Protocol[] pool = cache.get(protocol);
        return pool[(hash & Integer.MAX_VALUE) % pool.length];
    }

    /**
     * Returns instance(s) of the implementation for the protocol passed as argument.
     *
     * @since 1.17
     * @param string representation of the protocol e.g. http
     */
    public synchronized Protocol[] getProtocol(String protocol) {
        // get the protocol
        return cache.get(protocol);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.protocol.file;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.protocol.Protocol;
import com.digitalpebble.stormcrawler.protocol.ProtocolResponse;
import com.digitalpebble.stormcrawler.protocol.RobotRulesParser;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import crawlercommons.robots.BaseRobotRules;
import org.apache.storm.Config;

public class FileProtocol implements Protocol {

    private String encoding;

    @Override
    public void configure(Config conf) {
        encoding = ConfUtils.getString(conf, "file.encoding", "UTF-8");
    }

    @Override
    public ProtocolResponse getProtocolOutput(String url, Metadata md) throws Exception {
        FileResponse response = new FileResponse(url, md, this);
        return response.toProtocolResponse();
    }

    @Override
    public BaseRobotRules getRobotRules(String url) {
        return RobotRulesParser.EMPTY_RULES;
    }

    public String getEncoding() {
        return encoding;
    }

    @Override
    public void cleanup() {}
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.protocol.file;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.protocol.HttpHeaders;
import com.digitalpebble.stormcrawler.protocol.ProtocolResponse;
import java.io.File;
import java.io.FileInputStream;
import java.io.IOException;
import java.lang.invoke.MethodHandles;
import java.net.URL;
import java.net.URLDecoder;
import java.text.SimpleDateFormat;
import java.util.Date;
import java.util.Locale;
import org.apache.commons.io.IOUtils;
import org.apache.http.HttpStatus;
import org.slf4j.LoggerFactory;

public class FileResponse {

    static final SimpleDateFormat dateFormat =
            new SimpleDateFormat("EEE, dd MMM yyyy HH:mm:ss zzz", Locale.US);
    static final org.slf4j.Logger LOG =
            LoggerFactory.getLogger(MethodHandles.lookup().lookupClass());

    private byte[] content;
    private int statusCode;
    private Metadata metadata;

    public FileResponse(String u, Metadata md, FileProtocol fileProtocol) throws IOException {

        metadata = new Metadata();
        content = new byte[0];
        statusCode = HttpStatus.SC_INTERNAL_SERVER_ERROR;

        URL url = new URL(u);

        if (!url.getPath().equals(url.getFile())) {
            LOG.warn("url.getPath() != url.getFile(): {}.", url);
        }

        String path = "".equals(url.getPath()) ? "/" : url.getPath();

        File file = new File(URLDecoder.decode(path, fileProtocol.getEncoding()));

        if (!file.exists()) {
            statusCode = HttpStatus.SC_NOT_FOUND;
            return;
        }

        if (!file.canRead()) {
            statusCode = HttpStatus.SC_UNAUTHORIZED;
            return;
        }

        if (!file.equals(file.getCanonicalFile())) {
            metadata.setValue(
                    HttpHeaders.LOCATION, file.getCanonicalFile().toURI().toURL().toString());
            statusCode = HttpStatus.SC_MULTIPLE_CHOICES;
            return;
        }

        if (file.isDirectory()) {
            getDirAsHttpResponse(file);
        } else if (file.isFile()) {
            getFileAsHttpResponse(file);
        } else {
            statusCode = HttpStatus.SC_INTERNAL_SERVER_ERROR;
            return;
        }
    }

    public ProtocolResponse toProtocolResponse() {
        return new ProtocolResponse(content, statusCode, metadata);
    }

    private void getFileAsHttpResponse(File file) {
        long size = file.length();

        if (size > Integer.MAX_VALUE) {
            statusCode = HttpStatus.SC_BAD_REQUEST;
            return;
        }

        try {
            content = IOUtils.toByteArray(new FileInputStream(file), size);
        } catch (IOException | IllegalArgumentException e) {
            LOG.error("Exception while fetching file response {} ", file.getPath(), e);
            statusCode = HttpStatus.SC_METHOD_FAILURE;
            return;
        }

        metadata.setValue(HttpHeaders.CONTENT_LENGTH, Long.toString(size));
        metadata.setValue(HttpHeaders.LAST_MODIFIED, formatDate(file.lastModified()));
        statusCode = HttpStatus.SC_OK;
    }

    private void getDirAsHttpResponse(File file) {
        content = generateSitemap(file);
        metadata.setValue(HttpHeaders.CONTENT_TYPE, "application/xml");
        metadata.setValue("isSitemap", "true");
        statusCode = HttpStatus.SC_OK;
    }

    private static String formatDate(long date) {
        return dateFormat.format(new Date(date));
    }

    private byte[] generateSitemap(File dir) {
        File[] files = dir.listFiles();
        StringBuilder sb = new StringBuilder("<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n");
        sb.append("<urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n");
        sb.append("<url><loc>file://").append(dir.getPath()).append("</loc>\n");
        sb.append("  <lastmod>")
                .append(formatDate(dir.lastModified()))
                .append("</lastmod>\n</url>\n");
        for (File file : files) {
            sb.append("<url>\n  <loc>file://").append(file.getPath()).append("</loc>\n");
            sb.append("  <lastmod>")
                    .append(formatDate(file.lastModified()))
                    .append("</lastmod>\n</url>\n");
        }
        sb.append("</urlset>");
        return new String(sb).getBytes();
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.protocol.httpclient;

import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.persistence.Status;
import com.digitalpebble.stormcrawler.protocol.AbstractHttpProtocol;
import com.digitalpebble.stormcrawler.protocol.HttpHeaders;
import com.digitalpebble.stormcrawler.protocol.ProtocolResponse;
import com.digitalpebble.stormcrawler.proxy.*;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import com.digitalpebble.stormcrawler.util.CookieConverter;
import java.io.IOException;
import java.io.InputStream;
import java.net.MalformedURLException;
import java.util.ArrayList;
import java.util.Base64;
import java.util.Collection;
import java.util.LinkedList;
import java.util.List;
import java.util.Locale;
import org.apache.commons.lang.StringUtils;
import org.apache.commons.lang.mutable.MutableBoolean;
import org.apache.http.Header;
import org.apache.http.HeaderIterator;
import org.apache.http.HttpEntity;
import org.apache.http.HttpHost;
import org.apache.http.HttpResponse;
import org.apache.http.StatusLine;
import org.apache.http.auth.AuthScope;
import org.apache.http.auth.UsernamePasswordCredentials;
import org.apache.http.client.ResponseHandler;
import org.apache.http.client.config.AuthSchemes;
import org.apache.http.client.config.CookieSpecs;
import org.apache.http.client.config.RequestConfig;
import org.apache.http.client.methods.HttpGet;
import org.apache.http.client.methods.HttpHead;
import org.apache.http.client.methods.HttpRequestBase;
import org.apache.http.cookie.Cookie;
import org.apache.http.impl.client.BasicCredentialsProvider;
import org.apache.http.impl.client.CloseableHttpClient;
import org.apache.http.impl.client.HttpClientBuilder;
import org.apache.http.impl.client.HttpClients;
import org.apache.http.impl.conn.DefaultProxyRoutePlanner;
import org.apache.http.impl.conn.PoolingHttpClientConnectionManager;
import org.apache.http.message.BasicHeader;
import org.apache.http.util.Args;
import org.apache.http.util.ByteArrayBuffer;
import org.apache.storm.Config;
import org.slf4j.LoggerFactory;

/** Uses Apache httpclient to handle http and https */
public class HttpProtocol extends AbstractHttpProtocol
        implements ResponseHandler<ProtocolResponse> {

    private static final org.slf4j.Logger LOG = LoggerFactory.getLogger(HttpProtocol.class);

    private static final PoolingHttpClientConnectionManager CONNECTION_MANAGER =
            new PoolingHttpClientConnectionManager();

    private int globalMaxContent;

    private HttpClientBuilder builder;

    private RequestConfig requestConfig;
    private RequestConfig.Builder requestConfigBuilder;

    @Override
    public void configure(final Config conf) {

        super.configure(conf);

        // allow up to 200 connections or same as the number of threads used for
        // fetching
        int maxFetchThreads = ConfUtils.getInt(conf, "fetcher.threads.number", 200);
        CONNECTION_MANAGER.setMaxTotal(maxFetchThreads);
        int maxPerRoute = ConfUtils.getInt(conf, "fetcher.threads.per.queue", 1);
        if (maxPerRoute < 20) {
            maxPerRoute = 20;
        }
        CONNECTION_MANAGER.setDefaultMaxPerRoute(maxPerRoute);

        globalMaxContent = ConfUtils.getInt(conf, "http.content.limit", -1);

        String userAgent = getAgentString(conf);

        Collection<BasicHeader> defaultHeaders = new LinkedList<>();

        String accept = ConfUtils.getString(conf, "http.accept");
        if (StringUtils.isNotBlank(accept)) {
            defaultHeaders.add(new BasicHeader("Accept", accept));
        }

        customHeaders.forEach(
                h -> {
                    defaultHeaders.add(new BasicHeader(h.getKey(), h.getValue()));
                });

        String basicAuthUser = ConfUtils.getString(conf, "http.basicauth.user", null);

        // use a basic auth?
        if (StringUtils.isNotBlank(basicAuthUser)) {
            String basicAuthPass = ConfUtils.getString(conf, "http.basicauth.password", "");
            String encoding =
                    Base64.getEncoder()
                            .encodeToString((basicAuthUser + ":" + basicAuthPass).getBytes());
            defaultHeaders.add(new BasicHeader("Authorization", "Basic " + encoding));
        }

        String acceptLanguage = ConfUtils.getString(conf, "http.accept.language");
        if (StringUtils.isNotBlank(acceptLanguage)) {
            defaultHeaders.add(new BasicHeader("Accept-Language", acceptLanguage));
        }

        builder =
                HttpClients.custom()
                        .setUserAgent(userAgent)
                        .setDefaultHeaders(defaultHeaders)
                        .setConnectionManager(CONNECTION_MANAGER)
                        .setConnectionManagerShared(true)
                        .disableRedirectHandling()
                        .disableAutomaticRetries();

        int timeout = ConfUtils.getInt(conf, "http.timeout", 10000);

        requestConfigBuilder =
                RequestConfig.custom()
                        .setSocketTimeout(timeout)
                        .setConnectTimeout(timeout)
                        .setConnectionRequestTimeout(timeout)
                        .setCookieSpec(CookieSpecs.STANDARD);

        requestConfig = requestConfigBuilder.build();
    }

    @Override
    public ProtocolResponse getProtocolOutput(String url, Metadata md) throws Exception {

        LOG.debug("HTTP connection manager stats {}", CONNECTION_MANAGER.getTotalStats());

        // set default request config to global config
        RequestConfig reqConfig = requestConfig;

        // conditionally add a dynamic proxy
        if (proxyManager != null) {
            // retrieve proxy from proxy manager
            SCProxy prox = proxyManager.getProxy(md);

            // conditionally configure proxy authentication
            if (StringUtils.isNotBlank(prox.getUsername())) {
                List<String> authSchemes = new ArrayList<>();

                // Can make configurable and add more in future
                authSchemes.add(AuthSchemes.BASIC);
                requestConfigBuilder.setProxyPreferredAuthSchemes(authSchemes);

                BasicCredentialsProvider basicAuthCreds = new BasicCredentialsProvider();
                basicAuthCreds.setCredentials(
                        new AuthScope(prox.getAddress(), Integer.parseInt(prox.getPort())),
                        new UsernamePasswordCredentials(prox.getUsername(), prox.getPassword()));
                builder.setDefaultCredentialsProvider(basicAuthCreds);
            }

            HttpHost proxy = new HttpHost(prox.getAddress(), Integer.parseInt(prox.getPort()));
            DefaultProxyRoutePlanner routePlanner = new DefaultProxyRoutePlanner(proxy);
            builder.setRoutePlanner(routePlanner);

            // save start time for debugging speed impact of request config
            // build
            long buildStart = System.currentTimeMillis();

            // set request config to new configuration with dynamic proxy
            reqConfig = requestConfigBuilder.build();

            LOG.debug(
                    "time to build http request config with proxy: {}ms",
                    System.currentTimeMillis() - buildStart);

            LOG.debug("fetching with " + prox.toString());
        }

        HttpRequestBase request = new HttpGet(url);
        ResponseHandler<ProtocolResponse> responseHandler = this;

        if (md != null) {
            String useHead = md.getFirstValue("http.method.head");
            if ("true".equalsIgnoreCase(useHead)) {
                request = new HttpHead(url);
            }

            String lastModified = md.getFirstValue(HttpHeaders.LAST_MODIFIED);
            if (StringUtils.isNotBlank(lastModified)) {
                request.addHeader("If-Modified-Since", HttpHeaders.formatHttpDate(lastModified));
            }

            String ifNoneMatch = md.getFirstValue("etag", protocolMDprefix);
            if (StringUtils.isNotBlank(ifNoneMatch)) {
                request.addHeader("If-None-Match", ifNoneMatch);
            }

            String accept = md.getFirstValue("http.accept");
            if (StringUtils.isNotBlank(accept)) {
                request.setHeader(new BasicHeader("Accept", accept));
            }

            String acceptLanguage = md.getFirstValue("http.accept.language");
            if (StringUtils.isNotBlank(acceptLanguage)) {
                request.setHeader(new BasicHeader("Accept-Language", acceptLanguage));
            }

            String pageMaxContentStr = md.getFirstValue("http.content.limit");
            if (StringUtils.isNotBlank(pageMaxContentStr)) {
                try {
                    int pageMaxContent = Integer.parseInt(pageMaxContentStr);
                    responseHandler = getResponseHandlerWithContentLimit(pageMaxContent);
                } catch (NumberFormatException e) {
                    LOG.warn("Invalid http.content.limit in metadata: {}", pageMaxContentStr);
                }
            }

            if (useCookies) {
                addCookiesToRequest(request, md);
            }
        }

        request.setConfig(reqConfig);

        // no need to release the connection explicitly as this is handled
        // automatically. The client itself must be closed though.
        try (CloseableHttpClient httpclient = builder.build()) {
            return httpclient.execute(request, responseHandler);
        }
    }

    private void addCookiesToRequest(HttpRequestBase request, Metadata md) {
        String[] cookieStrings = md.getValues(RESPONSE_COOKIES_HEADER, protocolMDprefix);
        if (cookieStrings != null && cookieStrings.length > 0) {
            List<Cookie> cookies;
            try {
                cookies = CookieConverter.getCookies(cookieStrings, request.getURI().toURL());
                for (Cookie c : cookies) {
                    request.addHeader("Cookie", c.getName() + "=" + c.getValue());
                }
            } catch (MalformedURLException e) { // Bad url , nothing to do
            }
        }
    }

    @Override
    public ProtocolResponse handleResponse(HttpResponse response) throws IOException {
        return handleResponseWithContentLimit(response, globalMaxContent);
    }

    public ProtocolResponse handleResponseWithContentLimit(HttpResponse response, int maxContent)
            throws IOException {
        StatusLine statusLine = response.getStatusLine();
        int status = statusLine.getStatusCode();

        StringBuilder verbatim = new StringBuilder();
        if (storeHTTPHeaders) {
            verbatim.append(statusLine.toString()).append("\r\n");
        }

        Metadata metadata = new Metadata();
        HeaderIterator iter = response.headerIterator();
        while (iter.hasNext()) {
            Header header = iter.nextHeader();
            if (storeHTTPHeaders) {
                verbatim.append(header.toString()).append("\r\n");
            }
            metadata.addValue(header.getName().toLowerCase(Locale.ROOT), header.getValue());
        }

        MutableBoolean trimmed = new MutableBoolean();

        byte[] bytes = new byte[] {};

        if (!Status.REDIRECTION.equals(Status.fromHTTPCode(status))) {
            bytes = HttpProtocol.toByteArray(response.getEntity(), maxContent, trimmed);
            if (trimmed.booleanValue()) {
                metadata.setValue(ProtocolResponse.TRIMMED_RESPONSE_KEY, "true");
                LOG.warn("HTTP content trimmed to {}", bytes.length);
            }
        }

        if (storeHTTPHeaders) {
            verbatim.append("\r\n");
            metadata.setValue(ProtocolResponse.RESPONSE_HEADERS_KEY, verbatim.toString());
        }

        return new ProtocolResponse(bytes, status, metadata);
    }

    private ResponseHandler<ProtocolResponse> getResponseHandlerWithContentLimit(
            int pageMaxContent) {
        return new ResponseHandler<ProtocolResponse>() {
            public ProtocolResponse handleResponse(final HttpResponse response) throws IOException {
                return handleResponseWithContentLimit(response, pageMaxContent);
            }
        };
    }

    private static final byte[] toByteArray(
            final HttpEntity entity, int maxContent, MutableBoolean trimmed) throws IOException {

        if (entity == null) return new byte[] {};

        final InputStream instream = entity.getContent();
        if (instream == null) {
            return null;
        }
        Args.check(
                entity.getContentLength() <= Constants.MAX_ARRAY_SIZE,
                "HTTP entity too large to be buffered in memory");
        int reportedLength = (int) entity.getContentLength();
        // set default size for buffer: 100 KB
        int bufferInitSize = 102400;
        if (reportedLength != -1) {
            bufferInitSize = reportedLength;
        }
        // avoid init of too large a buffer when we will trim anyway
        if (maxContent != -1 && bufferInitSize > maxContent) {
            bufferInitSize = maxContent;
        }
        final ByteArrayBuffer buffer = new ByteArrayBuffer(bufferInitSize);
        final byte[] tmp = new byte[4096];
        int lengthRead;
        while ((lengthRead = instream.read(tmp)) != -1) {
            // check whether we need to trim
            if (maxContent != -1 && buffer.length() + lengthRead > maxContent) {
                buffer.append(tmp, 0, maxContent - buffer.length());
                trimmed.setValue(true);
                break;
            }
            buffer.append(tmp, 0, lengthRead);
        }
        return buffer.toByteArray();
    }

    public static void main(String args[]) throws Exception {
        HttpProtocol.main(new HttpProtocol(), args);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.proxy;

import com.digitalpebble.stormcrawler.Metadata;
import org.apache.storm.Config;

/**
 * Proxy manager is an abstract class specification that details the required interface of a proxy
 * manager
 */
public interface ProxyManager {
    void configure(Config conf);

    SCProxy getProxy(Metadata metadata);
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.proxy;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import org.apache.storm.Config;

/** SingleProxyManager is a ProxyManager implementation for a single proxy endpoint */
public class SingleProxyManager implements ProxyManager {
    private SCProxy proxy;

    public SingleProxyManager() {}

    public void configure(Config conf) {
        // values for single proxy
        String proxyHost = ConfUtils.getString(conf, "http.proxy.host", null);
        String proxyType = ConfUtils.getString(conf, "http.proxy.type", "HTTP");
        int proxyPort = ConfUtils.getInt(conf, "http.proxy.port", 8080);
        String proxyUsername = ConfUtils.getString(conf, "http.proxy.user", null);
        String proxyPassword = ConfUtils.getString(conf, "http.proxy.pass", null);

        // assemble proxy connection string
        String proxyString = proxyType.toLowerCase() + "://";

        // conditionally append authentication info
        if (proxyUsername != null
                && !proxyUsername.isEmpty()
                && proxyPassword != null
                && !proxyPassword.isEmpty()) {
            proxyString += proxyUsername + ":" + proxyPassword + "@";
        }

        // complete proxy string and create proxy
        this.proxy = new SCProxy(proxyString + String.format("%s:%d", proxyHost, proxyPort));
    }

    @Override
    public SCProxy getProxy(Metadata metadata) {
        return proxy;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.proxy;

import java.util.concurrent.atomic.AtomicInteger;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
import org.slf4j.LoggerFactory;

/**
 * Proxy class is used as the central interface to proxy based interactions with a single remote
 * server The class stores all information relating to the remote server, authentication, and usage
 * activity
 */
public class SCProxy {
    // define regex expression used to parse connection strings
    private static final Pattern PROXY_STRING_REGEX =
            Pattern.compile(
                    "(?<protocol>[^:]+)://(?:(?<username>[^:]+):(?<password>[^:]+)@)?(?<host>[^@:]+):(?<port>\\d{2,5})");

    // create logger
    protected static final org.slf4j.Logger LOG = LoggerFactory.getLogger(SCProxy.class);

    // define fields for basic information
    private String protocol;
    private String address;
    private String port;
    private String username;
    private String password;
    private String country;
    private String area;
    private String location;
    private String status;

    // define fields for management
    private AtomicInteger totalUsage;

    /** Default constructor for setting up the proxy */
    private void init() {
        // initialize usage tracker to 0
        this.totalUsage = new AtomicInteger();
    }

    /** Construct a proxy object from a valid proxy connection string */
    public SCProxy(String connectionString) throws IllegalArgumentException {
        // call default constructor
        this.init();

        // load connection string into regex matched
        Matcher matcher = PROXY_STRING_REGEX.matcher(connectionString);

        // ensure that connection string is a valid proxy
        if (!matcher.matches()) {
            throw new IllegalArgumentException(
                    "passed connection string is not of valid proxy format "
                            + "(<PROTO>://(<USER>:<PASS>@)<HOST>:<PORT>) : "
                            + connectionString);
        }

        // load required parameters
        this.protocol = matcher.group("protocol");
        this.address = matcher.group("host");
        this.port = matcher.group("port");

        // load optional authentication data
        try {
            this.username = matcher.group("username");
            this.password = matcher.group("password");
        } catch (IllegalArgumentException ignored) {
        }
    }

    /** Construct a proxy class from it's variables */
    public SCProxy(
            String protocol,
            String address,
            String port,
            String username,
            String password,
            String country,
            String area,
            String location,
            String status)
            throws IllegalArgumentException {
        // call default constructor
        this.init();

        // load required parameters
        this.protocol = protocol;
        this.address = address;
        this.port = port;

        // load optional parameters
        if (!username.isEmpty()) this.username = username;
        if (!password.isEmpty()) this.password = password;
        if (!country.isEmpty()) this.country = country;
        if (!area.isEmpty()) this.area = area;
        if (!location.isEmpty()) this.location = location;
        if (!status.isEmpty()) this.status = status;
    }

    /** Formats the proxy information into a URL compatible connection string */
    public String toString() {
        // assemble base string with address and password
        String proxyString = this.address + ":" + this.port;

        // conditionally add authentication details
        if (this.username != null && this.password != null) {
            // re-assemble url with auth details prepended
            proxyString = this.username + ":" + this.password + "@" + proxyString;
        }

        // prepend protocol string to url and return
        return this.protocol + "://" + proxyString;
    }

    /** Increments the usage tracker for the proxy */
    public void incrementUsage() {
        this.totalUsage.incrementAndGet();
    }

    /** Retrieves the current usage of the proxy */
    public int getUsage() {
        return this.totalUsage.get();
    }

    public String getProtocol() {
        return protocol;
    }

    public String getAddress() {
        return address;
    }

    public String getPort() {
        return port;
    }

    public String getUsername() {
        return username;
    }

    public String getPassword() {
        return password;
    }

    public String getCountry() {
        return country;
    }

    public String getArea() {
        return area;
    }

    public String getLocation() {
        return location;
    }

    public String getStatus() {
        return status;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.proxy;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.protocol.httpclient.HttpProtocol;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import java.io.File;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.net.URL;
import java.util.Random;
import java.util.Scanner;
import java.util.concurrent.atomic.AtomicInteger;
import org.apache.commons.lang.ArrayUtils;
import org.apache.storm.Config;
import org.slf4j.LoggerFactory;

/** MultiProxyManager is a ProxyManager implementation for a multiple proxy endpoints */
public class MultiProxyManager implements ProxyManager {
    public enum ProxyRotation {
        RANDOM,
        LEAST_USED,
        ROUND_ROBIN,
    }

    protected Random rng;
    private SCProxy[] proxies;
    private ProxyRotation rotation;
    private final AtomicInteger lastAccessedIndex = new AtomicInteger(0);

    private static final org.slf4j.Logger LOG = LoggerFactory.getLogger(HttpProtocol.class);

    /** Default constructor for setting up the proxy manager */
    private void init(ProxyRotation rotation) {
        // create rng with nano seed
        this.rng = new Random(System.nanoTime());
        // set rotation
        this.rotation = rotation;
    }

    public MultiProxyManager() {}

    @Override
    public void configure(Config conf) {
        // load proxy file from configuration
        String proxyFile = ConfUtils.getString(conf, "http.proxy.file", null);
        // load proxy rotation from config
        String proxyRot = ConfUtils.getString(conf, "http.proxy.rotation", "ROUND_ROBIN");

        // create variable to hold rotation scheme
        ProxyRotation proxyRotationScheme;

        // map rotation scheme to enum
        switch (proxyRot) {
            case "RANDOM":
                proxyRotationScheme = ProxyRotation.RANDOM;
                break;
            case "LEAST_USED":
                proxyRotationScheme = ProxyRotation.LEAST_USED;
                break;
            default:
                if (!proxyRot.equals("ROUND_ROBIN"))
                    LOG.error(
                            "invalid proxy rotation scheme passed `{}` defaulting to ROUND_ROBIN; options: {}",
                            proxyRot,
                            ProxyRotation.values());
                proxyRotationScheme = ProxyRotation.ROUND_ROBIN;
                break;
        }

        // call default constructor
        this.init(proxyRotationScheme);

        // create variable to hold file scanner
        Scanner scanner;

        // check if file exists in resources
        URL resourcesProxyFilePath = getClass().getClassLoader().getResource(proxyFile);

        // conditionally load file from resources
        if (resourcesProxyFilePath != null) {
            try {
                scanner = new Scanner(resourcesProxyFilePath.openStream());
            } catch (IOException e) {
                throw new RuntimeException("failed to load proxy resource file: " + proxyFile, e);
            }
        } else {
            // open file to load proxies
            File proxyFileObj = new File(proxyFile);

            // create new scanner to read file line-by-line
            try {
                scanner = new Scanner(proxyFileObj);
            } catch (FileNotFoundException e) {
                throw new RuntimeException("failed to load proxy file: " + proxyFile, e);
            }
        }

        // create array to hold the loaded proxies
        SCProxy[] fileProxies = new SCProxy[0];

        // iterate over lines in file loading them into proxy objects
        while (scanner.hasNextLine()) {
            // read line containing proxy connection string
            String proxyConnectionString = scanner.nextLine();

            // skip commented lines and empty lines
            if (proxyConnectionString.startsWith("#")
                    || proxyConnectionString.startsWith("//")
                    || proxyConnectionString.isEmpty()
                    || proxyConnectionString.trim().isEmpty()) continue;

            // attempt to load proxy connection string and add proxy to proxies array
            fileProxies =
                    (SCProxy[]) ArrayUtils.add(fileProxies, new SCProxy(proxyConnectionString));
        }

        // close scanner
        scanner.close();

        // ensure that at least 1 proxy was loaded
        if (fileProxies.length < 1) {
            throw new IllegalArgumentException(
                    "at least one proxy must be loaded to create a multi-proxy manager");
        }

        // assign proxies to class variable
        this.proxies = fileProxies;
    }

    public void configure(ProxyRotation rotation, String[] proxyList) throws RuntimeException {
        // call default constructor
        this.init(rotation);

        // create array to hold the loaded proxies
        SCProxy[] fileProxies = new SCProxy[0];

        // iterate over proxy list loading each proxy into a native proxy object
        for (String proxyConnectionString : proxyList) {
            // attempt to load proxy connection string and add proxy to proxies array
            fileProxies =
                    (SCProxy[]) ArrayUtils.add(fileProxies, new SCProxy(proxyConnectionString));
        }

        // ensure that at least 1 proxy was loaded
        if (proxyList.length < 1) {
            throw new RuntimeException(
                    "at least one proxy must be passed to create a multi-proxy manager");
        }

        // assign proxies to class variable
        this.proxies = fileProxies;
    }

    private SCProxy getRandom() {
        // retrieve a proxy at random from the proxy array and return
        return this.proxies[this.rng.nextInt(this.proxies.length)];
    }

    private SCProxy getRoundRobin() {
        // ensure that last accessed does not exceed proxy list length
        if (this.lastAccessedIndex.get() >= this.proxies.length) this.lastAccessedIndex.set(0);

        // retrieve the current proxy, increment usage index, and return
        return this.proxies[this.lastAccessedIndex.getAndIncrement()];
    }

    private SCProxy getLeastUsed() {
        // start with index 0 in the proxy list
        SCProxy p = this.proxies[0];
        // save total usage to prevent cost of locking attribute on each call (this is a lazy
        // implementation for speed)
        int usage = p.getUsage();

        // iterate over proxies 1->END to find the proxy least used
        for (int i = 1; i < this.proxies.length; i++) {
            // retrieve usage of the proxy
            int u = this.proxies[i].getUsage();
            // check if this proxy was used less
            if (u < usage) {
                // set the new proxy
                p = this.proxies[i];
                usage = u;
            }
        }

        // return the least recently used proxy
        return p;
    }

    public int proxyCount() {
        return this.proxies.length;
    }

    @Override
    public SCProxy getProxy(Metadata metadata) {
        // create a variable to hold the proxy generated in the following switch statement
        SCProxy proxy;

        // map the rotation algorithm to the correct function
        switch (this.rotation) {
            case ROUND_ROBIN:
                proxy = this.getRoundRobin();
                break;
            case LEAST_USED:
                proxy = this.getLeastUsed();
                break;
            default:
                proxy = this.getRandom();
        }

        // update usage for proxy
        proxy.incrementUsage();

        // return proxy
        return proxy;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.json;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.TestUtil;
import com.digitalpebble.stormcrawler.bolt.JSoupParserBolt;
import com.digitalpebble.stormcrawler.parse.ParsingTester;
import java.io.IOException;
import java.util.List;
import java.util.Map;
import org.apache.storm.task.OutputCollector;
import org.junit.Assert;
import org.junit.Before;
import org.junit.Test;

public class JsoupFilterTest extends ParsingTester {

    @Before
    public void setupParserBolt() {
        bolt = new JSoupParserBolt();
        setupParserBolt(bolt);
    }

    protected void prepareParserBolt(String configFile, Map parserConfig) {
        parserConfig.put("jsoup.filters.config.file", configFile);
        bolt.prepare(
                parserConfig, TestUtil.getMockedTopologyContext(), new OutputCollector(output));
    }

    @Test
    public void testLDJsonExtraction() throws IOException {

        prepareParserBolt("test.jsoupfilters.json");

        parse("http://www.digitalpebble.com", "digitalpebble.com.html");

        Assert.assertEquals(1, output.getEmitted().size());
        List<Object> parsedTuple = output.getEmitted().get(0);
        Metadata metadata = (Metadata) parsedTuple.get(2);
        Assert.assertNotNull(metadata);
        String[] scripts = metadata.getValues("streetAddress");
        Assert.assertNotNull(scripts);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.bolt;

import org.junit.Before;

public class SimpleFetcherBoltTest extends AbstractFetcherBoltTest {

    @Before
    public void setUpContext() throws Exception {
        bolt = new SimpleFetcherBolt();
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.bolt;

import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.TestUtil;
import com.digitalpebble.stormcrawler.parse.ParsingTester;
import com.digitalpebble.stormcrawler.protocol.HttpHeaders;
import crawlercommons.sitemaps.extension.Extension;
import java.io.IOException;
import java.util.Arrays;
import java.util.Collections;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.tuple.Values;
import org.junit.Assert;
import org.junit.Before;
import org.junit.Test;

public class SiteMapParserBoltTest extends ParsingTester {

    @Before
    public void setupParserBolt() {
        bolt = new SiteMapParserBolt();
        setupParserBolt(bolt);
    }

    // TODO add a test for a sitemap containing links
    // to other sitemap files

    @Test
    public void testSitemapParsing() throws IOException {

        prepareParserBolt("test.parsefilters.json");

        Metadata metadata = new Metadata();
        // specify that it is a sitemap file
        metadata.setValue(SiteMapParserBolt.isSitemapKey, "true");
        // and its mime-type
        metadata.setValue(HttpHeaders.CONTENT_TYPE, "application/xml");

        parse("http://www.digitalpebble.com/sitemap.xml", "digitalpebble.sitemap.xml", metadata);

        Assert.assertEquals(6, output.getEmitted(Constants.StatusStreamName).size());
        // TODO test that the new links have the right metadata
        List<Object> fields = output.getEmitted(Constants.StatusStreamName).get(0);
        Assert.assertEquals(3, fields.size());
    }

    @Test
    public void testSitemapParsingWithImageExtensions() throws IOException {
        Map parserConfig = new HashMap();
        parserConfig.put("sitemap.extensions", Collections.singletonList(Extension.IMAGE.name()));
        prepareParserBolt("test.parsefilters.json", parserConfig);

        Metadata metadata = new Metadata();
        // specify that it is a sitemap file
        metadata.setValue(SiteMapParserBolt.isSitemapKey, "true");
        // and its mime-type
        metadata.setValue(HttpHeaders.CONTENT_TYPE, "application/xml");

        parse(
                "http://www.digitalpebble.com/sitemap.xml",
                "digitalpebble.sitemap.extensions.image.xml",
                metadata);
        Values values = (Values) output.getEmitted(Constants.StatusStreamName).get(0);
        Metadata parsedMetadata = (Metadata) values.get(1);
        assertImageAttributes(parsedMetadata);
    }

    @Test
    public void testSitemapParsingWithMobileExtensions() throws IOException {
        Map parserConfig = new HashMap();
        parserConfig.put("sitemap.extensions", Collections.singletonList(Extension.MOBILE.name()));
        prepareParserBolt("test.parsefilters.json", parserConfig);

        Metadata metadata = new Metadata();
        // specify that it is a sitemap file
        metadata.setValue(SiteMapParserBolt.isSitemapKey, "true");
        // and its mime-type
        metadata.setValue(HttpHeaders.CONTENT_TYPE, "application/xml");

        parse(
                "http://www.digitalpebble.com/sitemap.xml",
                "digitalpebble.sitemap.extensions.mobile.xml",
                metadata);
        Values values = (Values) output.getEmitted(Constants.StatusStreamName).get(0);
        Metadata parsedMetadata = (Metadata) values.get(1);
        assertMobileAttributes(parsedMetadata);
    }

    @Test
    public void testSitemapParsingWithLinkExtensions() throws IOException {
        Map parserConfig = new HashMap();
        parserConfig.put("sitemap.extensions", Collections.singletonList(Extension.LINKS.name()));
        prepareParserBolt("test.parsefilters.json", parserConfig);

        Metadata metadata = new Metadata();
        // specify that it is a sitemap file
        metadata.setValue(SiteMapParserBolt.isSitemapKey, "true");
        // and its mime-type
        metadata.setValue(HttpHeaders.CONTENT_TYPE, "application/xml");

        parse(
                "http://www.digitalpebble.com/sitemap.xml",
                "digitalpebble.sitemap.extensions.links.xml",
                metadata);
        Values values = (Values) output.getEmitted(Constants.StatusStreamName).get(0);
        Metadata parsedMetadata = (Metadata) values.get(1);
        assertLinksAttributes(parsedMetadata);
    }

    @Test
    public void testSitemapParsingWithNewsExtensions() throws IOException {
        Map parserConfig = new HashMap();
        parserConfig.put("sitemap.extensions", Collections.singletonList(Extension.NEWS.name()));
        prepareParserBolt("test.parsefilters.json", parserConfig);

        Metadata metadata = new Metadata();
        // specify that it is a sitemap file
        metadata.setValue(SiteMapParserBolt.isSitemapKey, "true");
        // and its mime-type
        metadata.setValue(HttpHeaders.CONTENT_TYPE, "application/xml");

        parse(
                "http://www.digitalpebble.com/sitemap.xml",
                "digitalpebble.sitemap.extensions.news.xml",
                metadata);
        Values values = (Values) output.getEmitted(Constants.StatusStreamName).get(0);
        Metadata parsedMetadata = (Metadata) values.get(1);
        assertNewsAttributes(parsedMetadata);
    }

    @Test
    public void testSitemapParsingWithVideoExtensions() throws IOException {
        Map parserConfig = new HashMap();
        parserConfig.put("sitemap.extensions", Collections.singletonList(Extension.VIDEO.name()));
        prepareParserBolt("test.parsefilters.json", parserConfig);

        Metadata metadata = new Metadata();
        // specify that it is a sitemap file
        metadata.setValue(SiteMapParserBolt.isSitemapKey, "true");
        // and its mime-type
        metadata.setValue(HttpHeaders.CONTENT_TYPE, "application/xml");

        parse(
                "http://www.digitalpebble.com/sitemap.xml",
                "digitalpebble.sitemap.extensions.video.xml",
                metadata);
        Values values = (Values) output.getEmitted(Constants.StatusStreamName).get(0);
        Metadata parsedMetadata = (Metadata) values.get(1);
        assertVideoAttributes(parsedMetadata);
    }

    @Test
    public void testSitemapParsingWithAllExtensions() throws IOException {
        Map parserConfig = new HashMap();

        parserConfig.put(
                "sitemap.extensions",
                Arrays.asList(
                        Extension.MOBILE.name(),
                        Extension.NEWS.name(),
                        Extension.LINKS.name(),
                        Extension.VIDEO.name(),
                        Extension.IMAGE.name()));
        prepareParserBolt("test.parsefilters.json", parserConfig);

        Metadata metadata = new Metadata();
        // specify that it is a sitemap file
        metadata.setValue(SiteMapParserBolt.isSitemapKey, "true");
        // and its mime-type
        metadata.setValue(HttpHeaders.CONTENT_TYPE, "application/xml");

        parse(
                "http://www.digitalpebble.com/sitemap.xml",
                "digitalpebble.sitemap.extensions.all.xml",
                metadata);
        Values values = (Values) output.getEmitted(Constants.StatusStreamName).get(0);
        Metadata parsedMetadata = (Metadata) values.get(1);
        assertImageAttributes(parsedMetadata);
        assertNewsAttributes(parsedMetadata);
        assertVideoAttributes(parsedMetadata);
        assertLinksAttributes(parsedMetadata);
        assertMobileAttributes(parsedMetadata);
    }

    @Test(expected = IllegalArgumentException.class)
    public void testSitemapParsingWithIllegalExtensionConfigured() throws IOException {
        Map parserConfig = new HashMap();
        parserConfig.put("sitemap.extensions", Arrays.asList("AUDIONEWSLINKS"));
        prepareParserBolt("test.parsefilters.json", parserConfig);
    }

    @Test
    public void testSitemapParsingNoMT() throws IOException {

        Map parserConfig = new HashMap();
        parserConfig.put("sitemap.sniffContent", true);
        parserConfig.put("parsefilters.config.file", "test.parsefilters.json");
        bolt.prepare(
                parserConfig, TestUtil.getMockedTopologyContext(), new OutputCollector(output));

        Metadata metadata = new Metadata();
        // do not specify that it is a sitemap file
        // do not set the mimetype

        parse("http://www.digitalpebble.com/sitemap.xml", "digitalpebble.sitemap.xml", metadata);

        Assert.assertEquals(6, output.getEmitted(Constants.StatusStreamName).size());
        // TODO test that the new links have the right metadata
        List<Object> fields = output.getEmitted(Constants.StatusStreamName).get(0);
        Assert.assertEquals(3, fields.size());
    }

    @Test
    public void testNonSitemapParsing() throws IOException {

        prepareParserBolt("test.parsefilters.json");
        // do not specify that it is a sitemap file
        parse("http://www.digitalpebble.com", "digitalpebble.com.html", new Metadata());

        Assert.assertEquals(1, output.getEmitted().size());
    }

    private void assertNewsAttributes(Metadata metadata) {
        long numAttributes =
                metadata.keySet().stream()
                        .filter(key -> key.startsWith(Extension.NEWS.name() + "."))
                        .count();
        Assert.assertEquals(7, numAttributes);
        Assert.assertEquals(
                "The Example Times", metadata.getFirstValue(Extension.NEWS.name() + "." + "name"));
        Assert.assertEquals("en", metadata.getFirstValue(Extension.NEWS.name() + "." + "language"));
        Assert.assertArrayEquals(
                new String[] {"PressRelease", "Blog"},
                metadata.getValues(Extension.NEWS.name() + "." + "genres"));
        Assert.assertEquals(
                "2008-12-23T00:00Z",
                metadata.getFirstValue(Extension.NEWS.name() + "." + "publication_date"));
        Assert.assertEquals(
                "Companies A, B in Merger Talks",
                metadata.getFirstValue(Extension.NEWS.name() + "." + "title"));
        Assert.assertArrayEquals(
                new String[] {"business", "merger", "acquisition", "A", "B"},
                metadata.getValues(Extension.NEWS.name() + "." + "keywords"));
        Assert.assertArrayEquals(
                new String[] {"NASDAQ:A", "NASDAQ:B"},
                metadata.getValues(Extension.NEWS.name() + "." + "stock_tickers"));
    }

    private void assertImageAttributes(Metadata metadata) {
        long numAttributes =
                metadata.keySet().stream()
                        .filter(key -> key.startsWith(Extension.IMAGE.name() + "."))
                        .count();
        Assert.assertEquals(5, numAttributes);
        Assert.assertEquals(
                "This is the caption.",
                metadata.getFirstValue(Extension.IMAGE.name() + "." + "caption"));
        Assert.assertEquals(
                "http://example.com/photo.jpg",
                metadata.getFirstValue(Extension.IMAGE.name() + "." + "loc"));
        Assert.assertEquals(
                "Example photo shot in Limerick, Ireland",
                metadata.getFirstValue(Extension.IMAGE.name() + "." + "title"));
        Assert.assertEquals(
                "https://creativecommons.org/licenses/by/4.0/legalcode",
                metadata.getFirstValue(Extension.IMAGE.name() + "." + "license"));
        Assert.assertEquals(
                "Limerick, Ireland",
                metadata.getFirstValue(Extension.IMAGE.name() + "." + "geo_location"));
    }

    private void assertLinksAttributes(Metadata metadata) {
        long numAttributes =
                metadata.keySet().stream()
                        .filter(key -> key.startsWith(Extension.LINKS.name() + "."))
                        .count();
        Assert.assertEquals(3, numAttributes);
        Assert.assertEquals(
                "alternate", metadata.getFirstValue(Extension.LINKS.name() + "." + "params.rel"));
        Assert.assertEquals(
                "http://www.example.com/english/",
                metadata.getFirstValue(Extension.LINKS.name() + "." + "href"));
        Assert.assertEquals(
                "en", metadata.getFirstValue(Extension.LINKS.name() + "." + "params.hreflang"));
    }

    private void assertVideoAttributes(Metadata metadata) {
        long numAttributes =
                metadata.keySet().stream()
                        .filter(key -> key.startsWith(Extension.VIDEO.name() + "."))
                        .count();
        Assert.assertEquals(20, numAttributes);
        Assert.assertEquals(
                "http://www.example.com/thumbs/123.jpg",
                metadata.getFirstValue(Extension.VIDEO.name() + "." + "thumbnail_loc"));
        Assert.assertEquals(
                "Grilling steaks for summer",
                metadata.getFirstValue(Extension.VIDEO.name() + "." + "title"));
        Assert.assertEquals(
                "Alkis shows you how to get perfectly done steaks every time",
                metadata.getFirstValue(Extension.VIDEO.name() + "." + "description"));
        Assert.assertEquals(
                "http://www.example.com/video123.flv",
                metadata.getFirstValue(Extension.VIDEO.name() + "." + "content_loc"));
        Assert.assertEquals(
                "http://www.example.com/videoplayer.swf?video=123",
                metadata.getFirstValue(Extension.VIDEO.name() + "." + "player_loc"));
        // Assert.assertEquals("600", metadata.getFirstValue(Extension.VIDEO.name() +
        // "." + "duration"));
        Assert.assertEquals(
                "2009-11-05T19:20:30+08:00",
                metadata.getFirstValue(Extension.VIDEO.name() + "." + "expiration_date"));
        Assert.assertEquals("4.2", metadata.getFirstValue(Extension.VIDEO.name() + "." + "rating"));
        Assert.assertEquals(
                "12345", metadata.getFirstValue(Extension.VIDEO.name() + "." + "view_count"));
        Assert.assertEquals(
                "2007-11-05T19:20:30+08:00",
                metadata.getFirstValue(Extension.VIDEO.name() + "." + "publication_date"));
        Assert.assertEquals(
                "true", metadata.getFirstValue(Extension.VIDEO.name() + "." + "family_friendly"));
        Assert.assertArrayEquals(
                new String[] {"sample_tag1", "sample_tag2"},
                metadata.getValues(Extension.VIDEO.name() + "." + "tags"));
        Assert.assertArrayEquals(
                new String[] {"IE", "GB", "US", "CA"},
                metadata.getValues(Extension.VIDEO.name() + "." + "allowed_countries"));
        Assert.assertEquals(
                "http://cooking.example.com",
                metadata.getFirstValue(Extension.VIDEO.name() + "." + "gallery_loc"));
        Assert.assertEquals(
                "value: 1.99, currency: EUR, type: own, resolution: null",
                metadata.getFirstValue(Extension.VIDEO.name() + "." + "prices"));
        Assert.assertEquals(
                "true",
                metadata.getFirstValue(Extension.VIDEO.name() + "." + "requires_subscription"));
        Assert.assertEquals(
                "GrillyMcGrillerson",
                metadata.getFirstValue(Extension.VIDEO.name() + "." + "uploader"));
        Assert.assertEquals(
                "http://www.example.com/users/grillymcgrillerson",
                metadata.getFirstValue(Extension.VIDEO.name() + "." + "uploader_info"));
        Assert.assertEquals(
                "false", metadata.getFirstValue(Extension.VIDEO.name() + "." + "is_live"));
    }

    private void assertMobileAttributes(Metadata metadata) {
        long numAttributes =
                metadata.keySet().stream()
                        .filter(key -> key.startsWith(Extension.MOBILE.name() + "."))
                        .count();
        Assert.assertEquals(0, numAttributes);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.bolt;

import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.TestUtil;
import com.digitalpebble.stormcrawler.parse.ParsingTester;
import com.digitalpebble.stormcrawler.protocol.HttpHeaders;
import com.digitalpebble.stormcrawler.protocol.ProtocolResponse;
import java.io.IOException;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import org.apache.storm.task.OutputCollector;
import org.junit.Assert;
import org.junit.Before;
import org.junit.Test;

public class FeedParserBoltTest extends ParsingTester {

    @Before
    public void setupParserBolt() {
        bolt = new FeedParserBolt();
        setupParserBolt(bolt);
    }

    private void checkOutput() {
        Assert.assertEquals(170, output.getEmitted(Constants.StatusStreamName).size());
        List<Object> fields = output.getEmitted(Constants.StatusStreamName).get(0);
        Assert.assertEquals(3, fields.size());
    }

    @Test
    public void testFeedParsing() throws IOException {

        prepareParserBolt("test.parsefilters.json");

        Metadata metadata = new Metadata();
        // specify that it is a Feed file
        metadata.setValue(FeedParserBolt.isFeedKey, "true");
        parse("http://www.guardian.com/Feed.xml", "guardian.rss", metadata);
        checkOutput();
    }

    @Test
    public void testFeedParsingNoMT() throws IOException {

        Map parserConfig = new HashMap();
        parserConfig.put("feed.sniffContent", true);
        parserConfig.put("parsefilters.config.file", "test.parsefilters.json");
        parserConfig.put(ProtocolResponse.PROTOCOL_MD_PREFIX_PARAM, "http.");
        bolt.prepare(
                parserConfig, TestUtil.getMockedTopologyContext(), new OutputCollector(output));

        Metadata metadata = new Metadata();

        // set mime-type
        metadata.setValue("http." + HttpHeaders.CONTENT_TYPE, "application/rss+xml");

        parse("http://www.guardian.com/feed.xml", "guardian.rss", metadata);

        checkOutput();
    }

    @Test
    public void testFeedParsingDetextBytes() throws IOException {

        Map parserConfig = new HashMap();
        parserConfig.put("feed.sniffContent", true);
        parserConfig.put("parsefilters.config.file", "test.parsefilters.json");
        bolt.prepare(
                parserConfig, TestUtil.getMockedTopologyContext(), new OutputCollector(output));

        Metadata metadata = new Metadata();
        parse("http://www.guardian.com/feed.xml", "guardian.rss", metadata);

        checkOutput();
    }

    @Test
    public void testNonFeedParsing() throws IOException {

        prepareParserBolt("test.parsefilters.json");
        // do not specify that it is a feed file
        parse("http://www.digitalpebble.com", "digitalpebble.com.html", new Metadata());

        Assert.assertEquals(1, output.getEmitted().size());
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.bolt;

import static com.github.tomakehurst.wiremock.client.WireMock.aResponse;
import static com.github.tomakehurst.wiremock.client.WireMock.get;
import static com.github.tomakehurst.wiremock.client.WireMock.stubFor;
import static com.github.tomakehurst.wiremock.client.WireMock.urlMatching;
import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.TestOutputCollector;
import com.digitalpebble.stormcrawler.TestUtil;
import com.github.tomakehurst.wiremock.junit.WireMockRule;
import java.io.IOException;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.topology.base.BaseRichBolt;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.utils.Utils;
import org.junit.Assert;
import org.junit.Rule;
import org.junit.Test;

public abstract class AbstractFetcherBoltTest {

    BaseRichBolt bolt;

    private static final int port = 8089;

    @Rule public WireMockRule wireMockRule = new WireMockRule(port);

    @Test
    public void testDodgyURL() throws IOException {

        TestOutputCollector output = new TestOutputCollector();

        Map config = new HashMap();
        config.put("http.agent.name", "this is only a test");

        bolt.prepare(config, TestUtil.getMockedTopologyContext(), new OutputCollector(output));

        Tuple tuple = mock(Tuple.class);
        when(tuple.getSourceComponent()).thenReturn("source");
        when(tuple.getStringByField("url")).thenReturn("ahahaha");
        when(tuple.getValueByField("metadata")).thenReturn(null);
        bolt.execute(tuple);

        boolean acked = output.getAckedTuples().contains(tuple);
        boolean failed = output.getAckedTuples().contains(tuple);

        // should be acked or failed
        Assert.assertEquals(true, acked || failed);

        List<List<Object>> statusTuples = output.getEmitted(Constants.StatusStreamName);

        // we should get one tuple on the status stream
        // to notify that the URL is an error
        Assert.assertEquals(1, statusTuples.size());
    }

    @Test
    public void test304() {

        stubFor(get(urlMatching(".+")).willReturn(aResponse().withStatus(304)));

        TestOutputCollector output = new TestOutputCollector();

        Map config = new HashMap();
        config.put("http.agent.name", "this is only a test");

        bolt.prepare(config, TestUtil.getMockedTopologyContext(), new OutputCollector(output));

        Tuple tuple = mock(Tuple.class);
        when(tuple.getSourceComponent()).thenReturn("source");
        when(tuple.getStringByField("url")).thenReturn("http://localhost:" + port + "/");
        when(tuple.getValueByField("metadata")).thenReturn(null);
        bolt.execute(tuple);

        while (output.getAckedTuples().size() == 0 && output.getFailedTuples().size() == 0) {
            try {
                Thread.sleep(100);
            } catch (InterruptedException e) {
            }
        }

        boolean acked = output.getAckedTuples().contains(tuple);
        boolean failed = output.getFailedTuples().contains(tuple);

        // should be acked or failed
        Assert.assertEquals(true, acked || failed);

        List<List<Object>> statusTuples = output.getEmitted(Constants.StatusStreamName);

        // we should get one tuple on the status stream
        // to notify that the URL has been fetched
        Assert.assertEquals(1, statusTuples.size());

        // and none on the default stream as there is nothing to parse and/or
        // index
        Assert.assertEquals(0, output.getEmitted(Utils.DEFAULT_STREAM_ID).size());
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.bolt;

import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.TestUtil;
import com.digitalpebble.stormcrawler.parse.ParsingTester;
import com.digitalpebble.stormcrawler.util.RobotsTags;
import java.io.IOException;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import org.apache.storm.task.OutputCollector;
import org.junit.Assert;
import org.junit.Before;
import org.junit.Test;

public class JSoupParserBoltTest extends ParsingTester {

    /*
     *
     * some sample tags:
     *
     * <meta name="robots" content="index,follow"> <meta name="robots"
     * content="noindex,follow"> <meta name="robots" content="index,nofollow">
     * <meta name="robots" content="noindex,nofollow">
     *
     * <META HTTP-EQUIV="Pragma" CONTENT="no-cache">
     */

    Map stormConf = new HashMap();

    public static String[] tests = {
        "<html><head><title>test page</title>"
                + "<META NAME=\"ROBOTS\" CONTENT=\"NONE\"> "
                + "</head><body>"
                + " some text"
                + "</body></html>",
        "<html><head><title>test page</title>"
                + "<meta name=\"robots\" content=\"all\"> "
                + "</head><body>"
                + " some text"
                + "</body></html>",
        "<html><head><title>test page</title>"
                + "<MeTa NaMe=\"RoBoTs\" CoNtEnT=\"nOnE\"> "
                + "</head><body>"
                + " some text"
                + "</body></html>",
        "<html><head><title>test page</title>"
                + "<meta name=\"robots\" content=\"none\"> "
                + "</head><body>"
                + " some text"
                + "</body></html>",
        "<html><head><title>test page</title>"
                + "<meta name=\"robots\" content=\"noindex,nofollow\"> "
                + "</head><body>"
                + " some text"
                + "</body></html>",
        "<html><head><title>test page</title>"
                + "<meta name=\"robots\" content=\"noindex,follow\"> "
                + "</head><body>"
                + " some text"
                + "</body></html>",
        "<html><head><title>test page</title>"
                + "<meta name=\"robots\" content=\"index,nofollow\"> "
                + "</head><body>"
                + " some text"
                + "</body></html>",
        "<html><head><title>test page</title>"
                + "<meta name=\"robots\" content=\"index,follow\"> "
                + "<base href=\"http://www.nutch.org/\">"
                + "</head><body>"
                + " some text"
                + "</body></html>",
        "<html><head><title>test page</title>"
                + "<meta name=\"robots\"> "
                + "<base href=\"http://www.nutch.org/base/\">"
                + "</head><body>"
                + " some text"
                + "</body></html>",
    };

    public static final boolean[][] answers = {
        {true, true, true}, // NONE
        {false, false, false}, // all
        {true, true, true}, // nOnE
        {true, true, true}, // none
        {true, true, false}, // noindex,nofollow
        {true, false, false}, // noindex,follow
        {false, true, false}, // index,nofollow
        {false, false, false}, // index,follow
        {false, false, false}, // missing!
    };

    @Before
    public void setupParserBolt() {
        bolt = new JSoupParserBolt();
        setupParserBolt(bolt);
    }

    @Test
    /** Checks that content in script is not included in the text representation */
    public void testNoScriptInText() throws IOException {

        bolt.prepare(
                new HashMap(), TestUtil.getMockedTopologyContext(), new OutputCollector(output));

        parse("http://www.digitalpebble.com", "digitalpebble.com.html");

        List<Object> parsedTuple = output.getEmitted().remove(0);

        // check in the metadata that the values match
        String text = (String) parsedTuple.get(3);

        Assert.assertFalse(
                "Text should not contain the content of script tags",
                text.contains("urchinTracker"));
    }

    @Test
    /** Checks that individual links marked as rel="nofollow" are not followed */
    public void testNoFollowOutlinks() throws IOException {

        bolt.prepare(
                new HashMap(), TestUtil.getMockedTopologyContext(), new OutputCollector(output));

        parse("http://www.digitalpebble.com", "digitalpebble.com.html");

        List<List<Object>> statusTuples = output.getEmitted(Constants.StatusStreamName);

        Assert.assertEquals(10, statusTuples.size());
    }

    @Test
    public void testHTTPRobots() throws IOException {

        bolt.prepare(
                new HashMap(), TestUtil.getMockedTopologyContext(), new OutputCollector(output));

        Metadata metadata = new Metadata();
        metadata.setValues("X-Robots-Tag", new String[] {"noindex", "nofollow"});

        parse("http://www.digitalpebble.com", "digitalpebble.com.html", metadata);

        List<List<Object>> statusTuples = output.getEmitted(Constants.StatusStreamName);

        // no outlinks at all
        Assert.assertEquals(0, statusTuples.size());

        Assert.assertEquals(1, output.getEmitted().size());
        List<Object> parsedTuple = output.getEmitted().remove(0);

        // check in the metadata that the values match
        metadata = (Metadata) parsedTuple.get(2);
        Assert.assertNotNull(metadata);

        boolean isNoIndex =
                Boolean.parseBoolean(metadata.getFirstValue(RobotsTags.ROBOTS_NO_INDEX));
        boolean isNoFollow =
                Boolean.parseBoolean(metadata.getFirstValue(RobotsTags.ROBOTS_NO_FOLLOW));
        boolean isNoCache =
                Boolean.parseBoolean(metadata.getFirstValue(RobotsTags.ROBOTS_NO_CACHE));

        Assert.assertEquals("incorrect noIndex", true, isNoIndex);
        Assert.assertEquals("incorrect noFollow", true, isNoFollow);
        Assert.assertEquals("incorrect noCache", false, isNoCache);
    }

    @Test
    public void testRobotsMetaProcessor() throws IOException {

        bolt.prepare(
                new HashMap(), TestUtil.getMockedTopologyContext(), new OutputCollector(output));

        for (int i = 0; i < tests.length; i++) {

            byte[] bytes = tests[i].getBytes();

            parse("http://www.digitalpebble.com", bytes, new Metadata());

            Assert.assertEquals(1, output.getEmitted().size());
            List<Object> parsedTuple = output.getEmitted().remove(0);

            // check in the metadata that the values match
            Metadata metadata = (Metadata) parsedTuple.get(2);
            Assert.assertNotNull(metadata);

            boolean isNoIndex =
                    Boolean.parseBoolean(metadata.getFirstValue(RobotsTags.ROBOTS_NO_INDEX));
            boolean isNoFollow =
                    Boolean.parseBoolean(metadata.getFirstValue(RobotsTags.ROBOTS_NO_FOLLOW));
            boolean isNoCache =
                    Boolean.parseBoolean(metadata.getFirstValue(RobotsTags.ROBOTS_NO_CACHE));

            Assert.assertEquals("incorrect noIndex value on doc " + i, answers[i][0], isNoIndex);
            Assert.assertEquals("incorrect noFollow value on doc " + i, answers[i][1], isNoFollow);
            Assert.assertEquals("incorrect noCache value on doc " + i, answers[i][2], isNoCache);
        }
    }

    @Test
    public void testHTMLRedir() throws IOException {

        bolt.prepare(
                new HashMap(), TestUtil.getMockedTopologyContext(), new OutputCollector(output));

        parse("http://www.somesite.com", "redir.html");

        List<List<Object>> statusTuples = output.getEmitted(Constants.StatusStreamName);

        // one for the redir + one for the discovered
        Assert.assertEquals(2, statusTuples.size());
    }

    @Test
    public void testExecuteWithOutlinksLimit() throws IOException {
        stormConf.put("parser.emitOutlinks.max.per.page", 5);
        bolt.prepare(stormConf, TestUtil.getMockedTopologyContext(), new OutputCollector(output));

        parse("http://www.digitalpebble.com", "digitalpebble.com.html");

        List<List<Object>> statusTuples = output.getEmitted(Constants.StatusStreamName);

        // outlinks being limited by property
        Assert.assertEquals(5, statusTuples.size());
    }

    @Test
    public void testExecuteWithOutlinksLimitDisabled() throws IOException {
        stormConf.put("parser.emitOutlinks.max.per.page", -1);
        bolt.prepare(stormConf, TestUtil.getMockedTopologyContext(), new OutputCollector(output));

        parse("http://www.digitalpebble.com", "digitalpebble.com.html");

        List<List<Object>> statusTuples = output.getEmitted(Constants.StatusStreamName);

        // outlinks NOT being limited by property, since is disabled with -1
        Assert.assertEquals(10, statusTuples.size());
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.bolt;

import org.junit.Before;

public class FetcherBoltTest extends AbstractFetcherBoltTest {

    @Before
    public void setUpContext() throws Exception {
        bolt = new FetcherBolt();
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler;

import static org.mockito.Matchers.any;
import static org.mockito.Matchers.anyInt;
import static org.mockito.Matchers.anyString;
import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

import java.nio.charset.Charset;
import java.util.Map;
import org.apache.storm.metric.api.IMetric;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.tuple.Tuple;
import org.mockito.invocation.InvocationOnMock;
import org.mockito.stubbing.Answer;

public class TestUtil {

    private TestUtil() {}

    public static TopologyContext getMockedTopologyContext() {
        TopologyContext context = mock(TopologyContext.class);
        when(context.registerMetric(anyString(), any(IMetric.class), anyInt()))
                .thenAnswer(
                        new Answer<IMetric>() {

                            @Override
                            public IMetric answer(InvocationOnMock invocation) throws Throwable {
                                return invocation.getArgument(1, IMetric.class);
                            }
                        });
        return context;
    }

    public static Tuple getMockedTestTuple(String url, String content, Metadata metadata) {
        Tuple tuple = mock(Tuple.class);
        when(tuple.getStringByField("url")).thenReturn(url);
        when(tuple.getBinaryByField("content"))
                .thenReturn(content.getBytes(Charset.defaultCharset()));
        if (metadata == null) {
            when(tuple.contains("metadata")).thenReturn(Boolean.FALSE);
        } else {
            when(tuple.contains("metadata")).thenReturn(Boolean.TRUE);
            when(tuple.getValueByField("metadata")).thenReturn(metadata);
        }
        return tuple;
    }

    /** A more generic test tuple */
    public static Tuple getMockedTestTuple(final Map<String, Object> tupleValues) {
        Tuple tuple = mock(Tuple.class);
        when(tuple.contains(anyString()))
                .thenAnswer(
                        new Answer<Boolean>() {
                            @Override
                            public Boolean answer(InvocationOnMock invocation) throws Throwable {
                                return tupleValues.containsKey(
                                        invocation.getArgument(0, String.class));
                            }
                        });
        when(tuple.getValueByField(anyString()))
                .thenAnswer(
                        new Answer() {
                            @Override
                            public Object answer(InvocationOnMock invocation) throws Throwable {
                                return tupleValues.get(invocation.getArgument(0, String.class));
                            }
                        });
        when(tuple.getStringByField(anyString()))
                .thenAnswer(
                        new Answer<String>() {
                            @Override
                            public String answer(InvocationOnMock invocation) throws Throwable {
                                return (String)
                                        tupleValues.get(invocation.getArgument(0, String.class));
                            }
                        });
        return tuple;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler;

import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertTrue;

import java.io.IOException;
import java.util.Map;
import org.apache.storm.Config;
import org.apache.storm.serialization.KryoValuesDeserializer;
import org.apache.storm.serialization.KryoValuesSerializer;
import org.apache.storm.utils.Utils;
import org.junit.Test;

public class TestMetadataSerialization {

    @Test
    public void testSerialization() throws IOException {
        Map conf = Utils.readDefaultConfig();
        Config.registerSerialization(conf, Metadata.class);

        KryoValuesSerializer kvs = new KryoValuesSerializer(conf);
        Metadata md = new Metadata();
        md.addValue("this_key", "has a value");
        // defensive lock
        md.lock();

        boolean exception = false;
        try {
            md.addValue("this_should", "fail");
        } catch (Exception e) {
            exception = true;
        }

        assertTrue(exception);

        byte[] content = kvs.serializeObject(md);

        KryoValuesDeserializer kvd = new KryoValuesDeserializer(conf);
        Metadata md2 = (Metadata) kvd.deserializeObject(content);

        // compare md1 and md2
        assertEquals(md.toString(), md2.toString());
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.indexer;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.indexing.AbstractIndexerBolt;
import java.util.HashMap;
import java.util.Iterator;
import java.util.Map;
import org.apache.commons.lang.StringUtils;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.tuple.Tuple;

public class DummyIndexer extends AbstractIndexerBolt {
    OutputCollector _collector;
    Map<String, String> fields;

    @SuppressWarnings("rawtypes")
    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
        super.prepare(conf, context, collector);
        _collector = collector;
        fields = new HashMap<>();
    }

    @Override
    public void execute(Tuple tuple) {

        Metadata metadata = (Metadata) tuple.getValueByField("metadata");

        // tested by the TestOutputCollector
        boolean keep = filterDocument(metadata);
        if (!keep) {
            _collector.ack(tuple);
            return;
        }

        // display text of the document?
        if (StringUtils.isNotBlank(fieldNameForText())) {
            String text = tuple.getStringByField("text");
            fields.put(fieldNameForText(), trimText(text));
        }

        if (StringUtils.isNotBlank(fieldNameForURL())) {
            // Distinguish the value used for indexing
            // from the one used for the status
            String normalisedurl = valueForURL(tuple);
            fields.put(fieldNameForURL(), normalisedurl);
        }

        // which metadata to display?
        Map<String, String[]> keyVals = filterMetadata(metadata);

        Iterator<String> iterator = keyVals.keySet().iterator();
        while (iterator.hasNext()) {
            String fieldName = iterator.next();
            String[] values = keyVals.get(fieldName);
            for (String value : values) {
                fields.put(fieldName, value);
            }
        }

        _collector.ack(tuple);
    }

    private String trimValue(String value) {
        if (value.length() > 100) return value.length() + " chars";
        return value;
    }

    public Map<String, String> returnFields() {
        return fields;
    }
}

Parse Compilation Unit:
/*
 * Licensed to DigitalPebble Ltd under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.digitalpebble.stormcrawler.indexer;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.indexing.AbstractIndexerBolt;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.TreeSet;
import org.junit.Assert;
import org.junit.Before;
import org.junit.Test;

public class BasicIndexingTest extends IndexerTester {

    private static final String URL = "http://www.digitalpebble.com";

    @Before
    public void setupIndexerBolt() {
        bolt = new DummyIndexer();
        setupIndexerBolt(bolt);
    }

    @Test
    public void testEmptyCanonicalURL() throws Exception {
        Map config = new HashMap();
        config.put(AbstractIndexerBolt.urlFieldParamName, "url");
        config.put(AbstractIndexerBolt.canonicalMetadataParamName, "canonical");

        prepareIndexerBolt(config);

        index(URL, new Metadata());
        Map<String, String> fields = ((DummyIndexer) bolt).returnFields();

        Assert.assertEquals(
                "The URL should be used if no canonical URL is found", URL, fields.get("url"));
    }

    @Test
    public void testCanonicalURL() throws Exception {
        Map config = new HashMap();
        config.put(AbstractIndexerBolt.urlFieldParamName, "url");
        config.put(AbstractIndexerBolt.canonicalMetadataParamName, "canonical");

        Metadata metadata = new Metadata();
        metadata.setValue("canonical", "http://www.digitalpebble.com/");

        prepareIndexerBolt(config);

        index(URL, metadata);
        Map<String, String> fields = ((DummyIndexer) bolt).returnFields();

        Assert.assertEquals(
                "Use the canonical URL if found",
                "http://www.digitalpebble.com/",
                fields.get("url"));
    }

    @Test
    public void testRelativeCanonicalURL() throws Exception {
        Map config = new HashMap();
        config.put(AbstractIndexerBolt.urlFieldParamName, "url");
        config.put(AbstractIndexerBolt.canonicalMetadataParamName, "canonical");

        Metadata metadata = new Metadata();
        metadata.setValue("canonical", "/home");

        prepareIndexerBolt(config);

        index(URL, metadata);
        Map<String, String> fields = ((DummyIndexer) bolt).returnFields();

        Assert.assertEquals(
                "Use the canonical URL if found",
                "http://www.digitalpebble.com/home",
                fields.get("url"));
    }

    @Test
    public void testBadCanonicalURL() throws Exception {
        Map config = new HashMap();
        config.put(AbstractIndexerBolt.urlFieldParamName, "url");
        config.put(AbstractIndexerBolt.canonicalMetadataParamName, "canonical");

        Metadata metadata = new Metadata();
        metadata.setValue("canonical", "htp://www.digitalpebble.com/");

        prepareIndexerBolt(config);

        index(URL, metadata);
        Map<String, String> fields = ((DummyIndexer) bolt).returnFields();

        Assert.assertEquals(
                "Use the default URL if a bad canonical URL is found",
                "http://www.digitalpebble.com",
                fields.get("url"));
    }

    @Test
    public void testOtherHostCanonicalURL() throws Exception {
        Map config = new HashMap();
        config.put(AbstractIndexerBolt.urlFieldParamName, "url");
        config.put(AbstractIndexerBolt.canonicalMetadataParamName, "canonical");

        Metadata metadata = new Metadata();
        metadata.setValue("canonical", "http://www.google.com/");

        prepareIndexerBolt(config);

        index(URL, metadata);
        Map<String, String> fields = ((DummyIndexer) bolt).returnFields();

        Assert.assertEquals(
                "Ignore if the canonical URL references other host",
                "http://www.digitalpebble.com",
                fields.get("url"));
    }

    @Test
    public void testMissingCanonicalParamConfiguration() throws Exception {
        Map config = new HashMap();
        config.put(AbstractIndexerBolt.urlFieldParamName, "url");

        Metadata metadata = new Metadata();
        metadata.setValue("canonical", "http://www.digitalpebble.com/");

        prepareIndexerBolt(config);

        index(URL, metadata);
        Map<String, String> fields = ((DummyIndexer) bolt).returnFields();

        Assert.assertEquals(
                "Use the canonical URL if found",
                "http://www.digitalpebble.com",
                fields.get("url"));
    }

    @Test
    public void testFilterDocumentWithMetadata() throws Exception {
        Map config = new HashMap();
        config.put(AbstractIndexerBolt.urlFieldParamName, "url");
        config.put(AbstractIndexerBolt.metadataFilterParamName, "key1=value1");

        Metadata metadata = new Metadata();
        metadata.setValue("key1", "value1");

        prepareIndexerBolt(config);

        index(URL, metadata);
        Map<String, String> fields = ((DummyIndexer) bolt).returnFields();

        Assert.assertEquals(
                "The document must pass if the key/value is found in the metadata",
                "http://www.digitalpebble.com",
                fields.get("url"));
    }

    @Test
    public void testFilterDocumentWithoutMetadata() throws Exception {
        Map config = new HashMap();
        config.put(AbstractIndexerBolt.urlFieldParamName, "url");
        config.put(AbstractIndexerBolt.metadataFilterParamName, "key1=value1");

        prepareIndexerBolt(config);

        index(URL, new Metadata());
        Map<String, String> fields = ((DummyIndexer) bolt).returnFields();

        Assert.assertEquals(
                "The document must not pass if the key/value is not found in the metadata",
                0,
                fields.size());
    }

    @Test
    public void testFilterMetadata() throws Exception {
        Map config = new HashMap();
        config.put(AbstractIndexerBolt.urlFieldParamName, "url");

        final List vector = new ArrayList();
        vector.add("parse.title=title");
        vector.add("parse.keywords=keywords");

        config.put(AbstractIndexerBolt.metadata2fieldParamName, vector);

        prepareIndexerBolt(config);

        Metadata metadata = new Metadata();
        metadata.setValue("parse.title", "This is the title");
        metadata.setValue("parse.keywords", "keyword1, keyword2, keyword3");
        metadata.setValue("parse.description", "This is the description");

        index(URL, metadata);
        Map<String, String> fields = ((DummyIndexer) bolt).returnFields();

        Assert.assertArrayEquals(
                "Only the mapped metadata attributes should be indexed",
                new String[] {"keywords", "title", "url"},
                new TreeSet<>(fields.keySet()).toArray());
    }

    @Test
    public void testEmptyFilterMetadata() throws Exception {
        Map config = new HashMap();
        config.put(AbstractIndexerBolt.urlFieldParamName, "url");

        prepareIndexerBolt(config);

        Metadata metadata = new Metadata();
        metadata.setValue("parse.title", "This is the title");
        metadata.setValue("parse.keywords", "keyword1, keyword2, keyword3");
        metadata.setValue("parse.description", "This is the description");

        index(URL, metadata);
        Map<String, String> fields = ((DummyIndexer) bolt).returnFields();

        Assert.assertArrayEquals(
                "Index only the URL if no mapping is provided",
                new String[] {"url"},
                fields.keySet().toArray());
    }
}

Parse Compilation Unit:
/*
 * Licensed to DigitalPebble Ltd under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.digitalpebble.stormcrawler.indexer;

import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.TestOutputCollector;
import com.digitalpebble.stormcrawler.TestUtil;
import com.digitalpebble.stormcrawler.indexing.AbstractIndexerBolt;
import java.io.IOException;
import java.util.Map;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.tuple.Tuple;
import org.junit.After;

public class IndexerTester {
    AbstractIndexerBolt bolt;
    protected TestOutputCollector output;

    protected void setupIndexerBolt(AbstractIndexerBolt bolt) {
        this.bolt = bolt;
        output = new TestOutputCollector();
    }

    @After
    public void cleanupBolt() {
        if (bolt != null) {
            bolt.cleanup();
        }
        output = null;
    }

    protected void prepareIndexerBolt(Map config) {
        bolt.prepare(config, TestUtil.getMockedTopologyContext(), new OutputCollector(output));
    }

    protected void index(String url, Metadata metadata) {
        Tuple tuple = mock(Tuple.class);
        when(tuple.getStringByField("url")).thenReturn(url);
        when(tuple.getValueByField("metadata")).thenReturn(metadata);
        bolt.execute(tuple);
    }

    protected void index(String url, String content, Metadata metadata) throws IOException {
        Tuple tuple = mock(Tuple.class);
        when(tuple.getBinaryByField("content")).thenReturn(content.getBytes());
        when(tuple.getStringByField("url")).thenReturn(url);
        when(tuple.getValueByField("metadata")).thenReturn(metadata);
        bolt.execute(tuple);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.util;

import com.digitalpebble.stormcrawler.Metadata;
import java.net.MalformedURLException;
import org.junit.Assert;
import org.junit.Test;

public class RobotsTagsTest {
    @Test
    public void testHTTPHeaders() throws MalformedURLException {
        Metadata md = new Metadata();
        RobotsTags tags = new RobotsTags(md, "");
        Assert.assertEquals(false, tags.isNoCache());
        Assert.assertEquals(false, tags.isNoFollow());
        Assert.assertEquals(false, tags.isNoIndex());

        md = new Metadata();
        md.setValue("X-Robots-Tag", "none");
        tags = new RobotsTags(md, "");
        Assert.assertEquals(true, tags.isNoCache());
        Assert.assertEquals(true, tags.isNoFollow());
        Assert.assertEquals(true, tags.isNoIndex());

        md = new Metadata();
        md.setValues("X-Robots-Tag", new String[] {"noindex", "nofollow"});
        tags = new RobotsTags(md, "");
        Assert.assertEquals(false, tags.isNoCache());
        Assert.assertEquals(true, tags.isNoFollow());
        Assert.assertEquals(true, tags.isNoIndex());

        // expect the content to be incorrect
        md = new Metadata();
        md.setValue("X-Robots-Tag", "noindex, nofollow");
        tags = new RobotsTags(md, "");
        Assert.assertEquals(false, tags.isNoCache());
        Assert.assertEquals(true, tags.isNoFollow());
        Assert.assertEquals(true, tags.isNoIndex());
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.util;

import com.digitalpebble.stormcrawler.Metadata;
import java.net.MalformedURLException;
import java.util.HashMap;
import java.util.Map;
import org.junit.Assert;
import org.junit.Test;

public class MetadataTransferTest {
    @Test
    public void testTransfer() throws MalformedURLException {
        Map<String, Object> conf = new HashMap<>();
        conf.put(MetadataTransfer.trackDepthParamName, true);
        MetadataTransfer mdt = MetadataTransfer.getInstance(conf);
        Metadata parentMD = new Metadata();
        Metadata outlinkMD =
                mdt.getMetaForOutlink(
                        "http://www.example.com/outlink.html", "http://www.example.com", parentMD);
        // test the value of track seed and depth
        Assert.assertEquals("1", outlinkMD.getFirstValue(MetadataTransfer.depthKeyName));
        String[] urlpath = outlinkMD.getValues(MetadataTransfer.urlPathKeyName);
        Assert.assertEquals(1, urlpath.length);
    }

    @Test
    public void testCustomTransferClass() throws MalformedURLException {
        Map<String, Object> conf = new HashMap<>();
        conf.put(MetadataTransfer.metadataTransferClassParamName, "thisclassnameWillNEVERexist");
        boolean hasThrownException = false;
        try {
            MetadataTransfer.getInstance(conf);
        } catch (Exception e) {
            hasThrownException = true;
        }
        Assert.assertEquals(true, hasThrownException);

        conf = new HashMap<>();
        conf.put(
                MetadataTransfer.metadataTransferClassParamName,
                myCustomTransferClass.class.getName());
        hasThrownException = false;
        try {
            MetadataTransfer.getInstance(conf);
        } catch (Exception e) {
            hasThrownException = true;
        }
        Assert.assertEquals(false, hasThrownException);
    }
}

class myCustomTransferClass extends MetadataTransfer {}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.util;

import java.net.MalformedURLException;
import java.net.URL;
import java.util.List;
import org.apache.http.cookie.Cookie;
import org.junit.Assert;
import org.junit.Test;

public class CookieConverterTest {

    private static String securedUrl = "https://someurl.com";
    private static String unsecuredUrl = "http://someurl.com";
    private static String dummyCookieHeader = "nice tasty test cookie header!";
    private static String dummyCookieValue = "nice tasty test cookie value!";

    @Test
    public void testSimpleCookieAndUrl() {
        String[] cookiesStrings = new String[1];
        String dummyCookieString =
                buildCookieString(dummyCookieHeader, dummyCookieValue, null, null, null, null);
        cookiesStrings[0] = dummyCookieString;
        List<Cookie> result = CookieConverter.getCookies(cookiesStrings, getUrl(unsecuredUrl));
        Assert.assertEquals("Should have 1 cookie", 1, result.size());
        Assert.assertEquals(
                "Cookie header should be as defined", dummyCookieHeader, result.get(0).getName());
        Assert.assertEquals(
                "Cookie value should be as defined", dummyCookieValue, result.get(0).getValue());
    }

    @Test
    public void testNotExpiredCookie() {
        String[] cookiesStrings = new String[1];
        String dummyCookieString =
                buildCookieString(
                        dummyCookieHeader,
                        dummyCookieValue,
                        null,
                        "Tue, 11 Apr 2117 07:13:39 -0000",
                        null,
                        null);
        cookiesStrings[0] = dummyCookieString;
        List<Cookie> result = CookieConverter.getCookies(cookiesStrings, getUrl(unsecuredUrl));
        Assert.assertEquals("Should have 1 cookie", 1, result.size());
        Assert.assertEquals(
                "Cookie header should be as defined", dummyCookieHeader, result.get(0).getName());
        Assert.assertEquals(
                "Cookie value should be as defined", dummyCookieValue, result.get(0).getValue());
    }

    @Test
    public void testExpiredCookie() {
        String[] cookiesStrings = new String[1];
        String dummyCookieString =
                buildCookieString(
                        dummyCookieHeader,
                        dummyCookieValue,
                        null,
                        "Tue, 11 Apr 2016 07:13:39 -0000",
                        null,
                        null);
        cookiesStrings[0] = dummyCookieString;
        List<Cookie> result = CookieConverter.getCookies(cookiesStrings, getUrl(unsecuredUrl));
        Assert.assertEquals("Should have 0 cookies, since cookie was expired", 0, result.size());
    }

    @Test
    public void testValidPath() {
        String[] cookiesStrings = new String[1];
        String dummyCookieString =
                buildCookieString(dummyCookieHeader, dummyCookieValue, null, null, "/", null);
        cookiesStrings[0] = dummyCookieString;
        List<Cookie> result =
                CookieConverter.getCookies(cookiesStrings, getUrl(unsecuredUrl + "/somepage"));
        Assert.assertEquals("Should have 1 cookie", 1, result.size());
        Assert.assertEquals(
                "Cookie header should be as defined", dummyCookieHeader, result.get(0).getName());
        Assert.assertEquals(
                "Cookie value should be as defined", dummyCookieValue, result.get(0).getValue());
    }

    @Test
    public void testValidPath2() {
        String[] cookiesStrings = new String[1];
        String dummyCookieString =
                buildCookieString(dummyCookieHeader, dummyCookieValue, null, null, "/", null);
        cookiesStrings[0] = dummyCookieString;
        List<Cookie> result = CookieConverter.getCookies(cookiesStrings, getUrl(unsecuredUrl));
        Assert.assertEquals("Should have 1 cookie", 1, result.size());
        Assert.assertEquals(
                "Cookie header should be as defined", dummyCookieHeader, result.get(0).getName());
        Assert.assertEquals(
                "Cookie value should be as defined", dummyCookieValue, result.get(0).getValue());
    }

    @Test
    public void testValidPath3() {
        String[] cookiesStrings = new String[1];
        String dummyCookieString =
                buildCookieString(
                        dummyCookieHeader, dummyCookieValue, null, null, "/someFolder", null);
        cookiesStrings[0] = dummyCookieString;
        List<Cookie> result =
                CookieConverter.getCookies(cookiesStrings, getUrl(unsecuredUrl + "/someFolder"));
        Assert.assertEquals("Should have 1 cookie", 1, result.size());
        Assert.assertEquals(
                "Cookie header should be as defined", dummyCookieHeader, result.get(0).getName());
        Assert.assertEquals(
                "Cookie value should be as defined", dummyCookieValue, result.get(0).getValue());
    }

    @Test
    public void testValidPath4() {
        String[] cookiesStrings = new String[1];
        String dummyCookieString =
                buildCookieString(
                        dummyCookieHeader, dummyCookieValue, null, null, "/someFolder", null);
        cookiesStrings[0] = dummyCookieString;
        List<Cookie> result =
                CookieConverter.getCookies(
                        cookiesStrings, getUrl(unsecuredUrl + "/someFolder/SomeOtherFolder"));
        Assert.assertEquals("Should have 1 cookie", 1, result.size());
        Assert.assertEquals(
                "Cookie header should be as defined", dummyCookieHeader, result.get(0).getName());
        Assert.assertEquals(
                "Cookie value should be as defined", dummyCookieValue, result.get(0).getValue());
    }

    @Test
    public void testInvalidPath() {
        String[] cookiesStrings = new String[1];
        String dummyCookieString =
                buildCookieString(
                        dummyCookieHeader, dummyCookieValue, null, null, "/someFolder", null);
        cookiesStrings[0] = dummyCookieString;
        List<Cookie> result =
                CookieConverter.getCookies(
                        cookiesStrings, getUrl(unsecuredUrl + "/someOtherFolder/SomeFolder"));
        Assert.assertEquals("path mismatch, should have 0 cookies", 0, result.size());
    }

    @Test
    public void testValidDomain() {
        String[] cookiesStrings = new String[1];
        String dummyCookieString =
                buildCookieString(
                        dummyCookieHeader, dummyCookieValue, "someurl.com", null, null, null);
        cookiesStrings[0] = dummyCookieString;
        List<Cookie> result =
                CookieConverter.getCookies(
                        cookiesStrings, getUrl(unsecuredUrl + "/someFolder/SomeOtherFolder"));
        Assert.assertEquals("Should have 1 cookie", 1, result.size());
        Assert.assertEquals(
                "Cookie header should be as defined", dummyCookieHeader, result.get(0).getName());
        Assert.assertEquals(
                "Cookie value should be as defined", dummyCookieValue, result.get(0).getValue());
    }

    @Test
    public void testInvalidDomain() {
        String[] cookiesStrings = new String[1];
        String dummyCookieString =
                buildCookieString(
                        dummyCookieHeader, dummyCookieValue, "someOtherUrl.com", null, null, null);
        cookiesStrings[0] = dummyCookieString;
        List<Cookie> result =
                CookieConverter.getCookies(
                        cookiesStrings, getUrl(unsecuredUrl + "/someFolder/SomeOtherFolder"));
        Assert.assertEquals("Domain is not valid - Should have 0 cookies", 0, result.size());
    }

    @Test
    public void testSecurFlagHttp() {
        String[] cookiesStrings = new String[1];
        String dummyCookieString =
                buildCookieString(
                        dummyCookieHeader, dummyCookieValue, null, null, null, Boolean.TRUE);
        cookiesStrings[0] = dummyCookieString;
        List<Cookie> result =
                CookieConverter.getCookies(
                        cookiesStrings, getUrl(unsecuredUrl + "/someFolder/SomeOtherFolder"));
        Assert.assertEquals("Target url is not secured - Should have 0 cookies", 0, result.size());
    }

    @Test
    public void testSecurFlagHttpS() {
        String[] cookiesStrings = new String[1];
        String dummyCookieString =
                buildCookieString(
                        dummyCookieHeader, dummyCookieValue, null, null, null, Boolean.TRUE);
        cookiesStrings[0] = dummyCookieString;
        List<Cookie> result =
                CookieConverter.getCookies(
                        cookiesStrings, getUrl(securedUrl + "/someFolder/SomeOtherFolder"));
        Assert.assertEquals("Target url is  secured - Should have 1 cookie", 1, result.size());
        Assert.assertEquals(
                "Cookie header should be as defined", dummyCookieHeader, result.get(0).getName());
        Assert.assertEquals(
                "Cookie value should be as defined", dummyCookieValue, result.get(0).getValue());
    }

    @Test
    public void testFullCookie() {
        String[] cookiesStrings = new String[1];
        String dummyCookieString =
                buildCookieString(
                        dummyCookieHeader,
                        dummyCookieValue,
                        "someurl.com",
                        "Tue, 11 Apr 2117 07:13:39 -0000",
                        "/",
                        true);
        cookiesStrings[0] = dummyCookieString;
        List<Cookie> result =
                CookieConverter.getCookies(
                        cookiesStrings, getUrl(securedUrl + "/someFolder/SomeOtherFolder"));
        Assert.assertEquals("Should have 1 cookie", 1, result.size());
        Assert.assertEquals(
                "Cookie header should be as defined", dummyCookieHeader, result.get(0).getName());
        Assert.assertEquals(
                "Cookie value should be as defined", dummyCookieValue, result.get(0).getValue());
    }

    @Test
    public void test2Cookies() {
        String[] cookiesStrings = new String[2];
        String dummyCookieString =
                buildCookieString(
                        dummyCookieHeader,
                        dummyCookieValue,
                        "someurl.com",
                        "Tue, 11 Apr 2117 07:13:39 -0000",
                        "/",
                        true);
        String dummyCookieString2 =
                buildCookieString(
                        dummyCookieHeader + "2",
                        dummyCookieValue + "2",
                        "someurl.com",
                        "Tue, 11 Apr 2117 07:13:39 -0000",
                        "/",
                        true);
        cookiesStrings[0] = dummyCookieString;
        cookiesStrings[1] = dummyCookieString2;
        List<Cookie> result =
                CookieConverter.getCookies(
                        cookiesStrings, getUrl(securedUrl + "/someFolder/SomeOtherFolder"));
        Assert.assertEquals("Should have 2 cookies", 2, result.size());
        Assert.assertEquals(
                "Cookie header should be as defined", dummyCookieHeader, result.get(0).getName());
        Assert.assertEquals(
                "Cookie value should be as defined", dummyCookieValue, result.get(0).getValue());

        Assert.assertEquals(
                "Cookie header should be as defined",
                dummyCookieHeader + "2",
                result.get(1).getName());
        Assert.assertEquals(
                "Cookie value should be as defined",
                dummyCookieValue + "2",
                result.get(1).getValue());
    }

    @Test
    public void testDomainsChecker() {
        boolean result = CookieConverter.checkDomainMatchToUrl(".example.com", "www.example.com");
        Assert.assertEquals("domain is valid", true, result);
    }

    @Test
    public void testDomainsChecker2() {
        boolean result = CookieConverter.checkDomainMatchToUrl(".example.com", "example.com");
        Assert.assertEquals("domain is valid", true, result);
    }

    @Test
    public void testDomainsChecker3() {
        boolean result = CookieConverter.checkDomainMatchToUrl("example.com", "www.example.com");
        Assert.assertEquals("domain is valid", true, result);
    }

    @Test
    public void testDomainsChecker4() {
        boolean result = CookieConverter.checkDomainMatchToUrl("example.com", "anotherexample.com");
        Assert.assertEquals("domain is not valid", false, result);
    }

    private URL getUrl(String urlString) {
        try {
            return new URL(urlString);
        } catch (MalformedURLException e) {
            return null;
        }
    }

    private String buildCookieString(
            String header,
            String value,
            String domain,
            String expires,
            String path,
            Boolean secure) {
        StringBuilder builder = new StringBuilder(buildCookiePart(header, value));
        if (domain != null) {
            builder.append(buildCookiePart("domain", domain));
        }

        if (expires != null) {
            builder.append(buildCookiePart("expires", expires));
        }

        if (path != null) {
            builder.append(buildCookiePart("path", path));
        }

        if (secure != null) {
            builder.append("secure;");
        }

        return builder.toString();
    }

    private String buildCookiePart(String partName, String partValue) {
        return partName + "=" + partValue + ";";
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.util;

import com.digitalpebble.stormcrawler.parse.DocumentFragmentBuilder;
import java.io.IOException;
import java.net.MalformedURLException;
import org.jsoup.Jsoup;
import org.jsoup.nodes.Document;
import org.junit.Assert;
import org.junit.Test;
import org.w3c.dom.DocumentFragment;

public class RefreshTagTest {

    @Test
    public void testExtractRefreshURL() throws MalformedURLException, IOException {
        String expected = "http://www.example.com/";

        String[] htmlStrings =
                new String[] {
                    "<html><head><META http-equiv=\"refresh\" content=\"0; URL=http://www.example.com/\"></head><body>Lorem ipsum.</body></html>",
                    "<html><head><META http-equiv=\"refresh\" content=\"0;URL=http://www.example.com/\"></head><body>Lorem ipsum.</body></html>",
                };

        for (String htmlString : htmlStrings) {
            Document doc = Jsoup.parseBodyFragment(htmlString);
            DocumentFragment fragment = DocumentFragmentBuilder.fromJsoup(doc);
            String redirection = RefreshTag.extractRefreshURL(fragment);
            Assert.assertEquals(expected, redirection);
        }
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.protocol;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.protocol.DelegatorProtocol.FilteredProtocol;
import com.digitalpebble.stormcrawler.util.ConfUtils;
import java.io.FileNotFoundException;
import org.apache.storm.Config;
import org.junit.Assert;
import org.junit.Test;

public class DelegationProtocolTest {

    private static final String OKHTTP =
            "com.digitalpebble.stormcrawler.protocol.okhttp.HttpProtocol";
    private static final String APACHE =
            "com.digitalpebble.stormcrawler.protocol.httpclient.HttpProtocol";

    @Test
    public void getProtocolTest() throws FileNotFoundException {

        Config conf = new Config();

        ConfUtils.loadConf("src/test/resources/delegator-conf.yaml", conf);

        conf.put("http.agent.name", "this.is.only.a.test");

        DelegatorProtocol superProto = new DelegatorProtocol();
        superProto.configure(conf);

        // try single filter
        // TODO use a protocol which doesnt require an actual connection when
        // configured
        Metadata meta = new Metadata();
        meta.setValue("js", "true");

        FilteredProtocol pf = superProto.getProtocolFor("https://digitalpebble.com", meta);

        Assert.assertEquals(pf.getProtocolInstance().getClass().getName(), OKHTTP);

        // no filter at all
        meta = new Metadata();
        pf = superProto.getProtocolFor("https://www.example.com/robots.txt", meta);

        Assert.assertEquals(pf.getProtocolInstance().getClass().getName(), OKHTTP);

        // should match the last instance
        // as the one above has more than one filter
        meta = new Metadata();
        meta.setValue("domain", "example.com");

        pf = superProto.getProtocolFor("https://example.com", meta);

        Assert.assertEquals(pf.getProtocolInstance().getClass().getName(), OKHTTP);

        // everything should match
        meta = new Metadata();
        meta.setValue("test", "true");
        meta.setValue("depth", "3");
        meta.setValue("domain", "example.com");

        pf = superProto.getProtocolFor("https://www.example-two.com", meta);

        Assert.assertEquals(pf.getProtocolInstance().getClass().getName(), APACHE);

        // should not match
        meta = new Metadata();
        meta.setValue("test", "false");
        meta.setValue("depth", "3");
        meta.setValue("domain", "example.com");

        pf = superProto.getProtocolFor("https://www.example-two.com", meta);

        Assert.assertEquals(pf.getProtocolInstance().getClass().getName(), OKHTTP);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.protocol;

import org.junit.Assert;
import org.junit.Test;

public class HttpHeadersTest {

    @Test
    public void testHttpDate() {
        String[][] dates = { //
            {"Tue, 22 Sep 2020 08:00:00 GMT", "2020-09-22T08:00:00.000Z"}, //
            {"Sun, 06 Nov 1994 08:49:37 GMT", "1994-11-06T08:49:37.000Z"}, //
            {"Sun, 06 Nov 1994 20:49:37 GMT", "1994-11-06T20:49:37.000Z"}, //
        };
        for (int i = 0; i < dates.length; i++) {
            Assert.assertEquals(dates[i][0], HttpHeaders.formatHttpDate(dates[i][1]));
        }
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.filtering;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.filtering.basic.BasicURLFilter;
import com.fasterxml.jackson.databind.node.JsonNodeFactory;
import com.fasterxml.jackson.databind.node.ObjectNode;
import java.net.MalformedURLException;
import java.net.URL;
import java.util.HashMap;
import java.util.Map;
import org.junit.Assert;
import org.junit.Test;

public class BasicURLFilterTest {

    private URLFilter createFilter(int length, int repet) {
        BasicURLFilter filter = new BasicURLFilter();
        ObjectNode filterParams = new ObjectNode(JsonNodeFactory.instance);
        filterParams.put("maxPathRepetition", repet);
        filterParams.put("maxLength", length);
        Map<String, Object> conf = new HashMap<>();
        filter.configure(conf, filterParams);
        return filter;
    }

    @Test
    public void testRepetition() throws MalformedURLException {
        URLFilter filter = createFilter(-1, 3);
        Metadata metadata = new Metadata();

        URL targetURL = new URL("http://www.sourcedomain.com/a/a/a/index.html");
        String filterResult = filter.filter(targetURL, metadata, targetURL.toExternalForm());
        Assert.assertEquals(null, filterResult);

        targetURL = new URL("http://www.sourcedomain.com/a/b/a/index.html");
        filterResult = filter.filter(targetURL, metadata, targetURL.toExternalForm());
        Assert.assertEquals(targetURL.toExternalForm(), filterResult);
    }

    @Test
    public void testLength() throws MalformedURLException {
        URLFilter filter = createFilter(32, -1);
        Metadata metadata = new Metadata();

        URL targetURL = new URL("http://www.sourcedomain.com/a/a/a/index.html");
        String filterResult = filter.filter(targetURL, metadata, targetURL.toExternalForm());
        Assert.assertEquals(null, filterResult);

        targetURL = new URL("http://www.sourcedomain.com/");
        filterResult = filter.filter(targetURL, metadata, targetURL.toExternalForm());
        Assert.assertEquals(targetURL.toExternalForm(), filterResult);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.filtering;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.filtering.metadata.MetadataFilter;
import com.fasterxml.jackson.databind.node.JsonNodeFactory;
import com.fasterxml.jackson.databind.node.ObjectNode;
import java.net.MalformedURLException;
import java.net.URL;
import java.util.HashMap;
import java.util.Map;
import org.junit.Assert;
import org.junit.Test;

public class MetadataFilterTest {

    private URLFilter createFilter(String key, String value) {
        MetadataFilter filter = new MetadataFilter();
        ObjectNode filterParams = new ObjectNode(JsonNodeFactory.instance);
        filterParams.put(key, value);
        Map<String, Object> conf = new HashMap<>();
        filter.configure(conf, filterParams);
        return filter;
    }

    @Test
    public void testFilterNoMD() throws MalformedURLException {
        URLFilter filter = createFilter("key", "val");
        URL url = new URL("http://www.sourcedomain.com/");
        Metadata metadata = new Metadata();
        String filterResult = filter.filter(url, metadata, url.toExternalForm());
        Assert.assertEquals(url.toExternalForm(), filterResult);
    }

    @Test
    public void testFilterHit() throws MalformedURLException {
        URLFilter filter = createFilter("key", "val");
        URL url = new URL("http://www.sourcedomain.com/");
        Metadata metadata = new Metadata();
        metadata.addValue("key", "val");
        String filterResult = filter.filter(url, metadata, url.toExternalForm());
        Assert.assertEquals(null, filterResult);
    }

    @Test
    public void testFilterNoHit() throws MalformedURLException {
        URLFilter filter = createFilter("key", "val");
        URL url = new URL("http://www.sourcedomain.com/");
        Metadata metadata = new Metadata();
        metadata.addValue("key", "val2");
        metadata.addValue("key", "val3");
        String filterResult = filter.filter(url, metadata, url.toExternalForm());
        Assert.assertEquals(url.toExternalForm(), filterResult);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.filtering;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.filtering.depth.MaxDepthFilter;
import com.digitalpebble.stormcrawler.util.MetadataTransfer;
import com.fasterxml.jackson.databind.node.JsonNodeFactory;
import com.fasterxml.jackson.databind.node.ObjectNode;
import java.net.MalformedURLException;
import java.net.URL;
import java.util.HashMap;
import java.util.Map;
import org.junit.Assert;
import org.junit.Test;

public class MaxDepthFilterTest {

    private URLFilter createFilter(String key, int value) {
        MaxDepthFilter filter = new MaxDepthFilter();
        ObjectNode filterParams = new ObjectNode(JsonNodeFactory.instance);
        filterParams.put(key, value);
        Map<String, Object> conf = new HashMap<>();
        filter.configure(conf, filterParams);
        return filter;
    }

    @Test
    public void testDepthZero() throws MalformedURLException {
        URLFilter filter = createFilter("maxDepth", 0);
        URL url = new URL("http://www.sourcedomain.com/");
        Metadata metadata = new Metadata();
        String filterResult = filter.filter(url, metadata, url.toExternalForm());
        Assert.assertEquals(null, filterResult);
    }

    @Test
    public void testDepth() throws MalformedURLException {
        URLFilter filter = createFilter("maxDepth", 2);
        URL url = new URL("http://www.sourcedomain.com/");
        Metadata metadata = new Metadata();
        metadata.setValue(MetadataTransfer.depthKeyName, "2");
        String filterResult = filter.filter(url, metadata, url.toExternalForm());
        Assert.assertEquals(null, filterResult);
    }

    @Test
    public void testCustomDepthZero() throws MalformedURLException {
        URLFilter filter = createFilter("maxDepth", 3);
        URL url = new URL("http://www.sourcedomain.com/");
        Metadata metadata = new Metadata();
        metadata.setValue(MetadataTransfer.maxDepthKeyName, "0");
        String filterResult = filter.filter(url, metadata, url.toExternalForm());
        Assert.assertEquals(null, filterResult);
    }

    @Test
    public void testCustomDepth() throws MalformedURLException {
        URLFilter filter = createFilter("maxDepth", 1);
        URL url = new URL("http://www.sourcedomain.com/");
        Metadata metadata = new Metadata();
        metadata.setValue(MetadataTransfer.maxDepthKeyName, "2");
        metadata.setValue(MetadataTransfer.depthKeyName, "1");
        String filterResult = filter.filter(url, metadata, url.toExternalForm());
        Assert.assertEquals(url.toExternalForm(), filterResult);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.filtering;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.filtering.host.HostURLFilter;
import com.fasterxml.jackson.databind.node.JsonNodeFactory;
import com.fasterxml.jackson.databind.node.ObjectNode;
import java.net.MalformedURLException;
import java.net.URL;
import java.util.HashMap;
import java.util.Map;
import org.junit.Assert;
import org.junit.Test;

/**
 * Utility class which encapsulates the filtering of URLs based on the hostname or domain of the
 * source URL.
 */
public class HostURLFilterTest {

    private HostURLFilter createFilter(boolean ignoreOutsideHost, boolean ignoreOutsideDomain) {
        HostURLFilter filter = new HostURLFilter();
        ObjectNode filterParams = new ObjectNode(JsonNodeFactory.instance);
        filterParams.put("ignoreOutsideHost", Boolean.valueOf(ignoreOutsideHost));
        filterParams.put("ignoreOutsideDomain", Boolean.valueOf(ignoreOutsideDomain));
        Map<String, Object> conf = new HashMap<>();
        filter.configure(conf, filterParams);
        return filter;
    }

    @Test
    public void testAllAllowed() throws MalformedURLException {
        HostURLFilter allAllowed = createFilter(false, false);
        URL sourceURL = new URL("http://www.sourcedomain.com/index.html");
        Metadata metadata = new Metadata();
        String filterResult =
                allAllowed.filter(sourceURL, metadata, "http://www.sourcedomain.com/index.html");
        Assert.assertEquals("http://www.sourcedomain.com/index.html", filterResult);
        filterResult =
                allAllowed.filter(sourceURL, metadata, "http://www.anotherDomain.com/index.html");
        Assert.assertEquals("http://www.anotherDomain.com/index.html", filterResult);
        filterResult =
                allAllowed.filter(sourceURL, metadata, "http://sub.sourcedomain.com/index.html");
        Assert.assertEquals("http://sub.sourcedomain.com/index.html", filterResult);
    }

    @Test
    public void testAllForbidden() throws MalformedURLException {
        HostURLFilter allAllowed = createFilter(true, true);
        URL sourceURL = new URL("http://www.sourcedomain.com/index.html");
        Metadata metadata = new Metadata();

        String filterResult =
                allAllowed.filter(sourceURL, metadata, "http://www.sourcedomain.com/index.html");
        Assert.assertEquals("http://www.sourcedomain.com/index.html", filterResult);
        filterResult =
                allAllowed.filter(sourceURL, metadata, "http://www.anotherDomain.com/index.html");
        Assert.assertNull(filterResult);
        filterResult =
                allAllowed.filter(sourceURL, metadata, "http://sub.sourcedomain.com/index.html");
        Assert.assertNull(filterResult);
    }

    @Test
    public void testWithinHostOnly() throws MalformedURLException {
        HostURLFilter allAllowed = createFilter(true, false);
        URL sourceURL = new URL("http://www.sourcedomain.com/index.html");
        Metadata metadata = new Metadata();

        String filterResult =
                allAllowed.filter(sourceURL, metadata, "http://www.sourcedomain.com/index.html");
        Assert.assertEquals("http://www.sourcedomain.com/index.html", filterResult);
        filterResult =
                allAllowed.filter(sourceURL, metadata, "http://www.anotherDomain.com/index.html");
        Assert.assertNull(filterResult);
        filterResult =
                allAllowed.filter(sourceURL, metadata, "http://sub.sourcedomain.com/index.html");
        Assert.assertNull(filterResult);
    }

    @Test
    public void testWithinDomain() throws MalformedURLException {
        HostURLFilter allAllowed = createFilter(false, true);
        URL sourceURL = new URL("http://www.sourcedomain.com/index.html");
        Metadata metadata = new Metadata();

        String filterResult =
                allAllowed.filter(sourceURL, metadata, "http://www.sourcedomain.com/index.html");
        Assert.assertEquals("http://www.sourcedomain.com/index.html", filterResult);
        filterResult =
                allAllowed.filter(sourceURL, metadata, "http://www.anotherDomain.com/index.html");
        Assert.assertNull(filterResult);
        filterResult =
                allAllowed.filter(sourceURL, metadata, "http://sub.sourcedomain.com/index.html");
        Assert.assertEquals("http://sub.sourcedomain.com/index.html", filterResult);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.filtering;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.filtering.regex.FastURLFilter;
import com.fasterxml.jackson.databind.node.JsonNodeFactory;
import com.fasterxml.jackson.databind.node.ObjectNode;
import java.net.MalformedURLException;
import java.net.URL;
import java.util.HashMap;
import java.util.Map;
import org.junit.Assert;
import org.junit.Test;

public class FastURLFilterTest {

    private URLFilter createFilter() {
        ObjectNode filterParams = new ObjectNode(JsonNodeFactory.instance);
        filterParams.put("files", "fast.urlfilter.json");
        FastURLFilter filter = new FastURLFilter();
        Map<String, Object> conf = new HashMap<>();
        filter.configure(conf, filterParams);
        return filter;
    }

    @Test
    public void testImagesFilter() throws MalformedURLException {
        URL url = new URL("http://www.somedomain.com/image.jpg");
        Metadata metadata = new Metadata();
        String filterResult = createFilter().filter(url, metadata, url.toExternalForm());
        Assert.assertEquals(null, filterResult);
    }

    @Test
    public void testDomainNotAllowed() throws MalformedURLException {
        URL url = new URL("http://stormcrawler.net/");
        Metadata metadata = new Metadata();
        String filterResult = createFilter().filter(url, metadata, url.toExternalForm());
        Assert.assertEquals(null, filterResult);

        // allowed
        url = new URL("http://stormcrawler.net/digitalpebble/");
        filterResult = createFilter().filter(url, metadata, url.toExternalForm());
        Assert.assertEquals(url.toString(), filterResult);
    }

    @Test
    public void testMD() throws MalformedURLException {
        URL url = new URL("http://somedomain.net/");
        Metadata metadata = new Metadata();
        metadata.addValue("key", "value");
        String filterResult = createFilter().filter(url, metadata, url.toExternalForm());
        Assert.assertEquals(null, filterResult);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.filtering;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.filtering.regex.RegexURLFilter;
import com.fasterxml.jackson.databind.node.JsonNodeFactory;
import com.fasterxml.jackson.databind.node.ObjectNode;
import java.net.MalformedURLException;
import java.net.URL;
import java.util.HashMap;
import java.util.Map;
import org.junit.Assert;
import org.junit.Test;

public class RegexFilterTest {

    private URLFilter createFilter() {
        ObjectNode filterParams = new ObjectNode(JsonNodeFactory.instance);
        filterParams.put("regexFilterFile", "default-regex-filters.txt");
        return createFilter(filterParams);
    }

    private URLFilter createFilter(ObjectNode filterParams) {
        RegexURLFilter filter = new RegexURLFilter();
        Map<String, Object> conf = new HashMap<>();
        filter.configure(conf, filterParams);
        return filter;
    }

    @Test
    public void testProtocolFilter() throws MalformedURLException {
        URLFilter allAllowed = createFilter();
        URL url = new URL("ftp://www.someFTP.com/#0");
        Metadata metadata = new Metadata();
        String filterResult = allAllowed.filter(url, metadata, url.toExternalForm());
        String expected = null;
        Assert.assertEquals(expected, filterResult);
    }

    @Test
    public void testImagesFilter() throws MalformedURLException {
        URLFilter allAllowed = createFilter();
        URL url = new URL("http://www.someFTP.com/bla.gif");
        Metadata metadata = new Metadata();
        String filterResult = allAllowed.filter(url, metadata, url.toExternalForm());
        Assert.assertEquals(null, filterResult);

        url = new URL("http://www.someFTP.com/bla.GIF");
        filterResult = allAllowed.filter(url, metadata, url.toExternalForm());
        Assert.assertEquals(null, filterResult);

        url = new URL("http://www.someFTP.com/bla.GIF&somearg=0");
        filterResult = allAllowed.filter(url, metadata, url.toExternalForm());
        Assert.assertEquals(null, filterResult);

        url = new URL("http://www.someFTP.com/bla.GIF?somearg=0");
        filterResult = allAllowed.filter(url, metadata, url.toExternalForm());
        Assert.assertEquals(null, filterResult);

        // not this one : the gif is within the path
        url = new URL("http://www.someFTP.com/bla.GIF.orNot");
        filterResult = allAllowed.filter(url, metadata, url.toExternalForm());
        Assert.assertEquals(url.toExternalForm(), filterResult);

        url = new URL("http://www.someFTP.com/bla.mp4");
        filterResult = allAllowed.filter(url, metadata, url.toExternalForm());
        Assert.assertEquals(null, filterResult);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.filtering;

import static org.junit.Assert.assertEquals;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.filtering.basic.BasicURLNormalizer;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.node.JsonNodeFactory;
import com.fasterxml.jackson.databind.node.ObjectNode;
import java.net.MalformedURLException;
import java.net.URL;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import org.junit.Assert;
import org.junit.Test;

/**
 * Utility class which encapsulates the filtering of URLs based on the hostname or domain of the
 * source URL.
 */
public class BasicURLNormalizerTest {

    List<String> queryParamsToFilter = Arrays.asList("a", "foo");

    private URLFilter createFilter(boolean removeAnchor, boolean checkValidURI) {
        ObjectNode filterParams = new ObjectNode(JsonNodeFactory.instance);
        filterParams.put("removeAnchorPart", Boolean.valueOf(removeAnchor));
        filterParams.put("checkValidURI", Boolean.valueOf(checkValidURI));
        return createFilter(filterParams);
    }

    private URLFilter createFilter(List<String> queryElementsToRemove) {
        ObjectNode filterParams = new ObjectNode(JsonNodeFactory.instance);
        filterParams.set("queryElementsToRemove", getArrayNode(queryElementsToRemove));
        return createFilter(filterParams);
    }

    private URLFilter createFilter(boolean removeAnchor, List<String> queryElementsToRemove) {
        ObjectNode filterParams = new ObjectNode(JsonNodeFactory.instance);
        filterParams.set("queryElementsToRemove", getArrayNode(queryElementsToRemove));
        filterParams.put("removeAnchorPart", Boolean.valueOf(removeAnchor));
        return createFilter(filterParams);
    }

    private URLFilter createFilter(
            boolean removeAnchor, boolean unmangleQueryString, List<String> queryElementsToRemove) {
        ObjectNode filterParams = new ObjectNode(JsonNodeFactory.instance);
        filterParams.set("queryElementsToRemove", getArrayNode(queryElementsToRemove));
        filterParams.put("removeAnchorPart", Boolean.valueOf(removeAnchor));
        filterParams.put("unmangleQueryString", Boolean.valueOf(unmangleQueryString));
        return createFilter(filterParams);
    }

    private URLFilter createFilter(ObjectNode filterParams) {
        BasicURLNormalizer filter = new BasicURLNormalizer();
        Map<String, Object> conf = new HashMap<>();
        filter.configure(conf, filterParams);
        return filter;
    }

    @Test
    public void testAnchorFilter() throws MalformedURLException {
        URLFilter allAllowed = createFilter(true, false);
        URL url = new URL("http://www.sourcedomain.com/#0");
        Metadata metadata = new Metadata();
        String filterResult = allAllowed.filter(url, metadata, url.toExternalForm());
        String expected = "http://www.sourcedomain.com/";
        Assert.assertEquals(expected, filterResult);
    }

    @Test
    public void testAnchorFilterFalse() throws MalformedURLException {
        URLFilter allAllowed = createFilter(false, false);
        URL url = new URL("http://www.sourcedomain.com/#0");
        Metadata metadata = new Metadata();
        String filterResult = allAllowed.filter(url, metadata, url.toExternalForm());
        Assert.assertEquals(url.toExternalForm(), filterResult);
    }

    @Test
    public void testRemoveSomeOfManyQueryParams() throws MalformedURLException {
        URLFilter urlFilter = createFilter(queryParamsToFilter);
        URL testSourceUrl = new URL("http://google.com");
        String testUrl = "http://google.com?keep1=true&a=c&foo=baz&keep2=true";
        String expectedResult = "http://google.com?keep1=true&keep2=true";
        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), testUrl);
        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);
    }

    @Test
    public void testRemoveAllQueryParams() throws MalformedURLException {
        URLFilter urlFilter = createFilter(queryParamsToFilter);
        URL testSourceUrl = new URL("http://google.com");
        String testUrl = "http://google.com?a=c&foo=baz";
        String expectedResult = "http://google.com";
        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), testUrl);
        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);
    }

    @Test
    public void testRemoveDupeQueryParams() throws MalformedURLException {
        URLFilter urlFilter = createFilter(queryParamsToFilter);
        URL testSourceUrl = new URL("http://google.com");
        String testUrl = "http://google.com?a=c&foo=baz&foo=bar&test=true";
        String expectedResult = "http://google.com?test=true";
        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), testUrl);
        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);
    }

    @Test
    public void testPipeInUrlAndFilterStillWorks() throws MalformedURLException {
        URLFilter urlFilter = createFilter(queryParamsToFilter);
        URL testSourceUrl = new URL("http://google.com");
        String testUrl = "http://google.com?a=c|d&foo=baz&foo=bar&test=true";
        String expectedResult = "http://google.com?test=true";
        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), testUrl);
        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);
    }

    @Test
    public void testBothAnchorAndQueryFilter() throws MalformedURLException {
        URLFilter urlFilter = createFilter(true, queryParamsToFilter);
        URL testSourceUrl = new URL("http://google.com");
        String testUrl = "http://google.com?a=c|d&foo=baz&foo=bar&test=true#fragment=ohYeah";
        String expectedResult = "http://google.com?test=true";
        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), testUrl);
        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);
    }

    @Test
    public void testQuerySort() throws MalformedURLException {
        URLFilter urlFilter = createFilter(queryParamsToFilter);
        URL testSourceUrl = new URL("http://google.com");
        String testUrl = "http://google.com?a=c|d&foo=baz&foo=bar&test=true&z=2&d=4";
        String expectedResult = "http://google.com?d=4&test=true&z=2";
        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), testUrl);
        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);
    }

    @Test
    public void testMangledQueryString() throws MalformedURLException {
        URLFilter urlFilter = createFilter(queryParamsToFilter);
        URL testSourceUrl = new URL("http://google.com");
        String testUrl = "http://google.com&d=4&good=true";
        String expectedResult = "http://google.com?d=4&good=true";
        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), testUrl);
        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);
    }

    @Test
    public void testHashes() throws MalformedURLException {
        ObjectNode filterParams = new ObjectNode(JsonNodeFactory.instance);
        filterParams.put("removeHashes", true);
        URLFilter urlFilter = createFilter(filterParams);

        URL testSourceUrl = new URL("http://florida-chemical.com");
        String in =
                "http://www.florida-chemical.com/Diacetone-Alcohol-DAA-99.html?xid_0b629=12854b827878df26423d933a5baf86d5";
        String out = "http://www.florida-chemical.com/Diacetone-Alcohol-DAA-99.html";

        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), in);
        assertEquals("Failed to filter query string", out, normalizedUrl);

        in =
                "http://www.maroongroupllc.com/maroon/login/auth;jsessionid=8DBFC2FEDBD740BBC8B4D1A504A6DE7F";
        out = "http://www.maroongroupllc.com/maroon/login/auth";
        normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), in);
        assertEquals("Failed to filter query string", out, normalizedUrl);
    }

    @Test
    public void testDontFixMangledQueryString() throws MalformedURLException {
        URLFilter urlFilter = createFilter(true, false, queryParamsToFilter);
        URL testSourceUrl = new URL("http://google.com");
        String testUrl = "http://google.com&d=4&good=true";
        String expectedResult = "http://google.com&d=4&good=true";
        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), testUrl);
        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);
    }

    @Test
    public void testProperURLEncodingWithoutQueryParameter() throws MalformedURLException {
        URLFilter urlFilter = createFilter(queryParamsToFilter);
        String urlWithEscapedCharacters =
                "http://www.dillards.com/product/ASICS-Womens-GT2000-3-LiteShow%E2%84%A2-Running-Shoes_301_-1_301_504736989";
        URL testSourceUrl = new URL(urlWithEscapedCharacters);
        String testUrl = urlWithEscapedCharacters;
        String expectedResult = urlWithEscapedCharacters;
        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), testUrl);
        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);
    }

    @Test
    public void testProperURLEncodingWithQueryParameters() throws MalformedURLException {
        URLFilter urlFilter = createFilter(queryParamsToFilter);
        String urlWithEscapedCharacters =
                "http://www.dillards.com/product/ASICS-Womens-GT2000-3-LiteShow%E2%84%A2-Running-Shoes_301_-1_301_504736989?how=are&you=doing";
        URL testSourceUrl = new URL(urlWithEscapedCharacters);
        String testUrl = urlWithEscapedCharacters;
        String expectedResult = urlWithEscapedCharacters;
        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), testUrl);
        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);
    }

    @Test
    public void testProperURLEncodingWithBackSlash() throws MalformedURLException {
        URLFilter urlFilter = createFilter(queryParamsToFilter);
        String urlWithEscapedCharacters =
                "http://www.voltaix.com/\\SDS\\Silicon\\Trisilane\\Trisilane_SI050_USENG.pdf";
        String expectedResult =
                "http://www.voltaix.com/%5CSDS%5CSilicon%5CTrisilane%5CTrisilane_SI050_USENG.pdf";
        URL testSourceUrl = new URL(urlWithEscapedCharacters);
        String testUrl = urlWithEscapedCharacters;
        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), testUrl);
        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);
    }

    @Test
    public void testInvalidURI() throws MalformedURLException {
        URLFilter urlFilter = createFilter(true, true);
        // this one is now handled by the normaliser
        String nonURI = "http://www.quanjing.com/search.aspx?q=top-651451||1|60|1|2||||&Fr=4";
        URL testSourceUrl = new URL(nonURI);
        String expectedResult =
                "http://www.quanjing.com/search.aspx?q=top-651451%7C%7C1%7C60%7C1%7C2%7C%7C%7C%7C&Fr=4";
        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), nonURI);
        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);

        // this one is
        nonURI =
                "http://vins.lemonde.fr?utm_source=LeMonde_partenaire_hp&utm_medium=EMPLACEMENT PARTENAIRE&utm_term=&utm_content=&utm_campaign=LeMonde_partenaire_hp";
        testSourceUrl = new URL(nonURI);
        expectedResult =
                "http://vins.lemonde.fr?utm_source=LeMonde_partenaire_hp&utm_medium=EMPLACEMENT%20PARTENAIRE&utm_term=&utm_content=&utm_campaign=LeMonde_partenaire_hp";
        normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), nonURI);
        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);

        // check normalisation of paths
        // http://docs.oracle.com/javase/7/docs/api/java/net/URI.html#normalize()
        String nonNormURL =
                "http://docs.oracle.com/javase/7/docs/api/java/net/../net/./URI.html#normalize()";
        testSourceUrl = new URL(nonNormURL);
        expectedResult = "http://docs.oracle.com/javase/7/docs/api/java/net/URI.html";
        normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), nonNormURL);
        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);
    }

    @Test
    public void testLowerCasing() throws MalformedURLException {
        URLFilter urlFilter = createFilter(false, false);
        URL testSourceUrl = new URL("http://blablabla.org/");

        String inputURL = "HTTP://www.quanjing.com/";
        String expectedResult = inputURL.toLowerCase();
        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), inputURL);

        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);

        inputURL = "http://www.QUANJING.COM/";
        expectedResult = inputURL.toLowerCase();
        normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), inputURL);

        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);
    }

    // https://github.com/DigitalPebble/storm-crawler/issues/401
    @Test
    public void testNonStandardPercentEncoding() throws MalformedURLException {
        URLFilter urlFilter = createFilter(false, false);
        URL testSourceUrl = new URL("http://www.hurriyet.com.tr/index/?d=20160328&p=13");

        String inputURL = "http://www.hurriyet.com.tr/index/?d=20160328&p=13&s=ni%u011fde";
        String expectedURL = "http://www.hurriyet.com.tr/index/?d=20160328&p=13&s=ni%C4%9Fde";
        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), inputURL);

        assertEquals("Failed to filter query string", expectedURL, normalizedUrl);
    }

    @Test
    public void testHostIDNtoASCII() throws MalformedURLException {
        ObjectNode filterParams = new ObjectNode(JsonNodeFactory.instance);
        filterParams.put("hostIDNtoASCII", true);
        URLFilter urlFilter = createFilter(filterParams);
        URL testSourceUrl = new URL("http://www.example.com/");

        String inputURL = "http://seal6.com.ar/";
        String expectedURL = "http://xn--seal6-pta.com.ar/";
        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), inputURL);

        assertEquals("Failed to filter query string", expectedURL, normalizedUrl);

        inputURL = "http://./";
        expectedURL = "http://xn--80aj7acp.xn--j1amh/";
        normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), inputURL);

        assertEquals("Failed to filter query string", expectedURL, normalizedUrl);
    }

    private JsonNode getArrayNode(List<String> queryElementsToRemove) {
        ObjectMapper mapper = new ObjectMapper();
        return mapper.valueToTree(queryElementsToRemove);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.persistence;

import com.digitalpebble.stormcrawler.Metadata;
import java.net.MalformedURLException;
import java.util.Calendar;
import java.util.Date;
import java.util.HashMap;
import java.util.Map;
import java.util.Optional;
import org.apache.commons.lang.time.DateUtils;
import org.junit.Assert;
import org.junit.Test;

public class DefaultSchedulerTest {
    @Test
    public void testScheduler() throws MalformedURLException {
        Map<String, Object> stormConf = new HashMap<>();
        stormConf.put("fetchInterval.FETCHED.testKey=someValue", 360);
        stormConf.put("fetchInterval.testKey=someValue", 3600);
        DefaultScheduler scheduler = new DefaultScheduler();
        scheduler.init(stormConf);

        Metadata metadata = new Metadata();
        metadata.addValue("testKey", "someValue");
        Optional<Date> nextFetch = scheduler.schedule(Status.FETCHED, metadata);

        Calendar cal = Calendar.getInstance();
        cal.add(Calendar.MINUTE, 360);
        Assert.assertEquals(
                DateUtils.round(cal.getTime(), Calendar.SECOND),
                DateUtils.round(nextFetch.get(), Calendar.SECOND));

        nextFetch = scheduler.schedule(Status.ERROR, metadata);

        cal = Calendar.getInstance();
        cal.add(Calendar.MINUTE, 3600);
        Assert.assertEquals(
                DateUtils.round(cal.getTime(), Calendar.SECOND),
                DateUtils.round(nextFetch.get(), Calendar.SECOND));
    }

    @Test
    public void testCustomWithDot() throws MalformedURLException {
        Map<String, Object> stormConf = new HashMap<>();
        stormConf.put("fetchInterval.FETCHED.testKey.key2=someValue", 360);
        DefaultScheduler scheduler = new DefaultScheduler();
        scheduler.init(stormConf);

        Metadata metadata = new Metadata();
        metadata.addValue("testKey.key2", "someValue");
        Optional<Date> nextFetch = scheduler.schedule(Status.FETCHED, metadata);

        Calendar cal = Calendar.getInstance();
        cal.add(Calendar.MINUTE, 360);
        Assert.assertEquals(
                DateUtils.round(cal.getTime(), Calendar.SECOND),
                DateUtils.round(nextFetch.get(), Calendar.SECOND));
    }

    @Test
    public void testBadConfig() throws MalformedURLException {
        Map<String, Object> stormConf = new HashMap<>();
        stormConf.put("fetchInterval.DODGYSTATUS.testKey=someValue", 360);
        DefaultScheduler scheduler = new DefaultScheduler();
        boolean exception = false;
        try {
            scheduler.init(stormConf);
        } catch (IllegalArgumentException e) {
            exception = true;
        }
        Assert.assertTrue(exception);
    }

    @Test
    public void testNever() throws MalformedURLException {
        Map<String, Object> stormConf = new HashMap<>();
        stormConf.put("fetchInterval.error", -1);
        DefaultScheduler scheduler = new DefaultScheduler();
        scheduler.init(stormConf);

        Metadata metadata = new Metadata();
        Optional<Date> nextFetch = scheduler.schedule(Status.ERROR, metadata);

        Assert.assertEquals(false, nextFetch.isPresent());
    }

    @Test
    public void testSpecificNever() throws MalformedURLException {
        Map<String, Object> stormConf = new HashMap<>();
        stormConf.put("fetchInterval.FETCHED.isSpam=true", -1);
        DefaultScheduler scheduler = new DefaultScheduler();
        scheduler.init(stormConf);

        Metadata metadata = new Metadata();
        metadata.setValue("isSpam", "true");
        Optional<Date> nextFetch = scheduler.schedule(Status.FETCHED, metadata);

        Assert.assertEquals(false, nextFetch.isPresent());
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.persistence;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.persistence.urlbuffer.PriorityURLBuffer;
import com.digitalpebble.stormcrawler.persistence.urlbuffer.SimpleURLBuffer;
import com.digitalpebble.stormcrawler.persistence.urlbuffer.URLBuffer;
import java.net.MalformedURLException;
import org.junit.Assert;
import org.junit.Test;

public class URLBufferTest {
    @Test
    public void testSimpleURLBuffer() throws MalformedURLException {
        URLBuffer buffer = new SimpleURLBuffer();
        Assert.assertFalse(buffer.hasNext());
        buffer.add("http://a.net/test.html", new Metadata());
        buffer.add("http://a.net/test2.html", new Metadata());
        buffer.add("http://b.net/test.html", new Metadata());
        buffer.add("http://c.net/test.html", new Metadata());
        Assert.assertEquals("http://a.net/test.html", buffer.next().get(0));
        Assert.assertEquals("http://b.net/test.html", buffer.next().get(0));
        // should return false if already there
        boolean added = buffer.add("http://c.net/test.html", new Metadata());
        Assert.assertFalse(added);
        added = buffer.add("http://d.net/test.html", new Metadata());
        Assert.assertTrue(added);
        Assert.assertEquals("http://c.net/test.html", buffer.next().get(0));
        Assert.assertEquals("http://a.net/test2.html", buffer.next().get(0));
        Assert.assertEquals("http://d.net/test.html", buffer.next().get(0));
        Assert.assertFalse(buffer.hasNext());
    }

    @Test
    public void testPriorityBuffer() throws MalformedURLException, InterruptedException {
        URLBuffer buffer = new PriorityURLBuffer();
        Assert.assertFalse(buffer.hasNext());
        buffer.add("http://a.net/test.html", new Metadata());
        buffer.add("http://a.net/test2.html", new Metadata());
        buffer.add("http://b.net/test.html", new Metadata());
        buffer.add("http://c.net/test.html", new Metadata());

        buffer.acked("http://c.net/test.html");
        buffer.acked("http://c.net/test.html");
        buffer.acked("http://c.net/test.html");

        buffer.acked("http://b.net/test.html");

        // wait N seconds - should trigger a rerank
        Thread.sleep(10000);

        // c should come first - it has been acked more often
        Assert.assertEquals("http://c.net/test.html", buffer.next().get(0));

        // then b
        Assert.assertEquals("http://b.net/test.html", buffer.next().get(0));

        // then a
        Assert.assertEquals("http://a.net/test.html", buffer.next().get(0));

        Assert.assertEquals("http://a.net/test2.html", buffer.next().get(0));

        Assert.assertFalse(buffer.hasNext());
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.persistence;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.protocol.HttpHeaders;
import java.net.MalformedURLException;
import java.time.Instant;
import java.time.ZonedDateTime;
import java.time.format.DateTimeFormatter;
import java.util.Calendar;
import java.util.Date;
import java.util.GregorianCalendar;
import java.util.HashMap;
import java.util.Map;
import java.util.Optional;
import org.apache.commons.lang.time.DateUtils;
import org.junit.Assert;
import org.junit.Test;

public class AdaptiveSchedulerTest {

    private static String md5sumEmptyContent = "d41d8cd98f00b204e9800998ecf8427e";
    private static String md5sumSpaceContent = "7215ee9c7d9dc229d2921a40e899ec5f";

    private static Map<String, Object> getConf() {
        Map<String, Object> stormConf = new HashMap<>();
        stormConf.put("fetchInterval.FETCHED.testKey=someValue", 6);
        stormConf.put("fetchInterval.testKey=someValue", 8);
        stormConf.put("scheduler.adaptive.setLastModified", true);
        stormConf.put("scheduler.adaptive.fetchInterval.min", 2);
        stormConf.put("fetchInterval.default", 5);
        stormConf.put("scheduler.adaptive.fetchInterval.max", 10);
        stormConf.put("protocol.md.prefix", "protocol.");
        return stormConf;
    }

    /**
     * Verify setting the initial fetch interval by metadata and fetch status implemented in
     * DefaultScheduler
     */
    @Test
    public void testSchedulerInitialInterval() throws MalformedURLException {
        Scheduler scheduler = new AdaptiveScheduler();
        scheduler.init(getConf());

        Metadata metadata = new Metadata();
        metadata.addValue("testKey", "someValue");
        metadata.addValue("fetch.statusCode", "200");
        Optional<Date> nextFetch = scheduler.schedule(Status.FETCHED, metadata);

        Calendar cal = Calendar.getInstance();
        cal.add(Calendar.MINUTE, 6);
        Assert.assertEquals(
                DateUtils.round(cal.getTime(), Calendar.SECOND),
                DateUtils.round(nextFetch.get(), Calendar.SECOND));

        nextFetch = scheduler.schedule(Status.ERROR, metadata);

        cal = Calendar.getInstance();
        cal.add(Calendar.MINUTE, 8);
        Assert.assertEquals(
                DateUtils.round(cal.getTime(), Calendar.SECOND),
                DateUtils.round(nextFetch.get(), Calendar.SECOND));
    }

    @Test
    public void testSchedule() throws MalformedURLException {
        Scheduler scheduler = new AdaptiveScheduler();
        scheduler.init(getConf());

        Metadata metadata = new Metadata();
        metadata.addValue("fetch.statusCode", "200");
        metadata.addValue(AdaptiveScheduler.SIGNATURE_KEY, md5sumEmptyContent);
        Optional<Date> nextFetch = scheduler.schedule(Status.FETCHED, metadata);
        Instant firstFetch =
                DateUtils.round(Calendar.getInstance().getTime(), Calendar.SECOND).toInstant();

        /* verify initial fetch interval and last-modified time */
        String lastModified = metadata.getFirstValue(HttpHeaders.LAST_MODIFIED);
        Assert.assertNotNull(lastModified);
        Instant lastModifiedTime =
                DateUtils.round(
                                GregorianCalendar.from(
                                        DateTimeFormatter.ISO_OFFSET_DATE_TIME.parse(
                                                lastModified, ZonedDateTime::from)),
                                Calendar.SECOND)
                        .toInstant();
        Assert.assertEquals(firstFetch, lastModifiedTime);
        String fetchInterval = metadata.getFirstValue(AdaptiveScheduler.FETCH_INTERVAL_KEY);
        Assert.assertNotNull(fetchInterval);
        /* initial interval is the default interval */
        Assert.assertEquals(5, Integer.parseInt(fetchInterval));

        /* test with signature not modified */
        metadata.addValue(AdaptiveScheduler.SIGNATURE_OLD_KEY, md5sumEmptyContent);
        nextFetch = scheduler.schedule(Status.FETCHED, metadata);
        fetchInterval = metadata.getFirstValue(AdaptiveScheduler.FETCH_INTERVAL_KEY);
        Assert.assertNotNull(fetchInterval);
        /* interval should be bigger than initial interval */
        int fi1 = Integer.parseInt(fetchInterval);
        Assert.assertTrue(5 < fi1);
        /* last-modified time should be unchanged */
        Assert.assertEquals(lastModified, metadata.getFirstValue(HttpHeaders.LAST_MODIFIED));

        /* test with HTTP 304 "not modified" */
        metadata.setValue("fetch.statusCode", "304");
        nextFetch = scheduler.schedule(Status.FETCHED, metadata);
        fetchInterval = metadata.getFirstValue(AdaptiveScheduler.FETCH_INTERVAL_KEY);
        Assert.assertNotNull(fetchInterval);
        /* interval should be bigger than initial interval and interval from last step */
        int fi2 = Integer.parseInt(fetchInterval);
        Assert.assertTrue(5 < fi2);
        Assert.assertTrue(fi1 < fi2);
        /* last-modified time should be unchanged */
        Assert.assertEquals(lastModified, metadata.getFirstValue(HttpHeaders.LAST_MODIFIED));

        /* test with a changed signature */
        metadata.setValue("fetch.statusCode", "200");
        metadata.addValue(AdaptiveScheduler.SIGNATURE_KEY, md5sumSpaceContent);
        nextFetch = scheduler.schedule(Status.FETCHED, metadata);
        Instant lastFetch =
                DateUtils.round(Calendar.getInstance().getTime(), Calendar.SECOND).toInstant();
        fetchInterval = metadata.getFirstValue(AdaptiveScheduler.FETCH_INTERVAL_KEY);
        Assert.assertNotNull(fetchInterval);
        /* interval should now shrink */
        int fi3 = Integer.parseInt(fetchInterval);
        Assert.assertTrue(fi2 > fi3);
        /* last-modified time should fetch time of last fetch */
        lastModified = metadata.getFirstValue(HttpHeaders.LAST_MODIFIED);
        Assert.assertNotNull(lastModified);
        lastModifiedTime =
                DateUtils.round(
                                GregorianCalendar.from(
                                        DateTimeFormatter.ISO_OFFSET_DATE_TIME.parse(
                                                lastModified, ZonedDateTime::from)),
                                Calendar.SECOND)
                        .toInstant();
        Assert.assertEquals(lastFetch, lastModifiedTime);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.proxy;

import org.apache.storm.Config;
import org.junit.Assert;
import org.junit.Test;

public class SingleProxyManagerTest {
    @Test
    public void testSimpleProxyManager() throws RuntimeException {
        Config config = new Config();
        config.put("http.proxy.host", "example.com");
        config.put("http.proxy.type", "HTTP");
        config.put("http.proxy.port", 8080);
        config.put("http.proxy.user", "user1");
        config.put("http.proxy.pass", "pass1");

        SingleProxyManager pm = new SingleProxyManager();
        pm.configure(config);

        SCProxy proxy = pm.getProxy(null);

        Assert.assertEquals(proxy.getProtocol(), "http");
        Assert.assertEquals(proxy.getAddress(), "example.com");
        Assert.assertEquals(proxy.getPort(), "8080");
        Assert.assertEquals(proxy.getUsername(), "user1");
        Assert.assertEquals(proxy.getPassword(), "pass1");

        Assert.assertEquals(proxy.toString(), "http://user1:pass1@example.com:8080");
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.proxy;

import org.junit.Assert;
import org.junit.Test;

public class SCProxyTest {
    @Test
    public void testProxyConstructor() {
        String[] valid_inputs = {
            "http://example.com:8080",
            "https://example.com:8080",
            "http://user1:pass1@example.com:8080",
            "sock5://user1:pass1@example.com:8080",
            "http://example.com:80",
            "sock5://example.com:64000",
        };

        String[][] valid_outputs = {
            {"http", null, null, "example.com", "8080"},
            {"https", null, null, "example.com", "8080"},
            {"http", "user1", "pass1", "example.com", "8080"},
            {"sock5", "user1", "pass1", "example.com", "8080"},
            {"http", null, null, "example.com", "80"},
            {"sock5", null, null, "example.com", "64000"},
        };

        String[] invalid_inputs = {
            "http://example.com",
            "sock5://:example.com:8080",
            "example.com:8080",
            "https://user1@example.com:8080",
        };

        for (int i = 0; i < valid_inputs.length; i++) {
            SCProxy proxy = new SCProxy(valid_inputs[i]);

            Assert.assertEquals(proxy.getUsage(), 0);

            Assert.assertEquals(proxy.getProtocol(), valid_outputs[i][0]);
            Assert.assertEquals(proxy.getUsername(), valid_outputs[i][1]);
            Assert.assertEquals(proxy.getPassword(), valid_outputs[i][2]);
            Assert.assertEquals(proxy.getAddress(), valid_outputs[i][3]);
            Assert.assertEquals(proxy.getPort(), valid_outputs[i][4]);
        }

        for (String invalid_input : invalid_inputs) {
            boolean failed = false;

            try {
                new SCProxy(invalid_input);
            } catch (IllegalArgumentException ignored) {
                failed = true;
            }

            Assert.assertTrue(failed);
        }
    }

    @Test
    public void testToString() {
        String[] proxyStrings = {
            "http://example.com:8080",
            "https://example.com:8080",
            "http://user1:pass1@example.com:8080",
            "sock5://user1:pass1@example.com:8080",
            "http://example.com:80",
            "sock5://example.com:64000",
        };

        for (String proxyString : proxyStrings) {
            SCProxy proxy = new SCProxy(proxyString);
            Assert.assertEquals(proxyString, proxy.toString());
        }
    }

    @Test
    public void testIncrementUsage() {
        SCProxy proxy = new SCProxy("http://user1:pass1@example.com:8080");
        Assert.assertEquals(proxy.getUsage(), 0);
        proxy.incrementUsage();
        Assert.assertEquals(proxy.getUsage(), 1);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.proxy;

import java.io.FileWriter;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Paths;
import org.apache.storm.Config;
import org.junit.Assert;
import org.junit.Test;

public class MultiProxyManagerTest {
    @Test
    public void testMultiProxyManagerConstructorArray() {
        String[] proxyStrings = {
            "http://example.com:8080",
            "https://example.com:8080",
            "http://user1:pass1@example.com:8080",
            "sock5://user1:pass1@example.com:8080",
            "http://example.com:80",
            "sock5://example.com:64000",
        };

        MultiProxyManager pm = new MultiProxyManager();
        pm.configure(MultiProxyManager.ProxyRotation.RANDOM, proxyStrings);

        Assert.assertEquals(pm.proxyCount(), proxyStrings.length);
    }

    @Test
    public void testMultiProxyManagerConstructorFile() throws IOException {
        String[] proxyStrings = {
            "http://example.com:8080",
            "https://example.com:8080",
            "http://user1:pass1@example.com:8080",
            "sock5://user1:pass1@example.com:8080",
            "http://example.com:80",
            "sock5://example.com:64000",
        };

        FileWriter writer = new FileWriter("/tmp/proxies.txt");
        for (String proxyString : proxyStrings) {
            writer.write("# fake comment to test" + "\n");
            writer.write("// fake comment to test" + "\n");
            writer.write("       " + "\n");
            writer.write("\n");
            writer.write(proxyString + "\n");
        }
        writer.close();

        Config config = new Config();
        config.put("http.proxy.file", "/tmp/proxies.txt");
        config.put("http.proxy.rotation", "ROUND_ROBIN");

        MultiProxyManager pm = new MultiProxyManager();
        pm.configure(config);

        Assert.assertEquals(pm.proxyCount(), proxyStrings.length);

        Files.deleteIfExists(Paths.get("/tmp/proxies.txt"));
    }

    @Test
    public void testGetRandom() {
        String[] proxyStrings = {
            "http://example.com:8080",
            "https://example.com:8080",
            "http://user1:pass1@example.com:8080",
            "sock5://user1:pass1@example.com:8080",
            "http://example.com:80",
            "sock5://example.com:64000",
        };

        MultiProxyManager pm = new MultiProxyManager();
        pm.configure(MultiProxyManager.ProxyRotation.RANDOM, proxyStrings);

        for (int i = 0; i < 1000; i++) {
            SCProxy proxy = pm.getProxy(null);
            Assert.assertTrue(proxy.toString().length() > 0);
        }
    }

    @Test
    public void testGetRoundRobin() {
        String[] proxyStrings = {
            "http://example.com:8080",
            "https://example.com:8080",
            "http://user1:pass1@example.com:8080",
            "sock5://user1:pass1@example.com:8080",
            "http://example.com:80",
            "sock5://example.com:64000",
        };

        MultiProxyManager pm = new MultiProxyManager();
        pm.configure(MultiProxyManager.ProxyRotation.ROUND_ROBIN, proxyStrings);

        SCProxy proxy1 = pm.getProxy(null);
        SCProxy proxy2 = pm.getProxy(null);
        SCProxy proxy3 = pm.getProxy(null);

        Assert.assertNotEquals(proxy1.toString(), proxy2.toString());
        Assert.assertNotEquals(proxy1.toString(), proxy3.toString());
        Assert.assertNotEquals(proxy2.toString(), proxy1.toString());
        Assert.assertNotEquals(proxy2.toString(), proxy3.toString());
        Assert.assertNotEquals(proxy3.toString(), proxy1.toString());
        Assert.assertNotEquals(proxy3.toString(), proxy2.toString());

        for (int i = 0; i < 3; i++) {
            pm.getProxy(null);
        }

        SCProxy proxy4 = pm.getProxy(null);
        SCProxy proxy5 = pm.getProxy(null);
        SCProxy proxy6 = pm.getProxy(null);

        Assert.assertNotEquals(proxy4.toString(), proxy5.toString());
        Assert.assertNotEquals(proxy4.toString(), proxy6.toString());
        Assert.assertNotEquals(proxy5.toString(), proxy4.toString());
        Assert.assertNotEquals(proxy5.toString(), proxy6.toString());
        Assert.assertNotEquals(proxy6.toString(), proxy4.toString());
        Assert.assertNotEquals(proxy6.toString(), proxy5.toString());

        Assert.assertEquals(proxy1.toString(), proxy4.toString());
        Assert.assertEquals(proxy2.toString(), proxy5.toString());
        Assert.assertEquals(proxy3.toString(), proxy6.toString());
    }

    @Test
    public void testGetLeastUsed() {
        String[] proxyStrings = {
            "http://example.com:8080",
            "https://example.com:8080",
            "http://user1:pass1@example.com:8080",
            "sock5://user1:pass1@example.com:8080",
            "http://example.com:80",
            "sock5://example.com:64000",
        };

        MultiProxyManager pm = new MultiProxyManager();
        pm.configure(MultiProxyManager.ProxyRotation.LEAST_USED, proxyStrings);

        SCProxy proxy1 = pm.getProxy(null);
        SCProxy proxy2 = pm.getProxy(null);
        SCProxy proxy3 = pm.getProxy(null);

        Assert.assertNotEquals(proxy1.toString(), proxy2.toString());
        Assert.assertNotEquals(proxy1.toString(), proxy3.toString());
        Assert.assertNotEquals(proxy2.toString(), proxy1.toString());
        Assert.assertNotEquals(proxy2.toString(), proxy3.toString());
        Assert.assertNotEquals(proxy3.toString(), proxy1.toString());
        Assert.assertNotEquals(proxy3.toString(), proxy2.toString());

        Assert.assertEquals(proxy1.getUsage(), 1);
        Assert.assertEquals(proxy2.getUsage(), 1);
        Assert.assertEquals(proxy3.getUsage(), 1);

        for (int i = 0; i < 3; i++) {
            pm.getProxy(null);
        }

        SCProxy proxy4 = pm.getProxy(null);
        SCProxy proxy5 = pm.getProxy(null);
        SCProxy proxy6 = pm.getProxy(null);

        Assert.assertNotEquals(proxy4.toString(), proxy5.toString());
        Assert.assertNotEquals(proxy4.toString(), proxy6.toString());
        Assert.assertNotEquals(proxy5.toString(), proxy4.toString());
        Assert.assertNotEquals(proxy5.toString(), proxy6.toString());
        Assert.assertNotEquals(proxy6.toString(), proxy4.toString());
        Assert.assertNotEquals(proxy6.toString(), proxy5.toString());

        Assert.assertEquals(proxy4.getUsage(), 2);
        Assert.assertEquals(proxy5.getUsage(), 2);
        Assert.assertEquals(proxy6.getUsage(), 2);

        Assert.assertEquals(proxy1.toString(), proxy4.toString());
        Assert.assertEquals(proxy2.toString(), proxy5.toString());
        Assert.assertEquals(proxy3.toString(), proxy6.toString());
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.parse;

import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.TestOutputCollector;
import com.digitalpebble.stormcrawler.TestUtil;
import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.util.HashMap;
import java.util.Map;
import org.apache.commons.io.IOUtils;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.topology.base.BaseRichBolt;
import org.apache.storm.tuple.Tuple;
import org.junit.After;

public class ParsingTester {
    protected BaseRichBolt bolt;
    protected TestOutputCollector output;

    protected void setupParserBolt(BaseRichBolt bolt) {
        this.bolt = bolt;
        output = new TestOutputCollector();
    }

    @After
    public void cleanupBolt() {
        if (bolt != null) {
            bolt.cleanup();
        }
        output = null;
    }

    protected void prepareParserBolt(String configFile) {
        prepareParserBolt(configFile, new HashMap());
    }

    protected void prepareParserBolt(String configFile, Map parserConfig) {
        parserConfig.put("parsefilters.config.file", configFile);
        bolt.prepare(
                parserConfig, TestUtil.getMockedTopologyContext(), new OutputCollector(output));
    }

    protected void parse(String url, byte[] content, Metadata metadata) throws IOException {
        Tuple tuple = mock(Tuple.class);
        when(tuple.getBinaryByField("content")).thenReturn(content);
        when(tuple.getStringByField("url")).thenReturn(url);
        when(tuple.getValueByField("metadata")).thenReturn(metadata);
        bolt.execute(tuple);
    }

    protected void parse(String url, String filename) throws IOException {
        parse(url, filename, new Metadata());
    }

    protected void parse(String url, String filename, Metadata metadata) throws IOException {
        byte[] content = readContent(filename);
        Tuple tuple = mock(Tuple.class);
        when(tuple.getBinaryByField("content")).thenReturn(content);
        when(tuple.getStringByField("url")).thenReturn(url);
        when(tuple.getValueByField("metadata")).thenReturn(metadata);
        bolt.execute(tuple);
    }

    protected byte[] readContent(String filename) throws IOException {
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        IOUtils.copy(getClass().getClassLoader().getResourceAsStream(filename), baos);
        return baos.toByteArray();
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.parse;

import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.TestUtil;
import com.digitalpebble.stormcrawler.bolt.JSoupParserBolt;
import java.io.IOException;
import java.util.HashMap;
import java.util.Map;
import org.apache.storm.task.OutputCollector;
import org.junit.Assert;
import org.junit.Before;
import org.junit.Test;

/** Checks that there aren't any duplicate links coming out of JSoupParser Bolt */
public class DuplicateLinksTest extends ParsingTester {

    @Before
    public void setupParserBolt() {
        bolt = new JSoupParserBolt();
        setupParserBolt(bolt);
    }

    @Test
    public void testSitemapSubdocuments() throws IOException {
        Map config = new HashMap();
        // generate a dummy config file
        config.put("urlfilters.config.file", "basicurlnormalizer.json");
        bolt.prepare(config, TestUtil.getMockedTopologyContext(), new OutputCollector(output));
        Metadata metadata = new Metadata();
        parse("http://www.digitalpebble.com/duplicates.html", "duplicateLinks.html", metadata);
        Assert.assertEquals(1, output.getEmitted(Constants.StatusStreamName).size());
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.parse;

import static org.junit.Assert.assertNotNull;

import com.digitalpebble.stormcrawler.Constants;
import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.bolt.JSoupParserBolt;
import java.io.IOException;
import java.util.List;
import org.junit.Assert;
import org.junit.Before;
import org.junit.Test;

/** @see https://github.com/DigitalPebble/storm-crawler/pull/653 * */
public class StackOverflowTest extends ParsingTester {

    @Before
    public void setupParserBolt() {
        bolt = new JSoupParserBolt();
        setupParserBolt(bolt);
    }

    @Test
    public void testStackOverflow() throws IOException {
        prepareParserBolt("test.parsefilters.json");
        Metadata metadata = new Metadata();
        parse("http://polloxniner.blogspot.com", "stackexception.html", metadata);
        Assert.assertEquals(164, output.getEmitted(Constants.StatusStreamName).size());
    }

    /** @see https://github.com/DigitalPebble/storm-crawler/issues/666 * */
    @Test
    public void testNamespaceExtraction() throws IOException {
        prepareParserBolt("test.parsefilters.json");
        Metadata metadata = new Metadata();
        parse("http://polloxniner.blogspot.com", "stackexception.html", metadata);
        Assert.assertEquals(1, output.getEmitted().size());

        List<Object> obj = output.getEmitted().get(0);
        Metadata m = (Metadata) obj.get(2);
        assertNotNull(m.getFirstValue("title"));
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.parse;

import static org.junit.Assert.assertEquals;

import java.io.IOException;
import org.apache.storm.Config;
import org.jsoup.nodes.Document;
import org.jsoup.parser.Parser;
import org.junit.Test;

public class TextExtractorTest {

    @Test
    public void testMainContent() throws IOException {
        Config conf = new Config();
        conf.put(TextExtractor.INCLUDE_PARAM_NAME, "DIV[id=\"maincontent\"]");

        TextExtractor extractor = new TextExtractor(conf);

        String content =
                "<html>the<div id='maincontent'>main<div>content</div></div>of the page</html>";

        Document jsoupDoc = Parser.htmlParser().parseInput(content, "http://stormcrawler.net");
        String text = extractor.text(jsoupDoc.body());

        assertEquals("main content", text);
    }

    @Test
    public void testExclusion() throws IOException {
        Config conf = new Config();
        conf.put(TextExtractor.EXCLUDE_PARAM_NAME, "STYLE");

        TextExtractor extractor = new TextExtractor(conf);

        String content = "<html>the<style>main</style>content of the page</html>";

        Document jsoupDoc = Parser.htmlParser().parseInput(content, "http://stormcrawler.net");
        String text = extractor.text(jsoupDoc.body());

        assertEquals("the content of the page", text);
    }

    @Test
    public void testExclusionCase() throws IOException {
        Config conf = new Config();
        conf.put(TextExtractor.EXCLUDE_PARAM_NAME, "style");

        TextExtractor extractor = new TextExtractor(conf);

        String content = "<html>the<STYLE>main</STYLE>content of the page</html>";

        Document jsoupDoc = Parser.htmlParser().parseInput(content, "http://stormcrawler.net");
        String text = extractor.text(jsoupDoc.body());

        assertEquals("the content of the page", text);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.parse.filter;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.bolt.JSoupParserBolt;
import com.digitalpebble.stormcrawler.parse.ParsingTester;
import java.io.IOException;
import java.util.List;
import org.junit.Assert;
import org.junit.Before;
import org.junit.Test;

public class XPathFilterTest extends ParsingTester {

    @Before
    public void setupParserBolt() {
        bolt = new JSoupParserBolt();
        setupParserBolt(bolt);
    }

    @Test
    public void testBasicExtraction() throws IOException {

        prepareParserBolt("test.parsefilters.json");

        parse("http://www.digitalpebble.com", "digitalpebble.com.html");

        Assert.assertEquals(1, output.getEmitted().size());
        List<Object> parsedTuple = output.getEmitted().get(0);
        Metadata metadata = (Metadata) parsedTuple.get(2);
        Assert.assertNotNull(metadata);
        String concept = metadata.getFirstValue("concept");
        Assert.assertNotNull(concept);

        concept = metadata.getFirstValue("concept2");
        Assert.assertNotNull(concept);
    }

    @Test
    // https://github.com/DigitalPebble/storm-crawler/issues/219
    public void testScriptExtraction() throws IOException {

        prepareParserBolt("test.parsefilters.json");

        parse("http://www.digitalpebble.com", "digitalpebble.com.html");

        Assert.assertEquals(1, output.getEmitted().size());
        List<Object> parsedTuple = output.getEmitted().get(0);
        Metadata metadata = (Metadata) parsedTuple.get(2);
        Assert.assertNotNull(metadata);
        String[] scripts = metadata.getValues("js");
        Assert.assertNotNull(scripts);
        // should be 2 of them
        Assert.assertEquals(2, scripts.length);
        Assert.assertEquals("", scripts[0].trim());
        Assert.assertTrue(scripts[1].contains("urchinTracker();"));
    }

    @Test
    public void testLDJsonExtraction() throws IOException {

        prepareParserBolt("test.parsefilters.json");

        parse("http://www.digitalpebble.com", "digitalpebble.com.html");

        Assert.assertEquals(1, output.getEmitted().size());
        List<Object> parsedTuple = output.getEmitted().get(0);
        Metadata metadata = (Metadata) parsedTuple.get(2);
        Assert.assertNotNull(metadata);
        String[] scripts = metadata.getValues("streetAddress");
        Assert.assertNotNull(scripts);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.parse.filter;

import com.digitalpebble.stormcrawler.parse.ParseData;
import com.digitalpebble.stormcrawler.parse.ParseFilter;
import com.digitalpebble.stormcrawler.parse.ParseResult;
import java.io.ByteArrayInputStream;
import java.io.InputStream;
import javax.xml.parsers.DocumentBuilderFactory;
import javax.xml.xpath.XPath;
import javax.xml.xpath.XPathConstants;
import javax.xml.xpath.XPathExpression;
import javax.xml.xpath.XPathFactory;
import org.slf4j.LoggerFactory;
import org.w3c.dom.Document;
import org.w3c.dom.DocumentFragment;
import org.w3c.dom.Element;
import org.w3c.dom.Node;
import org.w3c.dom.NodeList;

public class SubDocumentsParseFilter extends ParseFilter {
    private static final org.slf4j.Logger LOG =
            LoggerFactory.getLogger(SubDocumentsParseFilter.class);

    @Override
    public void filter(String URL, byte[] content, DocumentFragment doc, ParseResult parse) {

        InputStream stream = new ByteArrayInputStream(content);

        try {
            DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();
            Document document = factory.newDocumentBuilder().parse(stream);
            Element root = document.getDocumentElement();

            XPath xPath = XPathFactory.newInstance().newXPath();
            XPathExpression expression = xPath.compile("//url");

            NodeList nodes = (NodeList) expression.evaluate(root, XPathConstants.NODESET);

            for (int i = 0; i < nodes.getLength(); i++) {
                Node node = nodes.item(i);

                expression = xPath.compile("loc");
                Node child = (Node) expression.evaluate(node, XPathConstants.NODE);

                // create a subdocument for each url found in the sitemap
                ParseData parseData = parse.get(child.getTextContent());

                NodeList childs = node.getChildNodes();
                for (int j = 0; j < childs.getLength(); j++) {
                    Node n = childs.item(j);
                    parseData.put(n.getNodeName(), n.getTextContent());
                }
            }
        } catch (Exception e) {
            LOG.error("Error processing sitemap from {}: {}", URL, e);
        }
    }

    @Override
    public boolean needsDOM() {
        return true;
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.parse.filter;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.bolt.JSoupParserBolt;
import com.digitalpebble.stormcrawler.parse.ParsingTester;
import java.io.IOException;
import java.util.List;
import org.junit.Assert;
import org.junit.Before;
import org.junit.Test;

public class CSVMetadataFilterTest extends ParsingTester {

    @Before
    public void setupParserBolt() {
        bolt = new JSoupParserBolt();
        setupParserBolt(bolt);
    }

    @Test
    public void testMultivalued() throws IOException {

        prepareParserBolt("test.parsefilters.json");

        parse("http://www.digitalpebble.com", "digitalpebble.com.html");

        Assert.assertEquals(1, output.getEmitted().size());
        List<Object> parsedTuple = output.getEmitted().get(0);
        Metadata metadata = (Metadata) parsedTuple.get(2);
        Assert.assertNotNull(metadata);
        String[] kws = metadata.getValues("keywords");
        Assert.assertNotNull(kws);
        Assert.assertEquals(12, kws.length);
    }
}

Parse Compilation Unit:
package com.digitalpebble.stormcrawler.parse.filter;

import com.digitalpebble.stormcrawler.parse.ParseResult;
import java.io.IOException;
import java.util.HashMap;
import org.junit.Assert;
import org.junit.Test;

public class CollectionTaggerTest {

    @Test
    public void testCollectionTagger() throws IOException {

        CollectionTagger tagger = new CollectionTagger();
        tagger.configure(new HashMap(), null);
        ParseResult parse = new ParseResult();
        String URL = "http://stormcrawler.net/";
        tagger.filter(URL, null, null, parse);
        String[] collections = parse.get(URL).getMetadata().getValues("collections");
        Assert.assertNotNull(collections);
        Assert.assertEquals(2, collections.length);

        URL = "http://baby.com/tiny-crawler/";
        tagger.filter(URL, null, null, parse);
        collections = parse.get(URL).getMetadata().getValues("collections");
        Assert.assertNull(collections);

        URL = "http://nutch.apache.org/";
        tagger.filter(URL, null, null, parse);
        collections = parse.get(URL).getMetadata().getValues("collections");
        Assert.assertNotNull(collections);
        Assert.assertEquals(1, collections.length);
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.parse.filter;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.bolt.JSoupParserBolt;
import com.digitalpebble.stormcrawler.parse.ParsingTester;
import java.io.IOException;
import java.util.HashMap;
import java.util.Map;
import org.junit.Assert;
import org.junit.Before;
import org.junit.Test;

public class SubDocumentsFilterTest extends ParsingTester {

    @Before
    public void setupParserBolt() {
        bolt = new JSoupParserBolt();
        setupParserBolt(bolt);
    }

    @Test
    public void testSitemapSubdocuments() throws IOException {
        Map config = new HashMap();
        config.put("detect.mimetype", false);
        prepareParserBolt("test.subdocfilter.json", config);

        Metadata metadata = new Metadata();

        parse("http://www.digitalpebble.com/sitemap.xml", "digitalpebble.sitemap.xml", metadata);

        Assert.assertEquals(6, output.getEmitted().size());
    }
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler;

import java.util.ArrayList;
import java.util.Collection;
import java.util.Collections;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import org.apache.storm.spout.ISpoutOutputCollector;
import org.apache.storm.task.IOutputCollector;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.utils.Utils;

public class TestOutputCollector implements IOutputCollector, ISpoutOutputCollector {
    private List<Tuple> acked = new ArrayList<>();
    private List<Tuple> failed = new ArrayList<>();
    private Map<String, List<List<Object>>> emitted = new HashMap<>();

    @Override
    public void reportError(Throwable error) {}

    @Override
    public List<Integer> emit(String streamId, Collection<Tuple> anchors, List<Object> tuples) {
        addEmittedTuple(streamId, tuples);
        // No idea what to return
        return null;
    }

    @Override
    public void emitDirect(
            int taskId, String streamId, Collection<Tuple> anchors, List<Object> tuple) {}

    @Override
    public void ack(Tuple input) {
        acked.add(input);
    }

    @Override
    public void fail(Tuple input) {
        failed.add(input);
    }

    public List<List<Object>> getEmitted() {
        return getEmitted(Utils.DEFAULT_STREAM_ID);
    }

    public List<List<Object>> getEmitted(String streamId) {
        return emitted.getOrDefault(streamId, Collections.emptyList());
    }

    public List<Tuple> getAckedTuples() {
        return acked;
    }

    public List<Tuple> getFailedTuples() {
        return failed;
    }

    @Override
    public List<Integer> emit(String streamId, List<Object> tuple, Object messageId) {
        addEmittedTuple(streamId, tuple);
        return null;
    }

    @Override
    public void emitDirect(int taskId, String streamId, List<Object> tuple, Object messageId) {
        addEmittedTuple(streamId, tuple);
    }

    private void addEmittedTuple(String streamId, List<Object> tuple) {
        emitted.computeIfAbsent(streamId, k -> new ArrayList<>()).add(tuple);
    }

    @Override
    public long getPendingCount() {
        return 0;
    }

    @Override
    public void resetTimeout(Tuple tuple) {}

    @Override
    public void flush() {}
}

Parse Compilation Unit:
package ${package};

/**
 * Licensed to DigitalPebble Ltd under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import com.digitalpebble.stormcrawler.*;
import com.digitalpebble.stormcrawler.bolt.FetcherBolt;
import com.digitalpebble.stormcrawler.bolt.JSoupParserBolt;
import com.digitalpebble.stormcrawler.bolt.SiteMapParserBolt;
import com.digitalpebble.stormcrawler.bolt.URLPartitionerBolt;
import com.digitalpebble.stormcrawler.bolt.FeedParserBolt;
import com.digitalpebble.stormcrawler.indexing.StdOutIndexer;
import com.digitalpebble.stormcrawler.persistence.StdOutStatusUpdater;
import com.digitalpebble.stormcrawler.spout.MemorySpout;
import com.digitalpebble.stormcrawler.tika.ParserBolt;
import com.digitalpebble.stormcrawler.tika.RedirectionBolt;

import org.apache.storm.topology.TopologyBuilder;
import org.apache.storm.tuple.Fields;

/**
 * Dummy topology to play with the spouts and bolts
 */
public class CrawlTopology extends ConfigurableTopology {

	public static void main(String[] args) throws Exception {
		ConfigurableTopology.start(new CrawlTopology(), args);
	}

	@Override
	protected int run(String[] args) {
		TopologyBuilder builder = new TopologyBuilder();

		String[] testURLs = new String[] { "http://www.lequipe.fr/", "http://www.lemonde.fr/", "http://www.bbc.co.uk/",
				"http://storm.apache.org/", "http://digitalpebble.com/" };

		builder.setSpout("spout", new MemorySpout(testURLs));

		builder.setBolt("partitioner", new URLPartitionerBolt()).shuffleGrouping("spout");

		builder.setBolt("fetch", new FetcherBolt()).fieldsGrouping("partitioner", new Fields("key"));

		builder.setBolt("sitemap", new SiteMapParserBolt()).localOrShuffleGrouping("fetch");

		builder.setBolt("feeds", new FeedParserBolt()).localOrShuffleGrouping("sitemap");

		builder.setBolt("parse", new JSoupParserBolt()).localOrShuffleGrouping("feeds");

		builder.setBolt("shunt", new RedirectionBolt()).localOrShuffleGrouping("parse");

		builder.setBolt("tika", new ParserBolt()).localOrShuffleGrouping("shunt", "tika");

		builder.setBolt("index", new StdOutIndexer()).localOrShuffleGrouping("shunt").localOrShuffleGrouping("tika");

		Fields furl = new Fields("url");

		// can also use MemoryStatusUpdater for simple recursive crawls
		builder.setBolt("status", new StdOutStatusUpdater()).fieldsGrouping("fetch", Constants.StatusStreamName, furl)
				.fieldsGrouping("sitemap", Constants.StatusStreamName, furl)
				.fieldsGrouping("feeds", Constants.StatusStreamName, furl)
				.fieldsGrouping("parse", Constants.StatusStreamName, furl)
				.fieldsGrouping("tika", Constants.StatusStreamName, furl)
				.fieldsGrouping("index", Constants.StatusStreamName, furl);

		return submit("crawl", conf, builder);
	}
}

Parse Compilation Unit:
/**
 * Licensed to DigitalPebble Ltd under one or more contributor license agreements. See the NOTICE
 * file distributed with this work for additional information regarding copyright ownership.
 * DigitalPebble licenses this file to You under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License. You may obtain a copy of the
 * License at
 *
 * <p>http://www.apache.org/licenses/LICENSE-2.0
 *
 * <p>Unless required by applicable law or agreed to in writing, software distributed under the
 * License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.digitalpebble.stormcrawler.filtering;

import static org.junit.Assert.assertEquals;

import com.digitalpebble.stormcrawler.Metadata;
import com.digitalpebble.stormcrawler.filtering.basic.BasicURLNormalizer;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.node.JsonNodeFactory;
import com.fasterxml.jackson.databind.node.ObjectNode;
import java.net.MalformedURLException;
import java.net.URL;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import org.junit.Assert;
import org.junit.Test;

/**
 * Utility class which encapsulates the filtering of URLs based on the hostname or domain of the
 * source URL.
 */
public class BasicURLNormalizerTest {

    List<String> queryParamsToFilter = Arrays.asList("a", "foo");

    private URLFilter createFilter(boolean removeAnchor, boolean checkValidURI) {
        ObjectNode filterParams = new ObjectNode(JsonNodeFactory.instance);
        filterParams.put("removeAnchorPart", Boolean.valueOf(removeAnchor));
        filterParams.put("checkValidURI", Boolean.valueOf(checkValidURI));
        return createFilter(filterParams);
    }

    private URLFilter createFilter(List<String> queryElementsToRemove) {
        ObjectNode filterParams = new ObjectNode(JsonNodeFactory.instance);
        filterParams.set("queryElementsToRemove", getArrayNode(queryElementsToRemove));
        return createFilter(filterParams);
    }

    private URLFilter createFilter(boolean removeAnchor, List<String> queryElementsToRemove) {
        ObjectNode filterParams = new ObjectNode(JsonNodeFactory.instance);
        filterParams.set("queryElementsToRemove", getArrayNode(queryElementsToRemove));
        filterParams.put("removeAnchorPart", Boolean.valueOf(removeAnchor));
        return createFilter(filterParams);
    }

    private URLFilter createFilter(
            boolean removeAnchor, boolean unmangleQueryString, List<String> queryElementsToRemove) {
        ObjectNode filterParams = new ObjectNode(JsonNodeFactory.instance);
        filterParams.set("queryElementsToRemove", getArrayNode(queryElementsToRemove));
        filterParams.put("removeAnchorPart", Boolean.valueOf(removeAnchor));
        filterParams.put("unmangleQueryString", Boolean.valueOf(unmangleQueryString));
        return createFilter(filterParams);
    }

    private URLFilter createFilter(ObjectNode filterParams) {
        BasicURLNormalizer filter = new BasicURLNormalizer();
        Map<String, Object> conf = new HashMap<>();
        filter.configure(conf, filterParams);
        return filter;
    }

    @Test
    public void testAnchorFilter() throws MalformedURLException {
        URLFilter allAllowed = createFilter(true, false);
        URL url = new URL("http://www.sourcedomain.com/#0");
        Metadata metadata = new Metadata();
        String filterResult = allAllowed.filter(url, metadata, url.toExternalForm());
        String expected = "http://www.sourcedomain.com/";
        Assert.assertEquals(expected, filterResult);
    }

    @Test
    public void testAnchorFilterFalse() throws MalformedURLException {
        URLFilter allAllowed = createFilter(false, false);
        URL url = new URL("http://www.sourcedomain.com/#0");
        Metadata metadata = new Metadata();
        String filterResult = allAllowed.filter(url, metadata, url.toExternalForm());
        Assert.assertEquals(url.toExternalForm(), filterResult);
    }

    @Test
    public void testRemoveSomeOfManyQueryParams() throws MalformedURLException {
        URLFilter urlFilter = createFilter(queryParamsToFilter);
        URL testSourceUrl = new URL("http://google.com");
        String testUrl = "http://google.com?keep1=true&a=c&foo=baz&keep2=true";
        String expectedResult = "http://google.com?keep1=true&keep2=true";
        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), testUrl);
        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);
    }

    @Test
    public void testRemoveAllQueryParams() throws MalformedURLException {
        URLFilter urlFilter = createFilter(queryParamsToFilter);
        URL testSourceUrl = new URL("http://google.com");
        String testUrl = "http://google.com?a=c&foo=baz";
        String expectedResult = "http://google.com";
        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), testUrl);
        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);
    }

    @Test
    public void testRemoveDupeQueryParams() throws MalformedURLException {
        URLFilter urlFilter = createFilter(queryParamsToFilter);
        URL testSourceUrl = new URL("http://google.com");
        String testUrl = "http://google.com?a=c&foo=baz&foo=bar&test=true";
        String expectedResult = "http://google.com?test=true";
        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), testUrl);
        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);
    }

    @Test
    public void testPipeInUrlAndFilterStillWorks() throws MalformedURLException {
        URLFilter urlFilter = createFilter(queryParamsToFilter);
        URL testSourceUrl = new URL("http://google.com");
        String testUrl = "http://google.com?a=c|d&foo=baz&foo=bar&test=true";
        String expectedResult = "http://google.com?test=true";
        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), testUrl);
        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);
    }

    @Test
    public void testBothAnchorAndQueryFilter() throws MalformedURLException {
        URLFilter urlFilter = createFilter(true, queryParamsToFilter);
        URL testSourceUrl = new URL("http://google.com");
        String testUrl = "http://google.com?a=c|d&foo=baz&foo=bar&test=true#fragment=ohYeah";
        String expectedResult = "http://google.com?test=true";
        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), testUrl);
        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);
    }

    @Test
    public void testQuerySort() throws MalformedURLException {
        URLFilter urlFilter = createFilter(queryParamsToFilter);
        URL testSourceUrl = new URL("http://google.com");
        String testUrl = "http://google.com?a=c|d&foo=baz&foo=bar&test=true&z=2&d=4";
        String expectedResult = "http://google.com?d=4&test=true&z=2";
        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), testUrl);
        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);
    }

    @Test
    public void testMangledQueryString() throws MalformedURLException {
        URLFilter urlFilter = createFilter(queryParamsToFilter);
        URL testSourceUrl = new URL("http://google.com");
        String testUrl = "http://google.com&d=4&good=true";
        String expectedResult = "http://google.com?d=4&good=true";
        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), testUrl);
        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);
    }

    @Test
    public void testHashes() throws MalformedURLException {
        ObjectNode filterParams = new ObjectNode(JsonNodeFactory.instance);
        filterParams.put("removeHashes", true);
        URLFilter urlFilter = createFilter(filterParams);

        URL testSourceUrl = new URL("http://florida-chemical.com");
        String in =
                "http://www.florida-chemical.com/Diacetone-Alcohol-DAA-99.html?xid_0b629=12854b827878df26423d933a5baf86d5";
        String out = "http://www.florida-chemical.com/Diacetone-Alcohol-DAA-99.html";

        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), in);
        assertEquals("Failed to filter query string", out, normalizedUrl);

        in =
                "http://www.maroongroupllc.com/maroon/login/auth;jsessionid=8DBFC2FEDBD740BBC8B4D1A504A6DE7F";
        out = "http://www.maroongroupllc.com/maroon/login/auth";
        normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), in);
        assertEquals("Failed to filter query string", out, normalizedUrl);
    }

    @Test
    public void testDontFixMangledQueryString() throws MalformedURLException {
        URLFilter urlFilter = createFilter(true, false, queryParamsToFilter);
        URL testSourceUrl = new URL("http://google.com");
        String testUrl = "http://google.com&d=4&good=true";
        String expectedResult = "http://google.com&d=4&good=true";
        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), testUrl);
        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);
    }

    @Test
    public void testProperURLEncodingWithoutQueryParameter() throws MalformedURLException {
        URLFilter urlFilter = createFilter(queryParamsToFilter);
        String urlWithEscapedCharacters =
                "http://www.dillards.com/product/ASICS-Womens-GT2000-3-LiteShow%E2%84%A2-Running-Shoes_301_-1_301_504736989";
        URL testSourceUrl = new URL(urlWithEscapedCharacters);
        String testUrl = urlWithEscapedCharacters;
        String expectedResult = urlWithEscapedCharacters;
        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), testUrl);
        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);
    }

    @Test
    public void testProperURLEncodingWithQueryParameters() throws MalformedURLException {
        URLFilter urlFilter = createFilter(queryParamsToFilter);
        String urlWithEscapedCharacters =
                "http://www.dillards.com/product/ASICS-Womens-GT2000-3-LiteShow%E2%84%A2-Running-Shoes_301_-1_301_504736989?how=are&you=doing";
        URL testSourceUrl = new URL(urlWithEscapedCharacters);
        String testUrl = urlWithEscapedCharacters;
        String expectedResult = urlWithEscapedCharacters;
        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), testUrl);
        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);
    }

    @Test
    public void testProperURLEncodingWithBackSlash() throws MalformedURLException {
        URLFilter urlFilter = createFilter(queryParamsToFilter);
        String urlWithEscapedCharacters =
                "http://www.voltaix.com/\\SDS\\Silicon\\Trisilane\\Trisilane_SI050_USENG.pdf";
        String expectedResult =
                "http://www.voltaix.com/%5CSDS%5CSilicon%5CTrisilane%5CTrisilane_SI050_USENG.pdf";
        URL testSourceUrl = new URL(urlWithEscapedCharacters);
        String testUrl = urlWithEscapedCharacters;
        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), testUrl);
        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);
    }

    @Test
    public void testInvalidURI() throws MalformedURLException {
        URLFilter urlFilter = createFilter(true, true);
        // this one is now handled by the normaliser
        String nonURI = "http://www.quanjing.com/search.aspx?q=top-651451||1|60|1|2||||&Fr=4";
        URL testSourceUrl = new URL(nonURI);
        String expectedResult =
                "http://www.quanjing.com/search.aspx?q=top-651451%7C%7C1%7C60%7C1%7C2%7C%7C%7C%7C&Fr=4";
        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), nonURI);
        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);

        // this one is
        nonURI =
                "http://vins.lemonde.fr?utm_source=LeMonde_partenaire_hp&utm_medium=EMPLACEMENT PARTENAIRE&utm_term=&utm_content=&utm_campaign=LeMonde_partenaire_hp";
        testSourceUrl = new URL(nonURI);
        expectedResult =
                "http://vins.lemonde.fr?utm_source=LeMonde_partenaire_hp&utm_medium=EMPLACEMENT%20PARTENAIRE&utm_term=&utm_content=&utm_campaign=LeMonde_partenaire_hp";
        normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), nonURI);
        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);

        // check normalisation of paths
        // http://docs.oracle.com/javase/7/docs/api/java/net/URI.html#normalize()
        String nonNormURL =
                "http://docs.oracle.com/javase/7/docs/api/java/net/../net/./URI.html#normalize()";
        testSourceUrl = new URL(nonNormURL);
        expectedResult = "http://docs.oracle.com/javase/7/docs/api/java/net/URI.html";
        normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), nonNormURL);
        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);
    }

    @Test
    public void testLowerCasing() throws MalformedURLException {
        URLFilter urlFilter = createFilter(false, false);
        URL testSourceUrl = new URL("http://blablabla.org/");

        String inputURL = "HTTP://www.quanjing.com/";
        String expectedResult = inputURL.toLowerCase();
        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), inputURL);

        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);

        inputURL = "http://www.QUANJING.COM/";
        expectedResult = inputURL.toLowerCase();
        normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), inputURL);

        assertEquals("Failed to filter query string", expectedResult, normalizedUrl);
    }

    // https://github.com/DigitalPebble/storm-crawler/issues/401
    @Test
    public void testNonStandardPercentEncoding() throws MalformedURLException {
        URLFilter urlFilter = createFilter(false, false);
        URL testSourceUrl = new URL("http://www.hurriyet.com.tr/index/?d=20160328&p=13");

        String inputURL = "http://www.hurriyet.com.tr/index/?d=20160328&p=13&s=ni%u011fde";
        String expectedURL = "http://www.hurriyet.com.tr/index/?d=20160328&p=13&s=ni%C4%9Fde";
        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), inputURL);

        assertEquals("Failed to filter query string", expectedURL, normalizedUrl);
    }

    @Test
    public void testHostIDNtoASCII() throws MalformedURLException {
        ObjectNode filterParams = new ObjectNode(JsonNodeFactory.instance);
        filterParams.put("hostIDNtoASCII", true);
        URLFilter urlFilter = createFilter(filterParams);
        URL testSourceUrl = new URL("http://www.example.com/");

        String inputURL = "http://seal6.com.ar/";
        String expectedURL = "http://xn--seal6-pta.com.ar/";
        String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), inputURL);

        assertEquals("Failed to filter query string", expectedURL, normalizedUrl);

        inputURL = "http://./";
        expectedURL = "http://xn--80aj7acp.xn--j1amh/";
        normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), inputURL);

        assertEquals("Failed to filter query string", expectedURL, normalizedUrl);
    }

    private JsonNode getArrayNode(List<String> queryElementsToRemove) {
        ObjectMapper mapper = new ObjectMapper();
        return mapper.valueToTree(queryElementsToRemove);
    }
}



Injecting: testRemoveAllQueryParams

New Method:
    @Test
     public void testRemoveAllQueryParams() throws MalformedURLException {
 URLFilter urlFilter = createFilter(queryParamsToFilter); URL testSourceUrl = new URL("http://google.com"); String testUrl = "http://google.com?a=c&foo=baz"; String expectedResult = "http://google.com"; String normalizedUrl = urlFilter.filter(testSourceUrl, new Metadata(), testUrl); org.junit.Assert.assertEquals(expectedResult,normalizedUrl); }
